Paper_ID,Paper,Author_ID,Author_name,Paper_Type,Conference_Type,Source_ID,Source,Year,Document_Type,Volume_Proceeding,Reviewer_1_ID,Reviewer_1,Reviewer_2_ID,Reviewer_2,Handler_ID,Handler,Handler_Type,Area_ID,Areas,Review_ID,Reviewer_Decision,Reviewer_Text,Publication
53e9b388b7602d9703e6d427,LLM: A Low Latency Messaging Infrastructure for Linux Clusters,53f79795dabfae9060acffeb,R. K. Shyamasundar,Short Paper,Expert Group,S28208,HiPC,2002,Conference,2552,RV60571,Brenda Webster,RV74923,Kevin Scott,H33651,tsccfqsrky,Chair,AR45589,low overhead negative acknowledgment,R51653,Rejected,"In this paper, we develop a messaging infrastructure, called LLM, to arrive at a robust and efficient low latency message passing infrastructure for kernel-to-kernel communication. The main focus is to overcome the high latencies associated with the conventional communication protocol stack management of TCP/IP. The LLM provides a transport protocol that offers high reliability at the fragment level keeping the acknowledgment overhead low given the high reliability levels of the LAN. The system utilizes some of the architectural facilities provided by the Linux kernel specially designed for optimization in the respective areas. Reliability against fragment losses is ensured by using a low overhead negative acknowledgment scheme. The implementation is in the form of loadable modules extending the Linux OS. In a typical implementation on a cluster of two nodes, each of uniprocessor Intel Pentium 400 MHz on a 10/100 Mbps LAN achieved an average round trip latency of .169ms as compared to the .531ms obtained by ICMP (Ping) protocol. A relative comparison of LLM with others is also provided.",No
53e9b388b7602d9703e6d427,LLM: A Low Latency Messaging Infrastructure for Linux Clusters,53f4570bdabfaedf4360dffb,Basant Rajan,Short Paper,Expert Group,S28208,HiPC,2002,Conference,2552,RV60571,Brenda Webster,RV74923,Kevin Scott,H33651,tsccfqsrky,Chair,AR45589,low overhead negative acknowledgment,R51653,Rejected,"In this paper, we develop a messaging infrastructure, called LLM, to arrive at a robust and efficient low latency message passing infrastructure for kernel-to-kernel communication. The main focus is to overcome the high latencies associated with the conventional communication protocol stack management of TCP/IP. The LLM provides a transport protocol that offers high reliability at the fragment level keeping the acknowledgment overhead low given the high reliability levels of the LAN. The system utilizes some of the architectural facilities provided by the Linux kernel specially designed for optimization in the respective areas. Reliability against fragment losses is ensured by using a low overhead negative acknowledgment scheme. The implementation is in the form of loadable modules extending the Linux OS. In a typical implementation on a cluster of two nodes, each of uniprocessor Intel Pentium 400 MHz on a 10/100 Mbps LAN achieved an average round trip latency of .169ms as compared to the .531ms obtained by ICMP (Ping) protocol. A relative comparison of LLM with others is also provided.",No
53e9b388b7602d9703e6d427,LLM: A Low Latency Messaging Infrastructure for Linux Clusters,53f43a1cdabfaedf435a1c23,Manish Prasad,Short Paper,Expert Group,S28208,HiPC,2002,Conference,2552,RV60571,Brenda Webster,RV74923,Kevin Scott,H33651,tsccfqsrky,Chair,AR45589,low overhead negative acknowledgment,R51653,Rejected,"In this paper, we develop a messaging infrastructure, called LLM, to arrive at a robust and efficient low latency message passing infrastructure for kernel-to-kernel communication. The main focus is to overcome the high latencies associated with the conventional communication protocol stack management of TCP/IP. The LLM provides a transport protocol that offers high reliability at the fragment level keeping the acknowledgment overhead low given the high reliability levels of the LAN. The system utilizes some of the architectural facilities provided by the Linux kernel specially designed for optimization in the respective areas. Reliability against fragment losses is ensured by using a low overhead negative acknowledgment scheme. The implementation is in the form of loadable modules extending the Linux OS. In a typical implementation on a cluster of two nodes, each of uniprocessor Intel Pentium 400 MHz on a 10/100 Mbps LAN achieved an average round trip latency of .169ms as compared to the .531ms obtained by ICMP (Ping) protocol. A relative comparison of LLM with others is also provided.",No
53e9b388b7602d9703e6d427,LLM: A Low Latency Messaging Infrastructure for Linux Clusters,53f466cbdabfaedf436476fe,Amit Jain,Short Paper,Expert Group,S28208,HiPC,2002,Conference,2552,RV60571,Brenda Webster,RV74923,Kevin Scott,H33651,tsccfqsrky,Chair,AR45589,low overhead negative acknowledgment,R51653,Rejected,"In this paper, we develop a messaging infrastructure, called LLM, to arrive at a robust and efficient low latency message passing infrastructure for kernel-to-kernel communication. The main focus is to overcome the high latencies associated with the conventional communication protocol stack management of TCP/IP. The LLM provides a transport protocol that offers high reliability at the fragment level keeping the acknowledgment overhead low given the high reliability levels of the LAN. The system utilizes some of the architectural facilities provided by the Linux kernel specially designed for optimization in the respective areas. Reliability against fragment losses is ensured by using a low overhead negative acknowledgment scheme. The implementation is in the form of loadable modules extending the Linux OS. In a typical implementation on a cluster of two nodes, each of uniprocessor Intel Pentium 400 MHz on a 10/100 Mbps LAN achieved an average round trip latency of .169ms as compared to the .531ms obtained by ICMP (Ping) protocol. A relative comparison of LLM with others is also provided.",No
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,53f43b05dabfaee43ec5cebf,Rodolfo V. García,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,54410868dabfae7d84bd6077,Fernando Rojas,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,53f42b00dabfaec22b9f0a6e,Carlos G. Puntonet,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,53f45e21dabfaec09f220e4d,Belén San Román,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,53f4745adabfaedd74ea0e9b,Luís Velázquez,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d83b,Advances in ataxia SCA-2 diagnosis using independent component analysis,53f45351dabfaedce55a728c,Roberto Rodríguez,Poster,Workshop,S29058,NOLISP,2009,Conference,5933,RV65259,Jonathan Roberts,RV74760,Candace Bennett,H31975,pcsjuubggu,Chair,AR41317,healthy subject,R57541,Accepted,"This work discusses a new approach for ataxia SCA-2 diagnosis based in the application of independent component analysis to the data obtained by electro-oculography in several experiments carried out over healthy and sick subjects. Abnormalities in the oculomotor system are well known clinical symptoms in patients of several neurodegenerative diseases, including modifications in latency, peak velocity, and deviation in saccadic movements, causing changes in the waveform of the patient response. The changes in the morphology waveform suggest a higher degree of statistic independence in sick patients when compared to healthy individuals regarding the patient response to the visual saccadic stimulus modeled by means of digital generated saccade waveforms. The electro-oculogram records of six patients diagnosed with ataxia SCA2 (a neurodegenerative hereditary disease) and six healthy subjects used as control were processed to extract saccades.",NOLISP 5933
53e9b388b7602d9703e6d4f2,Function Design of Ancient Books Repairing Archives Management System Based on Tacit Knowledge.,5406b317dabfae44f0850e41,Fu Jin,Short Paper,None,S27512,KSEM,2013,Journal,8041 LNAI,RV66732,Lori Stanton DVM,RV78929,Judith Escobar,H32353,vfrphehmca,Editor,AR46140,ancient books repairing archives,R52868,Rejected,"The ancient books repairing, in which masters teach apprentices to inherit repairing skills in a long time, is a Chinese traditional handicraft industry and has a lot of tacit knowledge. In this paper knowledge management is introduced, and then the ancient books repairing archives database and its management system are analyzed. The content setting of ancient books repairing archives and function design of its management system are tried to solve. The research conclusion has practical significance in promoting the digital construction process of ancient books repairing archives and improving the repairing quality and management efficiency of ancient books. © 2013 Springer-Verlag Berlin Heidelberg.",No
53e9b388b7602d9703e6d4f2,Function Design of Ancient Books Repairing Archives Management System Based on Tacit Knowledge.,53f433aadabfaee4dc767932,Miaoyan Li,Short Paper,None,S27512,KSEM,2013,Journal,8041 LNAI,RV66732,Lori Stanton DVM,RV78929,Judith Escobar,H32353,vfrphehmca,Editor,AR46140,ancient books repairing archives,R52868,Rejected,"The ancient books repairing, in which masters teach apprentices to inherit repairing skills in a long time, is a Chinese traditional handicraft industry and has a lot of tacit knowledge. In this paper knowledge management is introduced, and then the ancient books repairing archives database and its management system are analyzed. The content setting of ancient books repairing archives and function design of its management system are tried to solve. The research conclusion has practical significance in promoting the digital construction process of ancient books repairing archives and improving the repairing quality and management efficiency of ancient books. © 2013 Springer-Verlag Berlin Heidelberg.",No
53e9b388b7602d9703e6d4f2,Function Design of Ancient Books Repairing Archives Management System Based on Tacit Knowledge.,53f447c8dabfaeee229fcfff,Qingdong Du,Short Paper,None,S27512,KSEM,2013,Journal,8041 LNAI,RV66732,Lori Stanton DVM,RV78929,Judith Escobar,H32353,vfrphehmca,Editor,AR46140,ancient books repairing archives,R52868,Rejected,"The ancient books repairing, in which masters teach apprentices to inherit repairing skills in a long time, is a Chinese traditional handicraft industry and has a lot of tacit knowledge. In this paper knowledge management is introduced, and then the ancient books repairing archives database and its management system are analyzed. The content setting of ancient books repairing archives and function design of its management system are tried to solve. The research conclusion has practical significance in promoting the digital construction process of ancient books repairing archives and improving the repairing quality and management efficiency of ancient books. © 2013 Springer-Verlag Berlin Heidelberg.",No
53e9b388b7602d9703e6d627,On the potential of fixed-beam 60 GHz network interfaces in mobile devices,53f44128dabfaefedbb05c81,Kishore Ramachandran,Short Paper,None,S22383,PAM,2011,Journal,6579,RV62809,Christian Carlson,RV75535,Anna Rogers,H32934,cihijfzqja,Editor,AR49847,attractive technology,R56730,Rejected,"The small form-factor and significantly high bandwidth of 60 GHz wireless network interfaces make them an attractive technology for future bandwidth-hungry mobile devices. To overcome several challenges in making such 60 GHz communication practical, beamforming is widely accepted as an integral part of 60 GHz devices. In this paper, we perform a first-of-its-kind user study to answer a rather unconventional question: can users explicitly assist in aligning fixed-beam directional antennas on the transmit/receive side? Our measurements involving 30 users show significant promise, and lean us towards answering the question in the affirmative. The implication of these observations is in substantially simplifying the design of 60 GHz interfaces for mobile devices.",No
53e9b388b7602d9703e6d627,On the potential of fixed-beam 60 GHz network interfaces in mobile devices,53f455f9dabfaee1c0b2bdef,Ravi Kokku,Short Paper,None,S22383,PAM,2011,Journal,6579,RV62809,Christian Carlson,RV75535,Anna Rogers,H32934,cihijfzqja,Editor,AR49847,attractive technology,R56730,Rejected,"The small form-factor and significantly high bandwidth of 60 GHz wireless network interfaces make them an attractive technology for future bandwidth-hungry mobile devices. To overcome several challenges in making such 60 GHz communication practical, beamforming is widely accepted as an integral part of 60 GHz devices. In this paper, we perform a first-of-its-kind user study to answer a rather unconventional question: can users explicitly assist in aligning fixed-beam directional antennas on the transmit/receive side? Our measurements involving 30 users show significant promise, and lean us towards answering the question in the affirmative. The implication of these observations is in substantially simplifying the design of 60 GHz interfaces for mobile devices.",No
53e9b388b7602d9703e6d627,On the potential of fixed-beam 60 GHz network interfaces in mobile devices,53f448f0dabfaedd74df8e69,Rajesh Mahindra,Short Paper,None,S22383,PAM,2011,Journal,6579,RV62809,Christian Carlson,RV75535,Anna Rogers,H32934,cihijfzqja,Editor,AR49847,attractive technology,R56730,Rejected,"The small form-factor and significantly high bandwidth of 60 GHz wireless network interfaces make them an attractive technology for future bandwidth-hungry mobile devices. To overcome several challenges in making such 60 GHz communication practical, beamforming is widely accepted as an integral part of 60 GHz devices. In this paper, we perform a first-of-its-kind user study to answer a rather unconventional question: can users explicitly assist in aligning fixed-beam directional antennas on the transmit/receive side? Our measurements involving 30 users show significant promise, and lean us towards answering the question in the affirmative. The implication of these observations is in substantially simplifying the design of 60 GHz interfaces for mobile devices.",No
53e9b388b7602d9703e6d627,On the potential of fixed-beam 60 GHz network interfaces in mobile devices,53f4300fdabfaeb2ac00b855,Kenichi Maruhashi,Short Paper,None,S22383,PAM,2011,Journal,6579,RV62809,Christian Carlson,RV75535,Anna Rogers,H32934,cihijfzqja,Editor,AR49847,attractive technology,R56730,Rejected,"The small form-factor and significantly high bandwidth of 60 GHz wireless network interfaces make them an attractive technology for future bandwidth-hungry mobile devices. To overcome several challenges in making such 60 GHz communication practical, beamforming is widely accepted as an integral part of 60 GHz devices. In this paper, we perform a first-of-its-kind user study to answer a rather unconventional question: can users explicitly assist in aligning fixed-beam directional antennas on the transmit/receive side? Our measurements involving 30 users show significant promise, and lean us towards answering the question in the affirmative. The implication of these observations is in substantially simplifying the design of 60 GHz interfaces for mobile devices.",No
53e9b388b7602d9703e6daa3,An Efficient Pipelined Parallel Join Algorithm on Heterogeneous Distributed Architectures,53f46f84dabfaeecd6a2fe70,Mohamad Al Hajj Hassan,Full Paper,Symposium,S28487,Communications in Computer and Information Science,2008,Conference,47,RV63704,Brianna Powell,RV71978,Stacey Kennedy,H37621,snyzpnbsoq,Chair,AR40560,bulk synchronous parallel,R57438,Accepted,"Pipelined parallelism was largely studied and successfully implemented, on shared nothing machines, in several join algorithms in the presence of ideal conditions of load balancing between processors and in the absence of data skew. The aim of pipelining is to allow flexible resource allocation while avoiding unnecessary disk input/output for intermediate join results in the treatment of multi-join queries. The main drawback of pipelining in existing algorithms is that communication and load balancing remain limited to the use of static approaches (generated during query optimization phase) based on hashing to redistribute data over the network and therefore cannot solve data skew problem and load imbalance between processors on heterogeneous multi-processor architectures where the load of each processor may vary in a dynamic and unpredictable way. In this paper, we present a pipelined parallel algorithm for multi-join queries allowing to solve the problem of data skew while guaranteeing perfect balancing properties, on heterogeneous multi-processor Shared Nothing architectures. The performance of this algorithm is analyzed using the scalable portable BSP (Bulk Synchronous Parallel) cost model.",Communications in Computer and Information Science 47
53e9b388b7602d9703e6daa3,An Efficient Pipelined Parallel Join Algorithm on Heterogeneous Distributed Architectures,53f47369dabfaee02adc810e,Mostafa Bamha,Full Paper,Symposium,S28487,Communications in Computer and Information Science,2008,Conference,47,RV63704,Brianna Powell,RV71978,Stacey Kennedy,H37621,snyzpnbsoq,Chair,AR40560,bulk synchronous parallel,R57438,Accepted,"Pipelined parallelism was largely studied and successfully implemented, on shared nothing machines, in several join algorithms in the presence of ideal conditions of load balancing between processors and in the absence of data skew. The aim of pipelining is to allow flexible resource allocation while avoiding unnecessary disk input/output for intermediate join results in the treatment of multi-join queries. The main drawback of pipelining in existing algorithms is that communication and load balancing remain limited to the use of static approaches (generated during query optimization phase) based on hashing to redistribute data over the network and therefore cannot solve data skew problem and load imbalance between processors on heterogeneous multi-processor architectures where the load of each processor may vary in a dynamic and unpredictable way. In this paper, we present a pipelined parallel algorithm for multi-join queries allowing to solve the problem of data skew while guaranteeing perfect balancing properties, on heterogeneous multi-processor Shared Nothing architectures. The performance of this algorithm is analyzed using the scalable portable BSP (Bulk Synchronous Parallel) cost model.",Communications in Computer and Information Science 47
53e9b388b7602d9703e6d7bd,How push-to-talk makes talk less pushy,5609324c45cedb3396e4f190,Allison Woodruff,Short Paper,None,S25215,GROUP '03 Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work,2003,Journal,cs.HC/0311006,RV69800,David Perez,RV73438,Elizabeth Medina,H30675,rdwawgdgjd,Editor,AR46375,audio medium,R56872,Accepted,"This paper presents an exploratory study of college-age students using two-way, push-to-talk cellular radios. We describe the observed and reported use of cellular radio by the participants. We discuss how the half-duplex, lightweight cellular radio communication was associated with reduced interactional commitment, which meant the cellular radios could be used for a wide range of conversation styles. One such style, intermittent conversation, is characterized by response delays. Intermittent conversation is surprising in an audio medium, since it is typically associated with textual media such as instant messaging. We present design implications of our findings.",GROUP '03 Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work cs.HC/0311006
53e9b388b7602d9703e6d7bd,How push-to-talk makes talk less pushy,5448d378dabfae87b7e80be7,Paul M. Aoki,Short Paper,None,S25215,GROUP '03 Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work,2003,Journal,cs.HC/0311006,RV69800,David Perez,RV73438,Elizabeth Medina,H30675,rdwawgdgjd,Editor,AR46375,audio medium,R56872,Accepted,"This paper presents an exploratory study of college-age students using two-way, push-to-talk cellular radios. We describe the observed and reported use of cellular radio by the participants. We discuss how the half-duplex, lightweight cellular radio communication was associated with reduced interactional commitment, which meant the cellular radios could be used for a wide range of conversation styles. One such style, intermittent conversation, is characterized by response delays. Intermittent conversation is surprising in an audio medium, since it is typically associated with textual media such as instant messaging. We present design implications of our findings.",GROUP '03 Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work cs.HC/0311006
53e9b388b7602d9703e6d876,Detection of Masses in Mammographic Images Using Simpson's Diversity Index in Circular Regions and SVM,53f431a1dabfaedce54fb885,André Pereira Nunes,Demo Paper,Expert Group,S28908,MLDM,2009,Conference,5632,RV66581,Charles Mccarthy,RV74366,Daniel Gutierrez,H31490,bdwvbgjeqz,Chair,AR42116,major cause,R52985,Rejected,"Breast cancer is one of the major causes of death among women all over the world. Presently, mammographic analysis is the most used method for early detection of abnormalities. This paper presents a computational methodology to help the specialist with this task. In the first step, the K-Means clustering algorithm and the Template Matching technique are used to detect suspicious regions. Next, the texture of each region is described using the Simpson's Diversity Index, which is used in Ecology to measure the biodiversity of an ecosystem. Finally, the information of texture is used by SVM to classify the suspicious regions into two classes: masses and non-masses. The tests demonstrate that the methodology has 79.12% of accuracy, 77.27% of sensitivity, and 79.66% of specificity.",No
53e9b388b7602d9703e6d876,Detection of Masses in Mammographic Images Using Simpson's Diversity Index in Circular Regions and SVM,53f431d3dabfaee2a1cb54b8,Aristófanes Corrêa Silva,Demo Paper,Expert Group,S28908,MLDM,2009,Conference,5632,RV66581,Charles Mccarthy,RV74366,Daniel Gutierrez,H31490,bdwvbgjeqz,Chair,AR42116,major cause,R52985,Rejected,"Breast cancer is one of the major causes of death among women all over the world. Presently, mammographic analysis is the most used method for early detection of abnormalities. This paper presents a computational methodology to help the specialist with this task. In the first step, the K-Means clustering algorithm and the Template Matching technique are used to detect suspicious regions. Next, the texture of each region is described using the Simpson's Diversity Index, which is used in Ecology to measure the biodiversity of an ecosystem. Finally, the information of texture is used by SVM to classify the suspicious regions into two classes: masses and non-masses. The tests demonstrate that the methodology has 79.12% of accuracy, 77.27% of sensitivity, and 79.66% of specificity.",No
53e9b388b7602d9703e6d876,Detection of Masses in Mammographic Images Using Simpson's Diversity Index in Circular Regions and SVM,53f32ab4dabfae9a8448ba07,Anselmo Cardoso Paiva,Demo Paper,Expert Group,S28908,MLDM,2009,Conference,5632,RV66581,Charles Mccarthy,RV74366,Daniel Gutierrez,H31490,bdwvbgjeqz,Chair,AR42116,major cause,R52985,Rejected,"Breast cancer is one of the major causes of death among women all over the world. Presently, mammographic analysis is the most used method for early detection of abnormalities. This paper presents a computational methodology to help the specialist with this task. In the first step, the K-Means clustering algorithm and the Template Matching technique are used to detect suspicious regions. Next, the texture of each region is described using the Simpson's Diversity Index, which is used in Ecology to measure the biodiversity of an ecosystem. Finally, the information of texture is used by SVM to classify the suspicious regions into two classes: masses and non-masses. The tests demonstrate that the methodology has 79.12% of accuracy, 77.27% of sensitivity, and 79.66% of specificity.",No
53e9b388b7602d9703e6d895,Query Recommendations for Interactive Database Exploration,53f456b5dabfaeee22a354c4,Gloria Chatzopoulou,Demo Paper,Workshop,S29203,SSDBM,2009,Conference,5566,RV60506,Christopher Wallace,RV76291,Laurie Valenzuela,H31386,jkaayyfwam,Chair,AR46196,relevant data,R55335,Rejected,"Relational database systems are becoming increasingly popular in the scientific community to support the interactive exploration of large volumes of data. In this scenario, users employ a query interface (typically, a web-based client) to issue a series of SQL queries that aim to analyze the data and mine it for interesting information. First-time users, however, may not have the necessary knowledge to know where to start their exploration. Other times, users may simply overlook queries that retrieve important information. To assist users in this context, we draw inspiration from Web recommender systems and propose the use of personalized query recommendations. The idea is to track the querying behavior of each user, identify which parts of the database may be of interest for the corresponding data analysis task, and recommend queries that retrieve relevant data. We discuss the main challenges in this novel application of recommendation systems, and outline a possible solution based on collaborative filtering. Preliminary experimental results on real user traces demonstrate that our framework can generate effective query recommendations.",No
53e9b388b7602d9703e6d895,Query Recommendations for Interactive Database Exploration,53f46596dabfaee2a1dab6e1,Magdalini Eirinaki,Demo Paper,Workshop,S29203,SSDBM,2009,Conference,5566,RV60506,Christopher Wallace,RV76291,Laurie Valenzuela,H31386,jkaayyfwam,Chair,AR46196,relevant data,R55335,Rejected,"Relational database systems are becoming increasingly popular in the scientific community to support the interactive exploration of large volumes of data. In this scenario, users employ a query interface (typically, a web-based client) to issue a series of SQL queries that aim to analyze the data and mine it for interesting information. First-time users, however, may not have the necessary knowledge to know where to start their exploration. Other times, users may simply overlook queries that retrieve important information. To assist users in this context, we draw inspiration from Web recommender systems and propose the use of personalized query recommendations. The idea is to track the querying behavior of each user, identify which parts of the database may be of interest for the corresponding data analysis task, and recommend queries that retrieve relevant data. We discuss the main challenges in this novel application of recommendation systems, and outline a possible solution based on collaborative filtering. Preliminary experimental results on real user traces demonstrate that our framework can generate effective query recommendations.",No
53e9b388b7602d9703e6d895,Query Recommendations for Interactive Database Exploration,53f43e56dabfaedf435b7ba3,Neoklis Polyzotis,Demo Paper,Workshop,S29203,SSDBM,2009,Conference,5566,RV60506,Christopher Wallace,RV76291,Laurie Valenzuela,H31386,jkaayyfwam,Chair,AR46196,relevant data,R55335,Rejected,"Relational database systems are becoming increasingly popular in the scientific community to support the interactive exploration of large volumes of data. In this scenario, users employ a query interface (typically, a web-based client) to issue a series of SQL queries that aim to analyze the data and mine it for interesting information. First-time users, however, may not have the necessary knowledge to know where to start their exploration. Other times, users may simply overlook queries that retrieve important information. To assist users in this context, we draw inspiration from Web recommender systems and propose the use of personalized query recommendations. The idea is to track the querying behavior of each user, identify which parts of the database may be of interest for the corresponding data analysis task, and recommend queries that retrieve relevant data. We discuss the main challenges in this novel application of recommendation systems, and outline a possible solution based on collaborative filtering. Preliminary experimental results on real user traces demonstrate that our framework can generate effective query recommendations.",No
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,560d1b4c45ce1e5960b6f206,Tim O. F. Conrad,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,53f43073dabfaec22ba3f136,Alexander Leichtle,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,53f46a54dabfaeb22f54dfa6,Andre Hagehülsmann,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,53f46d33dabfaec09f25a6e1,Elmar Diederichs,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,543337e4dabfaeb54217c8fc,Sven Baumann,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,544871eddabfae87b7e25c56,Joachim Thiery,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db77,Beating the noise: new statistical methods for detecting signals in MALDI-TOF spectra below noise level,53f43aa5dabfaeb22f499d39,Christof Schütte,Demo Paper,Expert Group,S24260,CompLife,2006,Conference,4216,RV68937,Cathy Ferguson,RV76607,Anthony Chavez,H36719,xenplfiodj,Chair,AR42217,typical noise threshold,R57239,Accepted,"Background: The computer-assisted detection of small molecules by mass spectrometry in biological samples provides a snapshot of thousands of peptides, protein fragments and proteins in biological samples. This new analytical technology has the potential to identify disease associated proteomic patterns in blood serum. However, the presently available bioinformatic tools are not sensitive enough to identify clinically important low abundant proteins as hormons or tumor markers with only low blood concentrations. Aim: Find, analyze and compare serum proteom patterns in groups of human subjects having different properties such as disease status with a new workflow to enhance sensitivity and specificity. Problems: Mass data acquired from high-throughput platforms frequently are blurred and noisy. This complicates the reliable identification of peaks in general and very small peaks even below noise level in particular. However, this statement is only valid for single or few spectra. If the algorithm has access to a large number of spectra (e.g. N 1000), new possibilities arise, one of such being a statistical approach. Approach: Apply signal preprocessing steps followed by statistical analyses of the blurred data and the region below the typical noise threshold to identify signals usually hidden below this “barrier”. Results: A new analysis workflow has been developed that is able to accurately identify, analyze and determine peaks and their parameters even below noise level which other tools can not detect. A Comparison to commercial software has clearly proven this gain in sensitivity. These additional peaks can be used in subsequent steps to build better peak patterns for proteomic pattern analysis. We belive that this new approach will foster identification of new biomarkers having not been detectable by most algorithms currently available.",CompLife 4216
53e9b388b7602d9703e6db58,Non-linear Real Constraints in Constraint Logic Programming,5609325445cedb3396e4f38f,Hoon Hong,Full Paper,None,S25540,ALP,1992,Journal,632,RV65101,Jeremiah Callahan,RV77679,Phillip Johnson,H36896,cepuqrwtou,Editor,AR49156,non-linear real constraints,R59181,Accepted,Dealing with non-linear constraints over real numbers is one of the most important and non-trivial problems in constraint logic programming. We report our initial effort in tackling the problem with two methods developed in computer algebra during last three decades: Partial Cylindrical Algebraic Decomposition and Gröbner basis. We have implemented a prototype called RISC-CLP(Real). Experience with the prototype suggests that it is desirable and in fact feasible to provide a full support of non-linear constraints.,ALP 632
53e9b388b7602d9703e6d912,Decentralized user authentication in a global file system,5632059345cedb3399f96f7f,Michael Kaminsky,Short Paper,Workshop,S23700,SOSP,2003,Conference,37,RV61850,Mackenzie Acosta,RV76493,Kimberly Bailey,H33122,xsixfwxcor,Chair,AR44971,security,R54407,Accepted,"The challenge for user authentication in a global file system is allowing people to grant access to specific users and groups in remote administrative domains, without assuming any kind of pre-existing administrative relationship. The traditional approach to user authentication across administrative domains is for users to prove their identities through a chain of certificates. Certificates allow for general forms of delegation, but they often require more infrastructure than is necessary to support a network file system.This paper introduces an approach without certificates. Local authentication servers pre-fetch and cache remote user and group definitions from remote authentication servers. During a file access, an authentication server can establish identities for users based just on local information. This approach is particularly well-suited to file systems, and it provides a simple and intuitive interface that is similar to those found in local access control mechanisms. An implementation of the authentication server and a file server supporting access control lists demonstrate the viability of this design in the context of the Self-certifying File System (SFS). Experiments demonstrate that the authentication server can scale to groups with tens of thousands of members.",SOSP 37
53e9b388b7602d9703e6d912,Decentralized user authentication in a global file system,53f3757cdabfae4b349cf638,George Savvides,Short Paper,Workshop,S23700,SOSP,2003,Conference,37,RV61850,Mackenzie Acosta,RV76493,Kimberly Bailey,H33122,xsixfwxcor,Chair,AR44971,security,R54407,Accepted,"The challenge for user authentication in a global file system is allowing people to grant access to specific users and groups in remote administrative domains, without assuming any kind of pre-existing administrative relationship. The traditional approach to user authentication across administrative domains is for users to prove their identities through a chain of certificates. Certificates allow for general forms of delegation, but they often require more infrastructure than is necessary to support a network file system.This paper introduces an approach without certificates. Local authentication servers pre-fetch and cache remote user and group definitions from remote authentication servers. During a file access, an authentication server can establish identities for users based just on local information. This approach is particularly well-suited to file systems, and it provides a simple and intuitive interface that is similar to those found in local access control mechanisms. An implementation of the authentication server and a file server supporting access control lists demonstrate the viability of this design in the context of the Self-certifying File System (SFS). Experiments demonstrate that the authentication server can scale to groups with tens of thousands of members.",SOSP 37
53e9b388b7602d9703e6d912,Decentralized user authentication in a global file system,53f318f8dabfae9a84428dbb,David Mazières,Short Paper,Workshop,S23700,SOSP,2003,Conference,37,RV61850,Mackenzie Acosta,RV76493,Kimberly Bailey,H33122,xsixfwxcor,Chair,AR44971,security,R54407,Accepted,"The challenge for user authentication in a global file system is allowing people to grant access to specific users and groups in remote administrative domains, without assuming any kind of pre-existing administrative relationship. The traditional approach to user authentication across administrative domains is for users to prove their identities through a chain of certificates. Certificates allow for general forms of delegation, but they often require more infrastructure than is necessary to support a network file system.This paper introduces an approach without certificates. Local authentication servers pre-fetch and cache remote user and group definitions from remote authentication servers. During a file access, an authentication server can establish identities for users based just on local information. This approach is particularly well-suited to file systems, and it provides a simple and intuitive interface that is similar to those found in local access control mechanisms. An implementation of the authentication server and a file server supporting access control lists demonstrate the viability of this design in the context of the Self-certifying File System (SFS). Experiments demonstrate that the authentication server can scale to groups with tens of thousands of members.",SOSP 37
53e9b388b7602d9703e6d912,Decentralized user authentication in a global file system,53f47ee4dabfae9126cc5137,M. Frans Kaashoek,Short Paper,Workshop,S23700,SOSP,2003,Conference,37,RV61850,Mackenzie Acosta,RV76493,Kimberly Bailey,H33122,xsixfwxcor,Chair,AR44971,security,R54407,Accepted,"The challenge for user authentication in a global file system is allowing people to grant access to specific users and groups in remote administrative domains, without assuming any kind of pre-existing administrative relationship. The traditional approach to user authentication across administrative domains is for users to prove their identities through a chain of certificates. Certificates allow for general forms of delegation, but they often require more infrastructure than is necessary to support a network file system.This paper introduces an approach without certificates. Local authentication servers pre-fetch and cache remote user and group definitions from remote authentication servers. During a file access, an authentication server can establish identities for users based just on local information. This approach is particularly well-suited to file systems, and it provides a simple and intuitive interface that is similar to those found in local access control mechanisms. An implementation of the authentication server and a file server supporting access control lists demonstrate the viability of this design in the context of the Self-certifying File System (SFS). Experiments demonstrate that the authentication server can scale to groups with tens of thousands of members.",SOSP 37
53e9b38eb7602d9703e6dc57,Lowering undecidability bounds for decision questions in matrices,53f3642cdabfae4b3499316b,Paul Bell,Full Paper,Workshop,S27281,Developments in Language Theory,2006,Conference,4036,RV63231,Martin Ray,RV72941,Kayla Cross,H38619,teibzarieh,Chair,AR41543,matrix semigroups,R58454,Rejected,"In this paper we consider several reachability problems such as vector reachability, membership in matrix semigroups and reachability problems in piecewise linear maps. Since all of these questions are undecidable in general, we work on lowering the bounds for undecidability. In particular, we show an elementary proof of undecidability of the reachability problem for a set of 7 two-dimensional affine transformations. Then, using a modified version of a standard technique, we also prove the vector reachability problem is undecidable for two (rational) matrices in dimension 16. The above result can be used to show that the system of piecewise linear functions of dimension 17 with only two intervals has an undecidable set-to-point reachability problem. We also show that the “zero in the upper right corner” problem is undecidable for two integral matrices of dimension 18 lowering the bound from 23.",No
53e9b38eb7602d9703e6dc57,Lowering undecidability bounds for decision questions in matrices,53f44e68dabfaefedbb32beb,Igor Potapov,Full Paper,Workshop,S27281,Developments in Language Theory,2006,Conference,4036,RV63231,Martin Ray,RV72941,Kayla Cross,H38619,teibzarieh,Chair,AR41543,matrix semigroups,R58454,Rejected,"In this paper we consider several reachability problems such as vector reachability, membership in matrix semigroups and reachability problems in piecewise linear maps. Since all of these questions are undecidable in general, we work on lowering the bounds for undecidability. In particular, we show an elementary proof of undecidability of the reachability problem for a set of 7 two-dimensional affine transformations. Then, using a modified version of a standard technique, we also prove the vector reachability problem is undecidable for two (rational) matrices in dimension 16. The above result can be used to show that the system of piecewise linear functions of dimension 17 with only two intervals has an undecidable set-to-point reachability problem. We also show that the “zero in the upper right corner” problem is undecidable for two integral matrices of dimension 18 lowering the bound from 23.",No
53e9b38eb7602d9703e6dc59,Towards an Evolution Model of Expressive Music Performance,53f4568ddabfaee02ad5996e,Qijun Zhang,Demo Paper,Expert Group,S27537,ISDA (2),2006,Conference,2,RV65690,Philip Peterson,RV70101,Randy Humphrey,H34860,nbiyiladke,Chair,AR47582,hierarchical pulse set,R51656,Accepted,"This paper presents the design of representing the performance profile with hierarchical pulse sets (i.e., hierarchical duration vs. amplitude matrices), and then applying genetic algorithm (GA) to evolve the hierarchical pulse sets for music interpretation, where the fitness of GA is derived from the structure of the music to be performed. In our previous work (2006), we have shown that GA can evolve suitable pulse sets for musical performance. Also, commonality and diversity are found among the performance profiles decided by those evolved pulse sets. This paper reports the experiment results from an improved system where a new version of fitness rules has been devised. On basis of this system, we are proposing the next steps for the research, that is, to build a dynamic model that evolves expressive music performance through agent performers' interactions",ISDA (2) 2
53e9b38eb7602d9703e6dc59,Towards an Evolution Model of Expressive Music Performance,53f642c5dabfaecc55c71628,Eduardo Reck Miranda,Demo Paper,Expert Group,S27537,ISDA (2),2006,Conference,2,RV65690,Philip Peterson,RV70101,Randy Humphrey,H34860,nbiyiladke,Chair,AR47582,hierarchical pulse set,R51656,Accepted,"This paper presents the design of representing the performance profile with hierarchical pulse sets (i.e., hierarchical duration vs. amplitude matrices), and then applying genetic algorithm (GA) to evolve the hierarchical pulse sets for music interpretation, where the fitness of GA is derived from the structure of the music to be performed. In our previous work (2006), we have shown that GA can evolve suitable pulse sets for musical performance. Also, commonality and diversity are found among the performance profiles decided by those evolved pulse sets. This paper reports the experiment results from an improved system where a new version of fitness rules has been devised. On basis of this system, we are proposing the next steps for the research, that is, to build a dynamic model that evolves expressive music performance through agent performers' interactions",ISDA (2) 2
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,53f43b00dabfaee0d9b912f1,Antonietta M. Esposito,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,54891b16dabfae9b401347a0,Silvia Scarpetta,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,53f4440fdabfaefedbb0bf00,Flora Giudicepietro,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,53f45995dabfaee1c0b39df8,Stefano Masiello,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,53f37806dabfae4b349d81c9,Luca Pugliese,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dc6e,Nonlinear exploratory data analysis applied to seismic signals,53f4341ddabfaeb22f45b909,Anna Esposito,Short Paper,Workshop,S25615,WIRN/NAIS,2005,Conference,3931,RV60916,Beverly Campbell,RV74174,Angela Johnson,H34962,cfrzsneheb,Chair,AR43335,nonlinear exploratory data analysis,R54746,Rejected,"This paper compares three unsupervised projection methods: Principal Component Analysis (PCA), which is linear, Self-Organizing Map (SOM) and Curvilinear Component Analysis (CCA), which are both nonlinear. Performance comparison of the three methods is made on a set of seismic data recorded on Stromboli that includes three classes of signals: explosion-quakes, landslides, and microtremors. The unsupervised analysis of the signals is able to discover the nature of the seismic events. Our analysis shows that the SOM algorithm discriminates better than CCA and PCA on the data under examination.",No
53e9b38eb7602d9703e6dcbe,Solving fully-observable non-deterministic planning problems via translation into a general game,53f43509dabfaee02acbe20d,Peter Kissmann,Short Paper,None,S26583,KI,2009,Journal,5803,RV64047,Shelly Oneill,RV76193,Brian Khan,H30235,jneaoujjmy,Editor,AR42980,existing static analysis tool,R55198,Rejected,"In this paper, we propose a symbolic planner based on BDDs, which calculates strong and strong cyclic plans for a given non-deterministic input. The efficiency of the planning approach is based on a translation of the nondeterministic planning problems into a two-player turn-taking game, with a set of actions selected by the solver and a set of actions taken by the environment. The formalism we use is a PDDL-like planning domain definition language that has been derived to parse and instantiate general games. This conversion allows to derive a concise description of planning domains with a minimized state vector, thereby exploiting existing static analysis tools for deterministic planning.",No
53e9b38eb7602d9703e6dcbe,Solving fully-observable non-deterministic planning problems via translation into a general game,54409465dabfae805a6d55de,Stefan Edelkamp,Short Paper,None,S26583,KI,2009,Journal,5803,RV64047,Shelly Oneill,RV76193,Brian Khan,H30235,jneaoujjmy,Editor,AR42980,existing static analysis tool,R55198,Rejected,"In this paper, we propose a symbolic planner based on BDDs, which calculates strong and strong cyclic plans for a given non-deterministic input. The efficiency of the planning approach is based on a translation of the nondeterministic planning problems into a two-player turn-taking game, with a set of actions selected by the solver and a set of actions taken by the environment. The formalism we use is a PDDL-like planning domain definition language that has been derived to parse and instantiate general games. This conversion allows to derive a concise description of planning domains with a minimized state vector, thereby exploiting existing static analysis tools for deterministic planning.",No
53e9b388b7602d9703e6db85,An agent model for incremental rough set-based rule induction in customer relationship management,53f42ea3dabfaeb22f4185be,Yu-Neng Fan,Demo Paper,None,S25028,HAIS (1),2012,Journal,7208,RV60817,Thomas Gonzales,RV76158,Michael Brown,H34714,befwfwnwyf,Editor,AR46933,qualitative information,R51615,Rejected,"Compared to other methods, rough set (RS) has the advantage of combining both qualitative and quantitative information in decision analysis, which is extremely important for customer relationship management (CRM). In this paper, we introduce an application of a multi-agent embedded incremental rough set-based rule induction to CRM, namely Incremental Rough Set-based Rule Induction Agent (IRSRIA). The rule induction is based on creating agents within the main modeling processes. This method is suitable for qualitative information and also takes into account user preferences. Furthermore, we designed an incremental architecture for addressing dynamic database problems of rough set-based rule induction, making it unnecessary to re-compute the whole dataset when the database is updated. As a result, huge degrees of computation time and memory space are saved when executing IRSRIA. Finally, we apply our method to a case study of a cell phone purchase. The results show the practical viability and efficiency of this method, and thus this paper forms the basis for solving many other similar problems that occur in the service industry.",No
53e9b388b7602d9703e6db85,An agent model for incremental rough set-based rule induction in customer relationship management,53f43518dabfaee4dc777a60,Ching-Chin Chern,Demo Paper,None,S25028,HAIS (1),2012,Journal,7208,RV60817,Thomas Gonzales,RV76158,Michael Brown,H34714,befwfwnwyf,Editor,AR46933,qualitative information,R51615,Rejected,"Compared to other methods, rough set (RS) has the advantage of combining both qualitative and quantitative information in decision analysis, which is extremely important for customer relationship management (CRM). In this paper, we introduce an application of a multi-agent embedded incremental rough set-based rule induction to CRM, namely Incremental Rough Set-based Rule Induction Agent (IRSRIA). The rule induction is based on creating agents within the main modeling processes. This method is suitable for qualitative information and also takes into account user preferences. Furthermore, we designed an incremental architecture for addressing dynamic database problems of rough set-based rule induction, making it unnecessary to re-compute the whole dataset when the database is updated. As a result, huge degrees of computation time and memory space are saved when executing IRSRIA. Finally, we apply our method to a case study of a cell phone purchase. The results show the practical viability and efficiency of this method, and thus this paper forms the basis for solving many other similar problems that occur in the service industry.",No
53e9b38eb7602d9703e6dd19,Finite sets of words and computing,53f4cd31dabfaeed22f804e8,Juhani Karhumäki,Demo Paper,None,S21870,MCU,2004,Journal,3354,RV68576,Robert Lee,RV71615,Kyle Hall,H32865,ssvxbqrvio,Editor,AR40158,powerful computation,R55317,Accepted,"We discuss about two recent undecidability results in formal language theory. The corresponding problems are very simply formulated questions on finite sets of words. In particular, these results underline how finite sets of words can be used to perform powerful computations.",MCU 3354
53e9b38eb7602d9703e6dd36,CPlanT: An Acquaintance Model-Based Coalition Formation Multi-agent System,53f45c75dabfaee2a1d8a37e,Michal Pechoucek,Demo Paper,None,S24201,CEEMAS,2001,Journal,2296,RV64124,Brandy Joseph,RV72770,Jessica Martinez DDS,H31959,baslzpjtuu,Editor,AR41289,coalition formation,R52226,Accepted,"This article will describe the CPlanT multi-agent system that has been designed in order to suggest and test alternative approaches to coalition formation. The system has been implemented for planning the Operations Other Than War (OOTW) coalition, which is the domain with a strong emphasis put on maintaining privacy of communicated information among collaborating actors - agents and important level of autonomy of their decision making. The paper presents a concept of an acquaintance model as computational models of agents' mutual awareness. In the acquaintance model the agents administer and maintain different types social knowledge that are exploited in the coalition formation process.",CEEMAS 2296
53e9b38eb7602d9703e6dd36,CPlanT: An Acquaintance Model-Based Coalition Formation Multi-agent System,544081f9dabfae805a6c85a4,Vladimír Marík,Demo Paper,None,S24201,CEEMAS,2001,Journal,2296,RV64124,Brandy Joseph,RV72770,Jessica Martinez DDS,H31959,baslzpjtuu,Editor,AR41289,coalition formation,R52226,Accepted,"This article will describe the CPlanT multi-agent system that has been designed in order to suggest and test alternative approaches to coalition formation. The system has been implemented for planning the Operations Other Than War (OOTW) coalition, which is the domain with a strong emphasis put on maintaining privacy of communicated information among collaborating actors - agents and important level of autonomy of their decision making. The paper presents a concept of an acquaintance model as computational models of agents' mutual awareness. In the acquaintance model the agents administer and maintain different types social knowledge that are exploited in the coalition formation process.",CEEMAS 2296
53e9b38eb7602d9703e6dd36,CPlanT: An Acquaintance Model-Based Coalition Formation Multi-agent System,53f430cbdabfaedf43541b87,Jaroslav Barta,Demo Paper,None,S24201,CEEMAS,2001,Journal,2296,RV64124,Brandy Joseph,RV72770,Jessica Martinez DDS,H31959,baslzpjtuu,Editor,AR41289,coalition formation,R52226,Accepted,"This article will describe the CPlanT multi-agent system that has been designed in order to suggest and test alternative approaches to coalition formation. The system has been implemented for planning the Operations Other Than War (OOTW) coalition, which is the domain with a strong emphasis put on maintaining privacy of communicated information among collaborating actors - agents and important level of autonomy of their decision making. The paper presents a concept of an acquaintance model as computational models of agents' mutual awareness. In the acquaintance model the agents administer and maintain different types social knowledge that are exploited in the coalition formation process.",CEEMAS 2296
53e9b38eb7602d9703e6dd6a,WMPI - Message Passing Interface for Win32 Clusters,53f434badabfaedd74d9129f,José Marinho,Demo Paper,None,S22018,PVM/MPI,1998,Journal,1497,RV66330,Andrew Chavez,RV70030,Terry Martin,H36130,jjapwucudm,Editor,AR45023,win32 clusters,R52693,Accepted,"This paper describes WMPI1, the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface are presented, along with some performance test results (for release v1.1), that evaluate how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIX® machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.",PVM/MPI 1497
53e9b38eb7602d9703e6dd6a,WMPI - Message Passing Interface for Win32 Clusters,548573b5dabfae8a11fb29f0,João Gabriel Silva,Demo Paper,None,S22018,PVM/MPI,1998,Journal,1497,RV66330,Andrew Chavez,RV70030,Terry Martin,H36130,jjapwucudm,Editor,AR45023,win32 clusters,R52693,Accepted,"This paper describes WMPI1, the first full implementation of the Message Passing Interface standard (MPI) for clusters of Microsoft's Windows platforms (Win32). Its internal architecture and user interface are presented, along with some performance test results (for release v1.1), that evaluate how much of the total underlying system capacity for communication is delivered to the MPI based parallel applications. WMPI is based on MPICH, a portable implementation of the MPI standard for UNIX® machines from the Argonne National Laboratory and, even when performance requisites cannot be satisfied, it is a useful tool for application developing, teaching and training. WMPI processes are also compatible with MPICH processes running on Unix workstations.",PVM/MPI 1497
53e9b38eb7602d9703e6dbef,Wavelet-Based Image Compression on the Reconfigurable Computer ACE-V,53f4ce05dabfaeed1df8135c,Hagen Gädke,Poster,Regular Conference,S25518,Lecture Notes in Computer Science,2004,Conference,3203,RV60538,Charlotte Stark,RV73972,Craig Welch,H37967,rmicsqreft,Chair,AR41292,image compression,R56307,Accepted,"Wavelet-based image compression has been suggested previously as a means to evaluate and compare both traditional and reconfigurable computers in terms of performance and resource requirements. We present a reconfigurable implementation of such an application that not only achieves a performance comparable to that of recent CPUs, but does so at a fraction of their power consumption.",Lecture Notes in Computer Science 3203
53e9b38eb7602d9703e6dbef,Wavelet-Based Image Compression on the Reconfigurable Computer ACE-V,5440e4cadabfae805a7057d8,Andreas Koch,Poster,Regular Conference,S25518,Lecture Notes in Computer Science,2004,Conference,3203,RV60538,Charlotte Stark,RV73972,Craig Welch,H37967,rmicsqreft,Chair,AR41292,image compression,R56307,Accepted,"Wavelet-based image compression has been suggested previously as a means to evaluate and compare both traditional and reconfigurable computers in terms of performance and resource requirements. We present a reconfigurable implementation of such an application that not only achieves a performance comparable to that of recent CPUs, but does so at a fraction of their power consumption.",Lecture Notes in Computer Science 3203
53e9b38eb7602d9703e6dd96,Adding Trust to P2P Distribution of Paid Content,53f383dedabfae4b34a065c2,Alex Sherman,Demo Paper,Expert Group,S20058,ISC,2009,Conference,5735,RV69100,Jennifer Craig,RV76509,Daniel Carrillo,H35016,djzxmapbse,Chair,AR42723,p2p distribution,R51163,Accepted,"While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.",ISC 5735
53e9b38eb7602d9703e6dd96,Adding Trust to P2P Distribution of Paid Content,5487393adabfae9b401343b0,Angelos Stavrou,Demo Paper,Expert Group,S20058,ISC,2009,Conference,5735,RV69100,Jennifer Craig,RV76509,Daniel Carrillo,H35016,djzxmapbse,Chair,AR42723,p2p distribution,R51163,Accepted,"While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.",ISC 5735
53e9b38eb7602d9703e6dd96,Adding Trust to P2P Distribution of Paid Content,5489ceaddabfae8a11fb480e,Jason Nieh,Demo Paper,Expert Group,S20058,ISC,2009,Conference,5735,RV69100,Jennifer Craig,RV76509,Daniel Carrillo,H35016,djzxmapbse,Chair,AR42723,p2p distribution,R51163,Accepted,"While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.",ISC 5735
53e9b38eb7602d9703e6dd96,Adding Trust to P2P Distribution of Paid Content,53f4ab34dabfaeb22f575ac2,Angelos D. Keromytis,Demo Paper,Expert Group,S20058,ISC,2009,Conference,5735,RV69100,Jennifer Craig,RV76509,Daniel Carrillo,H35016,djzxmapbse,Chair,AR42723,p2p distribution,R51163,Accepted,"While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.",ISC 5735
53e9b38eb7602d9703e6dd96,Adding Trust to P2P Distribution of Paid Content,5602764645cedb3395fa6ba5,Cliff Stein,Demo Paper,Expert Group,S20058,ISC,2009,Conference,5735,RV69100,Jennifer Craig,RV76509,Daniel Carrillo,H35016,djzxmapbse,Chair,AR42723,p2p distribution,R51163,Accepted,"While peer-to-peer (P2P) file-sharing is a powerful and cost-effective content distribution model, most paid-for digital-content providers (CPs) use direct download to deliver their content. CPs are hesitant to rely on a P2P distribution model because it introduces a number of security concerns including content pollution by malicious peers, and lack of enforcement of authorized downloads. Furthermore, because users communicate directly with one another, the users can easily form illegal file-sharing clusters to exchange copyrighted content. Such exchange could hurt the content providers' profits. We present a P2P system TP2P, where we introduce a notion of trusted auditors (TAs). TAs are P2P peers that police the system by covertly monitoring and taking measures against misbehaving peers. This policing allows TP2P to enable a stronger security model making P2P a viable alternative for the distribution of paid digital content. Through analysis and simulation, we show the effectiveness of even a small number of TAs at policing the system. In a system with as many as 60% of misbehaving users, even a small number of TAs can detect 99% of illegal cluster formation. We develop a simple economic model to show that even with such a large presence of malicious nodes, TP2P can improve CP's profits (which could translate to user savings) by 62% to 122%, even while assuming conservative estimates of content and bandwidth costs. We implemented TP2P as a layer on top of BitTorrent and demonstrated experimentally using PlanetLab that our system provides trusted P2P file sharing with negligible performance overhead.",ISC 5735
53e9b38eb7602d9703e6dc12,Application of CLIPS Expert System to Malware Detection System,53f4678bdabfaedf4364aa6d,Ruili Zhou,Full Paper,None,S27755,CIS (1),2008,Journal,1,RV63559,Patricia Diaz,RV72639,Amy Sanders,H30396,kdhsfqvdzb,Editor,AR49534,expert system,R57600,Accepted,"Malware detection is a crucial aspect of software security. Traditional signature-based detection method cannot detect zero-day attacks and some malware adopting some circumvention techniques such as polymorphic, metamorphic, obfuscation and packer. So some anomaly-based detection techniques are introduced to overcome this drawback, but these techniques have high false alarm rate and the complexity involved in determining what features should be learned in the training phase. In order to overcome these shortcomings, we propose a malware detection system based on expert systems in this paper. This system integrates signature-based analysis and anomaly-detection technique together. The signature is anomaly behavioral signatures. Accord to expertise about malware’s major suspicious behaviors, we build the knowledge base of the expert system. And we design a behavior gathering component to intercept anomaly behaviors happened in the operating system and get significant traces leaved by malware, then present these behaviors and traces as facts. The expert system uses the knowledge base and behaviors facts to infer and give the results. This system can detect not only known malware, but some zero-day attacks using known techniques and also malware adopting low-level techniques, such as polymorphic and packer.",CIS (1) 1
53e9b38eb7602d9703e6dc12,Application of CLIPS Expert System to Malware Detection System,53f45b01dabfaedf4361caf7,Jianfeng Pan,Full Paper,None,S27755,CIS (1),2008,Journal,1,RV63559,Patricia Diaz,RV72639,Amy Sanders,H30396,kdhsfqvdzb,Editor,AR49534,expert system,R57600,Accepted,"Malware detection is a crucial aspect of software security. Traditional signature-based detection method cannot detect zero-day attacks and some malware adopting some circumvention techniques such as polymorphic, metamorphic, obfuscation and packer. So some anomaly-based detection techniques are introduced to overcome this drawback, but these techniques have high false alarm rate and the complexity involved in determining what features should be learned in the training phase. In order to overcome these shortcomings, we propose a malware detection system based on expert systems in this paper. This system integrates signature-based analysis and anomaly-detection technique together. The signature is anomaly behavioral signatures. Accord to expertise about malware’s major suspicious behaviors, we build the knowledge base of the expert system. And we design a behavior gathering component to intercept anomaly behaviors happened in the operating system and get significant traces leaved by malware, then present these behaviors and traces as facts. The expert system uses the knowledge base and behaviors facts to infer and give the results. This system can detect not only known malware, but some zero-day attacks using known techniques and also malware adopting low-level techniques, such as polymorphic and packer.",CIS (1) 1
53e9b38eb7602d9703e6dc12,Application of CLIPS Expert System to Malware Detection System,53f44b98dabfaeee22a0bf5f,Xiaobin Tan,Full Paper,None,S27755,CIS (1),2008,Journal,1,RV63559,Patricia Diaz,RV72639,Amy Sanders,H30396,kdhsfqvdzb,Editor,AR49534,expert system,R57600,Accepted,"Malware detection is a crucial aspect of software security. Traditional signature-based detection method cannot detect zero-day attacks and some malware adopting some circumvention techniques such as polymorphic, metamorphic, obfuscation and packer. So some anomaly-based detection techniques are introduced to overcome this drawback, but these techniques have high false alarm rate and the complexity involved in determining what features should be learned in the training phase. In order to overcome these shortcomings, we propose a malware detection system based on expert systems in this paper. This system integrates signature-based analysis and anomaly-detection technique together. The signature is anomaly behavioral signatures. Accord to expertise about malware’s major suspicious behaviors, we build the knowledge base of the expert system. And we design a behavior gathering component to intercept anomaly behaviors happened in the operating system and get significant traces leaved by malware, then present these behaviors and traces as facts. The expert system uses the knowledge base and behaviors facts to infer and give the results. This system can detect not only known malware, but some zero-day attacks using known techniques and also malware adopting low-level techniques, such as polymorphic and packer.",CIS (1) 1
53e9b38eb7602d9703e6dc12,Application of CLIPS Expert System to Malware Detection System,54410d39dabfae805a723109,Hongsheng Xi,Full Paper,None,S27755,CIS (1),2008,Journal,1,RV63559,Patricia Diaz,RV72639,Amy Sanders,H30396,kdhsfqvdzb,Editor,AR49534,expert system,R57600,Accepted,"Malware detection is a crucial aspect of software security. Traditional signature-based detection method cannot detect zero-day attacks and some malware adopting some circumvention techniques such as polymorphic, metamorphic, obfuscation and packer. So some anomaly-based detection techniques are introduced to overcome this drawback, but these techniques have high false alarm rate and the complexity involved in determining what features should be learned in the training phase. In order to overcome these shortcomings, we propose a malware detection system based on expert systems in this paper. This system integrates signature-based analysis and anomaly-detection technique together. The signature is anomaly behavioral signatures. Accord to expertise about malware’s major suspicious behaviors, we build the knowledge base of the expert system. And we design a behavior gathering component to intercept anomaly behaviors happened in the operating system and get significant traces leaved by malware, then present these behaviors and traces as facts. The expert system uses the knowledge base and behaviors facts to infer and give the results. This system can detect not only known malware, but some zero-day attacks using known techniques and also malware adopting low-level techniques, such as polymorphic and packer.",CIS (1) 1
53e9b38eb7602d9703e6de3d,Moving cast shadows detection based on ratio edge,53f4b8f7dabfaed83b77b46e,Wei Zhang,Short Paper,Workshop,S26163,ICPR (4),2006,Conference,4,RV60379,Logan Stevens,RV79933,Bruce Dunn,H32904,vbbckjkjmr,Chair,AR48040,physical model,R57585,Rejected,"In this paper we propose a novel method for moving cast shadows detection. Based on the analysis to the physical model of moving shadows, we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each image pixel into foreground object or moving shadow. Experimental results on typical scenes show that the proposed method can detect moving shadows robustly.",No
53e9b38eb7602d9703e6de3d,Moving cast shadows detection based on ratio edge,53f4383cdabfaee2a1cf7a4a,Xiangzhong Fang,Short Paper,Workshop,S26163,ICPR (4),2006,Conference,4,RV60379,Logan Stevens,RV79933,Bruce Dunn,H32904,vbbckjkjmr,Chair,AR48040,physical model,R57585,Rejected,"In this paper we propose a novel method for moving cast shadows detection. Based on the analysis to the physical model of moving shadows, we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each image pixel into foreground object or moving shadow. Experimental results on typical scenes show that the proposed method can detect moving shadows robustly.",No
53e9b38eb7602d9703e6de3d,Moving cast shadows detection based on ratio edge,542a9df6dabfae646d5782d2,Xiaokang Yang,Short Paper,Workshop,S26163,ICPR (4),2006,Conference,4,RV60379,Logan Stevens,RV79933,Bruce Dunn,H32904,vbbckjkjmr,Chair,AR48040,physical model,R57585,Rejected,"In this paper we propose a novel method for moving cast shadows detection. Based on the analysis to the physical model of moving shadows, we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each image pixel into foreground object or moving shadow. Experimental results on typical scenes show that the proposed method can detect moving shadows robustly.",No
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f42fe2dabfaedf435360ac,M. Domínguez-Morales,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f42e8edabfaedf43525d0d,A. Jimenez-Fernandez,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,54341d5ddabfaebba5842e57,R. Paz,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f45d82dabfaedd74e47e06,M. R. López-Torres,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f4391adabfaefedbae2c16,E. Cerezuela-Escudero,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f42fc4dabfaee4dc737419,A. Linares-Barranco,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,53f477d5dabfaee43ed3ab4c,G. Jimenez-Moreno,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6de50,An approach to distance estimation with stereo vision using address-event-representation,543347c2dabfaeb4c6ab4843,A. Morgado,Full Paper,None,S21702,ICONIP (1),2011,Journal,7062,RV69135,Ashley Scott,RV77487,Jeremiah Brown,H36310,wmhfauxpht,Editor,AR42256,distance estimation,R59788,Accepted,"Image processing in digital computer systems usually considers the visual information as a sequence of frames. These frames are from cameras that capture reality for a short period of time. They are renewed and transmitted at a rate of 25-30 fps (typical real-time scenario). Digital video processing has to process each frame in order to obtain a result or detect a feature. In stereo vision, existing algorithms used for distance estimation use frames from two digital cameras and process them pixel by pixel to obtain similarities and differences from both frames; after that, depending on the scene and the features extracted, an estimate of the distance of the different objects of the scene is calculated. Spike-based processing is a relatively new approach that implements the processing by manipulating spikes one by one at the time they are transmitted, like a human brain. The mammal nervous system is able to solve much more complex problems, such as visual recognition by manipulating neuron spikes. The spike-based philosophy for visual information processing based on the neuro-inspired Address-Event-Representation (AER) is achieving nowadays very high performances. In this work we propose a two-DVS-retina system, composed of other elements in a chain, which allow us to obtain a distance estimation of the moving objects in a close environment. We will analyze each element of this chain and propose a Multi Hold&Fire algorithm that obtains the differences between both retinas.",ICONIP (1) 7062
53e9b38eb7602d9703e6deee,Frontiers in surface-based microwave and millimeter wavelength radiometry,53f43fbddabfaec09f1b7df7,Ed R. Westwater,Full Paper,None,S22195,"Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International",2004,Journal,2,RV68550,Keith Sandoval,RV72604,Robert Newman,H39358,edwdvnunfu,Editor,AR41053,atmospheric humidity,R56636,Accepted,"Surface-based radiometric sensing of atmospheric parameters has a long history of providing useful measurements of temperature, water vapor, and cloud liquid. In this Special Tributary Session to Professor Calvin Swift, several contemporary instruments are discussed and representative results are presented. Recent and promising developments include new absorption models, improved retrieval techniques, multifrequency radiometers, scanning observations of clouds, and combined active-passive remote sensing","Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International 2"
53e9b38eb7602d9703e6deee,Frontiers in surface-based microwave and millimeter wavelength radiometry,540fd6eddabfae450f4ab643,Susanne Crewell,Full Paper,None,S22195,"Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International",2004,Journal,2,RV68550,Keith Sandoval,RV72604,Robert Newman,H39358,edwdvnunfu,Editor,AR41053,atmospheric humidity,R56636,Accepted,"Surface-based radiometric sensing of atmospheric parameters has a long history of providing useful measurements of temperature, water vapor, and cloud liquid. In this Special Tributary Session to Professor Calvin Swift, several contemporary instruments are discussed and representative results are presented. Recent and promising developments include new absorption models, improved retrieval techniques, multifrequency radiometers, scanning observations of clouds, and combined active-passive remote sensing","Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International 2"
53e9b38eb7602d9703e6deee,Frontiers in surface-based microwave and millimeter wavelength radiometry,53f44c61dabfaee0d9bcbfe2,Christian Mätzler,Full Paper,None,S22195,"Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International",2004,Journal,2,RV68550,Keith Sandoval,RV72604,Robert Newman,H39358,edwdvnunfu,Editor,AR41053,atmospheric humidity,R56636,Accepted,"Surface-based radiometric sensing of atmospheric parameters has a long history of providing useful measurements of temperature, water vapor, and cloud liquid. In this Special Tributary Session to Professor Calvin Swift, several contemporary instruments are discussed and representative results are presented. Recent and promising developments include new absorption models, improved retrieval techniques, multifrequency radiometers, scanning observations of clouds, and combined active-passive remote sensing","Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings. 2004 IEEE International 2"
53e9b38eb7602d9703e6df51,The DYPSA algorithm for estimation of glottal closure instants in voiced speech.,53f4421bdabfaefedbb07658,Anastasis Kounoudes,Short Paper,None,S28322,ICASSP,2002,Journal,1,RV67051,Christopher Watson,RV72417,Tonya Hendrix,H30447,wpyddatixq,Editor,AR44307,algorithm design and analysis,R53503,Rejected,"We present the DYPSA algorithm for automatic and reliable estimation of glottal closure instants (GCIs) in voiced speech. Reliable GCI estimation is essential for closed-phase speech analysis, from which can be derived features of the vocal tract and, separately, the voice source. It has been shown that such features can be used with significant advantages in applications such as speaker recognition. DYPSA is automatic and operates using the speech signal alone without the need for an EGG or Laryngograph signal. It incorporates a new technique for estimating GCI candidates and employs dynamic programming to select the most likely candidates according to a defined cost function. We review and evaluate three existing methods and compare our new algorithm to them. Results for DYPSA show GCI detection accuracy to within ±0.25ms on 87% of the test database and fewer than 1% false alarms and misses.",No
53e9b38eb7602d9703e6df51,The DYPSA algorithm for estimation of glottal closure instants in voiced speech.,54480b0adabfae87b7dca069,Patrick A. Naylor,Short Paper,None,S28322,ICASSP,2002,Journal,1,RV67051,Christopher Watson,RV72417,Tonya Hendrix,H30447,wpyddatixq,Editor,AR44307,algorithm design and analysis,R53503,Rejected,"We present the DYPSA algorithm for automatic and reliable estimation of glottal closure instants (GCIs) in voiced speech. Reliable GCI estimation is essential for closed-phase speech analysis, from which can be derived features of the vocal tract and, separately, the voice source. It has been shown that such features can be used with significant advantages in applications such as speaker recognition. DYPSA is automatic and operates using the speech signal alone without the need for an EGG or Laryngograph signal. It incorporates a new technique for estimating GCI candidates and employs dynamic programming to select the most likely candidates according to a defined cost function. We review and evaluate three existing methods and compare our new algorithm to them. Results for DYPSA show GCI detection accuracy to within ±0.25ms on 87% of the test database and fewer than 1% false alarms and misses.",No
53e9b38eb7602d9703e6df51,The DYPSA algorithm for estimation of glottal closure instants in voiced speech.,54082c6adabfae8faa62cde4,Mike Brookes,Short Paper,None,S28322,ICASSP,2002,Journal,1,RV67051,Christopher Watson,RV72417,Tonya Hendrix,H30447,wpyddatixq,Editor,AR44307,algorithm design and analysis,R53503,Rejected,"We present the DYPSA algorithm for automatic and reliable estimation of glottal closure instants (GCIs) in voiced speech. Reliable GCI estimation is essential for closed-phase speech analysis, from which can be derived features of the vocal tract and, separately, the voice source. It has been shown that such features can be used with significant advantages in applications such as speaker recognition. DYPSA is automatic and operates using the speech signal alone without the need for an EGG or Laryngograph signal. It incorporates a new technique for estimating GCI candidates and employs dynamic programming to select the most likely candidates according to a defined cost function. We review and evaluate three existing methods and compare our new algorithm to them. Results for DYPSA show GCI detection accuracy to within ±0.25ms on 87% of the test database and fewer than 1% false alarms and misses.",No
53e9b38eb7602d9703e6e681,A work-stealing scheduler for X10's task parallelism with suspension,53f472cddabfaeecd6a3c3ed,Olivier Tardieu,Demo Paper,Symposium,S22711,PPOPP,2012,Conference,47,RV66527,Edwin Miller,RV77117,Rachel Hartman,H36892,fjacnorkdf,Chair,AR47026,x10 language,R58395,Accepted,"The X10 programming language is intended to ease the programming of scalable concurrent and distributed applications. X10 augments a familiar imperative object-oriented programming model with constructs to support light-weight asynchronous tasks as well as execution across multiple address spaces. A crucial aspect of X10's runtime system is the scheduling of concurrent tasks. Work-stealing schedulers have been shown to efficiently load balance fine-grain divide-and-conquer task-parallel program on SMPs and multicores. But X10 is not limited to shared-memory fork-join parallelism. X10 permits tasks to suspend and synchronize by means of conditional atomic blocks and remote task invocations. In this paper, we demonstrate that work-stealing scheduling principles are applicable to a rich programming language such as X10, achieving performance at scale without compromising expressivity, ease of use, or portability. We design and implement a portable work-stealing execution engine for X10. While this engine is biased toward the efficient execution of fork-join parallelism in shared memory, it handles the full X10 language, especially conditional atomic blocks and distribution. We show that this engine improves the run time of a series of benchmark programs by several orders of magnitude when used in combination with the C++ backend compiler and runtime for X10. It achieves scaling comparable to state-of-the art work-stealing scheduler implementations---the Cilk++ compiler and the Java fork/join framework---despite the dramatic increase in generality.",PPOPP 47
53e9b38eb7602d9703e6e681,A work-stealing scheduler for X10's task parallelism with suspension,5429c05edabfaec7081b0b1c,Haichuan Wang,Demo Paper,Symposium,S22711,PPOPP,2012,Conference,47,RV66527,Edwin Miller,RV77117,Rachel Hartman,H36892,fjacnorkdf,Chair,AR47026,x10 language,R58395,Accepted,"The X10 programming language is intended to ease the programming of scalable concurrent and distributed applications. X10 augments a familiar imperative object-oriented programming model with constructs to support light-weight asynchronous tasks as well as execution across multiple address spaces. A crucial aspect of X10's runtime system is the scheduling of concurrent tasks. Work-stealing schedulers have been shown to efficiently load balance fine-grain divide-and-conquer task-parallel program on SMPs and multicores. But X10 is not limited to shared-memory fork-join parallelism. X10 permits tasks to suspend and synchronize by means of conditional atomic blocks and remote task invocations. In this paper, we demonstrate that work-stealing scheduling principles are applicable to a rich programming language such as X10, achieving performance at scale without compromising expressivity, ease of use, or portability. We design and implement a portable work-stealing execution engine for X10. While this engine is biased toward the efficient execution of fork-join parallelism in shared memory, it handles the full X10 language, especially conditional atomic blocks and distribution. We show that this engine improves the run time of a series of benchmark programs by several orders of magnitude when used in combination with the C++ backend compiler and runtime for X10. It achieves scaling comparable to state-of-the art work-stealing scheduler implementations---the Cilk++ compiler and the Java fork/join framework---despite the dramatic increase in generality.",PPOPP 47
53e9b38eb7602d9703e6e681,A work-stealing scheduler for X10's task parallelism with suspension,542df5b1dabfae11fc4b64a3,Haibo Lin,Demo Paper,Symposium,S22711,PPOPP,2012,Conference,47,RV66527,Edwin Miller,RV77117,Rachel Hartman,H36892,fjacnorkdf,Chair,AR47026,x10 language,R58395,Accepted,"The X10 programming language is intended to ease the programming of scalable concurrent and distributed applications. X10 augments a familiar imperative object-oriented programming model with constructs to support light-weight asynchronous tasks as well as execution across multiple address spaces. A crucial aspect of X10's runtime system is the scheduling of concurrent tasks. Work-stealing schedulers have been shown to efficiently load balance fine-grain divide-and-conquer task-parallel program on SMPs and multicores. But X10 is not limited to shared-memory fork-join parallelism. X10 permits tasks to suspend and synchronize by means of conditional atomic blocks and remote task invocations. In this paper, we demonstrate that work-stealing scheduling principles are applicable to a rich programming language such as X10, achieving performance at scale without compromising expressivity, ease of use, or portability. We design and implement a portable work-stealing execution engine for X10. While this engine is biased toward the efficient execution of fork-join parallelism in shared memory, it handles the full X10 language, especially conditional atomic blocks and distribution. We show that this engine improves the run time of a series of benchmark programs by several orders of magnitude when used in combination with the C++ backend compiler and runtime for X10. It achieves scaling comparable to state-of-the art work-stealing scheduler implementations---the Cilk++ compiler and the Java fork/join framework---despite the dramatic increase in generality.",PPOPP 47
53e9b38eb7602d9703e6e6ab,Color-Based 3D Model Classification Using Hopfield Neural Network,5609327845cedb3396e4f9a5,Wei Wei,Short Paper,None,S23257,CSSE (1),2008,Journal,1,RV60449,Deborah Burton,RV77233,Michael Moran,H32700,gvejdslmfl,Editor,AR41153,content-based 3d model retrieval system,R50670,Accepted,"With the increasing amount and extensive usage of 3D models in many areas, there is a demanding need for developing 3D models retrieval systems. Because of the importance of appearance properties in content-based 3D model retrieval system, the paper presents a 3D model classification method based on color information. Firstly, the system architecture is introduced. Next, the proposed method that adopts Hopfield neural network to analyze the color features of 3D models is presented. Experimental results of classifying 3D VRML model using the proposed method are then analyzed and have shown the effectiveness of our method.",CSSE (1) 1
53e9b38eb7602d9703e6e6ab,Color-Based 3D Model Classification Using Hopfield Neural Network,542e0c0fdabfae11fc4bf348,Yubin Yang,Short Paper,None,S23257,CSSE (1),2008,Journal,1,RV60449,Deborah Burton,RV77233,Michael Moran,H32700,gvejdslmfl,Editor,AR41153,content-based 3d model retrieval system,R50670,Accepted,"With the increasing amount and extensive usage of 3D models in many areas, there is a demanding need for developing 3D models retrieval systems. Because of the importance of appearance properties in content-based 3D model retrieval system, the paper presents a 3D model classification method based on color information. Firstly, the system architecture is introduced. Next, the proposed method that adopts Hopfield neural network to analyze the color features of 3D models is presented. Experimental results of classifying 3D VRML model using the proposed method are then analyzed and have shown the effectiveness of our method.",CSSE (1) 1
53e9b38eb7602d9703e6e6ab,Color-Based 3D Model Classification Using Hopfield Neural Network,53f42b49dabfaedd74d15cc5,Jinjie Lin,Short Paper,None,S23257,CSSE (1),2008,Journal,1,RV60449,Deborah Burton,RV77233,Michael Moran,H32700,gvejdslmfl,Editor,AR41153,content-based 3d model retrieval system,R50670,Accepted,"With the increasing amount and extensive usage of 3D models in many areas, there is a demanding need for developing 3D models retrieval systems. Because of the importance of appearance properties in content-based 3D model retrieval system, the paper presents a 3D model classification method based on color information. Firstly, the system architecture is introduced. Next, the proposed method that adopts Hopfield neural network to analyze the color features of 3D models is presented. Experimental results of classifying 3D VRML model using the proposed method are then analyzed and have shown the effectiveness of our method.",CSSE (1) 1
53e9b38eb7602d9703e6e6ab,Color-Based 3D Model Classification Using Hopfield Neural Network,53f4534adabfaee4dc80598d,Jiabin Ruan,Short Paper,None,S23257,CSSE (1),2008,Journal,1,RV60449,Deborah Burton,RV77233,Michael Moran,H32700,gvejdslmfl,Editor,AR41153,content-based 3d model retrieval system,R50670,Accepted,"With the increasing amount and extensive usage of 3D models in many areas, there is a demanding need for developing 3D models retrieval systems. Because of the importance of appearance properties in content-based 3D model retrieval system, the paper presents a 3D model classification method based on color information. Firstly, the system architecture is introduced. Next, the proposed method that adopts Hopfield neural network to analyze the color features of 3D models is presented. Experimental results of classifying 3D VRML model using the proposed method are then analyzed and have shown the effectiveness of our method.",CSSE (1) 1
53e9b38eb7602d9703e6e5f2,A framework for query reformulation between knowledge base peers,560f40c745ce1e5961a797a7,Biao Qin,Short Paper,None,S21390,WAIM,2006,Journal,4016,RV67374,Antonio Bender,RV79322,Stephanie Mitchell,H38579,wnfakgkdoe,Editor,AR44119,class connector,R51557,Rejected,"The problem of sharing data in peer-to-peer environment has received considerable attention in recent years. However, knowledge sharing in peer architectures has received very little attention. This paper proposes a framework for query reformulation in peer architectures. We first consider a mapping language based on a particular description logic that includes class connectors. Then a set of rules are proposed for building graphs. Because the axioms in a knowledge base have different properties, our graph generation algorithm classifies the generated graphs into four sets (Ugraph, Bgraph, Cgraph and Dgraph). Furthermore, based on the properties of the unification nodes, our algorithms can reformulate each kind of atom in a special way. Finally we do extensive simulation experiments and simulation results show that the proposed method has better performance than those of Mork's [8].",No
53e9b38eb7602d9703e6e5f2,A framework for query reformulation between knowledge base peers,5429999fdabfaec708198c48,Shan Wang,Short Paper,None,S21390,WAIM,2006,Journal,4016,RV67374,Antonio Bender,RV79322,Stephanie Mitchell,H38579,wnfakgkdoe,Editor,AR44119,class connector,R51557,Rejected,"The problem of sharing data in peer-to-peer environment has received considerable attention in recent years. However, knowledge sharing in peer architectures has received very little attention. This paper proposes a framework for query reformulation in peer architectures. We first consider a mapping language based on a particular description logic that includes class connectors. Then a set of rules are proposed for building graphs. Because the axioms in a knowledge base have different properties, our graph generation algorithm classifies the generated graphs into four sets (Ugraph, Bgraph, Cgraph and Dgraph). Furthermore, based on the properties of the unification nodes, our algorithms can reformulate each kind of atom in a special way. Finally we do extensive simulation experiments and simulation results show that the proposed method has better performance than those of Mork's [8].",No
53e9b38eb7602d9703e6e5f2,A framework for query reformulation between knowledge base peers,542a549adabfae646d5510c6,Xiaoyong Du,Short Paper,None,S21390,WAIM,2006,Journal,4016,RV67374,Antonio Bender,RV79322,Stephanie Mitchell,H38579,wnfakgkdoe,Editor,AR44119,class connector,R51557,Rejected,"The problem of sharing data in peer-to-peer environment has received considerable attention in recent years. However, knowledge sharing in peer architectures has received very little attention. This paper proposes a framework for query reformulation in peer architectures. We first consider a mapping language based on a particular description logic that includes class connectors. Then a set of rules are proposed for building graphs. Because the axioms in a knowledge base have different properties, our graph generation algorithm classifies the generated graphs into four sets (Ugraph, Bgraph, Cgraph and Dgraph). Furthermore, based on the properties of the unification nodes, our algorithms can reformulate each kind of atom in a special way. Finally we do extensive simulation experiments and simulation results show that the proposed method has better performance than those of Mork's [8].",No
53e9b38eb7602d9703e6e6ea,Decision Making Technique of Processing Technology.,53f42ba3dabfaeb2acfceeaa,Qing-hua Zou,Full Paper,None,S28487,Communications in Computer and Information Science,2011,Journal,224,RV62432,Holly Carpenter,RV75064,Taylor Hickman,H32852,hvodfkxsaq,Editor,AR46769,technology,R53710,Accepted,"Has discussed Decision Making Technique of Processing Technology(DMTPT), apply the theories of both value engineering and operations research, having obtained optimal design indexes for either individual or comprehensive processing technology. The key computer program of Hungary's algorithm for optimal design of technology has been given.",Communications in Computer and Information Science 224
53e9b38eb7602d9703e6e627,Deciding monadic theories of hyperalgebraic trees,53f46a7fdabfaeee22a62efd,Teodor Knapik,Full Paper,Regular Conference,S29459,TLCA,2001,Conference,2044,RV63410,Tiffany Richardson,RV74873,Kathryn Douglas,H39178,tdxptsyvnn,Chair,AR48452,monadic theory,R51589,Rejected,"We show that the monadic second-order theory of any infinite tree generated by a higher-order grammar of level 2 subject to a certain syntactic restriction is decidable. By this we extend the result of Courcelle [6] that the MSO theory of a tree generated by a grammar of level 1 (algebraic) is decidable. To this end, we develop a technique of representing infinite trees by infinite λ-terms, in such a way that the MSO theory of a tree can be interpreted in the MSO theory of a λ-term.",No
53e9b38eb7602d9703e6e627,Deciding monadic theories of hyperalgebraic trees,53f43a69dabfaee0d9b8a7cb,Damian Niwinski,Full Paper,Regular Conference,S29459,TLCA,2001,Conference,2044,RV63410,Tiffany Richardson,RV74873,Kathryn Douglas,H39178,tdxptsyvnn,Chair,AR48452,monadic theory,R51589,Rejected,"We show that the monadic second-order theory of any infinite tree generated by a higher-order grammar of level 2 subject to a certain syntactic restriction is decidable. By this we extend the result of Courcelle [6] that the MSO theory of a tree generated by a grammar of level 1 (algebraic) is decidable. To this end, we develop a technique of representing infinite trees by infinite λ-terms, in such a way that the MSO theory of a tree can be interpreted in the MSO theory of a λ-term.",No
53e9b38eb7602d9703e6e627,Deciding monadic theories of hyperalgebraic trees,53f4388fdabfaedd74db4bc2,Paweł Urzyczyn,Full Paper,Regular Conference,S29459,TLCA,2001,Conference,2044,RV63410,Tiffany Richardson,RV74873,Kathryn Douglas,H39178,tdxptsyvnn,Chair,AR48452,monadic theory,R51589,Rejected,"We show that the monadic second-order theory of any infinite tree generated by a higher-order grammar of level 2 subject to a certain syntactic restriction is decidable. By this we extend the result of Courcelle [6] that the MSO theory of a tree generated by a grammar of level 1 (algebraic) is decidable. To this end, we develop a technique of representing infinite trees by infinite λ-terms, in such a way that the MSO theory of a tree can be interpreted in the MSO theory of a λ-term.",No
53e9b38eb7602d9703e6e62a,Formal Specification of Symbolic-Probabilistic Systems,5448ee93dabfae87b7e981a9,Natalia López,Full Paper,Expert Group,S25518,Lecture Notes in Computer Science,2004,Conference,3236,RV60011,Amanda Malone,RV75120,Michael Cervantes,H36532,xjnigbnbrv,Chair,AR40082,formal specification,R56571,Rejected,"We consider the formal specification and validation of systems where probabilistic information is not given by means of fixed values but as sets of probabilities. These sets will be intervals contained in (0, 1] indicating the possible value of the real probability. In order to specify this kind of systems we will introduce a suitable extension of the notion of finite state machine. Essentially, choices between transitions outgoing from a state and having the same input action axe probabilistically resolved. We will also present some implementation relations to assess the conformance of an implementation to a specification. The first implementation relation will clearly present the probabilistic constraints of the specification, but it will be unfeasible from the practical point of view. The other relations will overcome the problems of the first one by introducing a notion of conformance up to a given level of confidence. These relations will assess the validity of an implementation with respect to a specification by considering a finite sample of executions of the implementation and comparing it with the probabilistic constraints imposed by the specification.",No
53e9b38eb7602d9703e6e62a,Formal Specification of Symbolic-Probabilistic Systems,5405706cdabfae92b41d3b16,Manuel Núñez,Full Paper,Expert Group,S25518,Lecture Notes in Computer Science,2004,Conference,3236,RV60011,Amanda Malone,RV75120,Michael Cervantes,H36532,xjnigbnbrv,Chair,AR40082,formal specification,R56571,Rejected,"We consider the formal specification and validation of systems where probabilistic information is not given by means of fixed values but as sets of probabilities. These sets will be intervals contained in (0, 1] indicating the possible value of the real probability. In order to specify this kind of systems we will introduce a suitable extension of the notion of finite state machine. Essentially, choices between transitions outgoing from a state and having the same input action axe probabilistically resolved. We will also present some implementation relations to assess the conformance of an implementation to a specification. The first implementation relation will clearly present the probabilistic constraints of the specification, but it will be unfeasible from the practical point of view. The other relations will overcome the problems of the first one by introducing a notion of conformance up to a given level of confidence. These relations will assess the validity of an implementation with respect to a specification by considering a finite sample of executions of the implementation and comparing it with the probabilistic constraints imposed by the specification.",No
53e9b38eb7602d9703e6e62a,Formal Specification of Symbolic-Probabilistic Systems,53f4c842dabfaee57877d90a,Ismael Rodríguez,Full Paper,Expert Group,S25518,Lecture Notes in Computer Science,2004,Conference,3236,RV60011,Amanda Malone,RV75120,Michael Cervantes,H36532,xjnigbnbrv,Chair,AR40082,formal specification,R56571,Rejected,"We consider the formal specification and validation of systems where probabilistic information is not given by means of fixed values but as sets of probabilities. These sets will be intervals contained in (0, 1] indicating the possible value of the real probability. In order to specify this kind of systems we will introduce a suitable extension of the notion of finite state machine. Essentially, choices between transitions outgoing from a state and having the same input action axe probabilistically resolved. We will also present some implementation relations to assess the conformance of an implementation to a specification. The first implementation relation will clearly present the probabilistic constraints of the specification, but it will be unfeasible from the practical point of view. The other relations will overcome the problems of the first one by introducing a notion of conformance up to a given level of confidence. These relations will assess the validity of an implementation with respect to a specification by considering a finite sample of executions of the implementation and comparing it with the probabilistic constraints imposed by the specification.",No
53e9b38eb7602d9703e6eadf,Quantized Ranking for Permutation-Based Indexing.,53f78543dabfae92b40c2294,Hisham Mohamed,Short Paper,None,S27111,similarity search and applications,2013,Journal,52,RV64065,Jonathan Peters,RV75252,James Ray,H33165,uzoryeiazt,Editor,AR43593,big data,R59688,Rejected,"The K-Nearest Neighbor (K-NN) search problem is the way to find the K closest and most similar objects to a given query. The K-NN is essential for many applications such as information retrieval and visualization, machine learning and data mining. The exponential growth of data imposes to find approximate approaches to this problem. Permutation-based indexing is one of the most recent techniques for approximate similarity search. Objects are represented by permutation lists ordering their distances to a set of selected reference objects, following the idea that two neighboring objects have the same surrounding. In this paper, we propose a novel quantized representation of permutation lists with its related data structure for effective retrieval on single and multicore architectures. Our novel permutation-based indexing strategy is built to be fast, memory efficient and scalable. This is experimentally demonstrated in comparison to existing proposals using several large-scale datasets of millions of documents and of different dimensions. HighlightsA Multi-core indexing and searching implementations of our data structure.Test our proposal on the full CoPhIR dataset 106-million features.Compare our proposal to all the available permutation based indexing technique with larger datasets (1-million and 10-million).Compare our proposal to other approximate similarity search techniques like LSH-Forest and AM-Tree.",No
53e9b38eb7602d9703e6eadf,Quantized Ranking for Permutation-Based Indexing.,540544efdabfae8faa5bb0ee,Stéphane Marchand-Maillet,Short Paper,None,S27111,similarity search and applications,2013,Journal,52,RV64065,Jonathan Peters,RV75252,James Ray,H33165,uzoryeiazt,Editor,AR43593,big data,R59688,Rejected,"The K-Nearest Neighbor (K-NN) search problem is the way to find the K closest and most similar objects to a given query. The K-NN is essential for many applications such as information retrieval and visualization, machine learning and data mining. The exponential growth of data imposes to find approximate approaches to this problem. Permutation-based indexing is one of the most recent techniques for approximate similarity search. Objects are represented by permutation lists ordering their distances to a set of selected reference objects, following the idea that two neighboring objects have the same surrounding. In this paper, we propose a novel quantized representation of permutation lists with its related data structure for effective retrieval on single and multicore architectures. Our novel permutation-based indexing strategy is built to be fast, memory efficient and scalable. This is experimentally demonstrated in comparison to existing proposals using several large-scale datasets of millions of documents and of different dimensions. HighlightsA Multi-core indexing and searching implementations of our data structure.Test our proposal on the full CoPhIR dataset 106-million features.Compare our proposal to all the available permutation based indexing technique with larger datasets (1-million and 10-million).Compare our proposal to other approximate similarity search techniques like LSH-Forest and AM-Tree.",No
53e9b38eb7602d9703e6ebc1,Interventional radiology robot for CT and MRI guided percutaneous interventions.,53f45226dabfaec09f1f6996,Nikolai Hungr,Poster,Workshop,S23075,MICCAI,2011,Conference,14,RV61873,Mr. David Jenkins II,RV72327,Joshua Juarez,H39046,euzvesnssl,Chair,AR48602,dof robot,R57050,Accepted,"This paper introduces a new patient-mounted CT and MRI guided interventional radiology robot for percutaneous needle interventions. The 5 DOF robot uses ultrasonic motors and pneumatics to position the needle and then insert it progressively. The needle position and inclination can be registered in the images using two strategically placed fiducials visible in both imaging modalities. A first prototype is presented and described in terms of its sterilization, CT and MRI compatibility, and precision. Tests showed that 1) it is entirely sterilizable with hydrogen peroxide gas, 2) no image artifacts or deformations are noticeable in the CT and MRI images, 3) does not affect the SNR of MR images, and 4) its mechanical error is less than 5mm.",MICCAI 14
53e9b38eb7602d9703e6ebc1,Interventional radiology robot for CT and MRI guided percutaneous interventions.,53f4670ddabfaee2a1db15bd,Céline Fouard,Poster,Workshop,S23075,MICCAI,2011,Conference,14,RV61873,Mr. David Jenkins II,RV72327,Joshua Juarez,H39046,euzvesnssl,Chair,AR48602,dof robot,R57050,Accepted,"This paper introduces a new patient-mounted CT and MRI guided interventional radiology robot for percutaneous needle interventions. The 5 DOF robot uses ultrasonic motors and pneumatics to position the needle and then insert it progressively. The needle position and inclination can be registered in the images using two strategically placed fiducials visible in both imaging modalities. A first prototype is presented and described in terms of its sterilization, CT and MRI compatibility, and precision. Tests showed that 1) it is entirely sterilizable with hydrogen peroxide gas, 2) no image artifacts or deformations are noticeable in the CT and MRI images, 3) does not affect the SNR of MR images, and 4) its mechanical error is less than 5mm.",MICCAI 14
53e9b38eb7602d9703e6ebc1,Interventional radiology robot for CT and MRI guided percutaneous interventions.,53f452e9dabfaee0d9be63aa,Adeline Robert,Poster,Workshop,S23075,MICCAI,2011,Conference,14,RV61873,Mr. David Jenkins II,RV72327,Joshua Juarez,H39046,euzvesnssl,Chair,AR48602,dof robot,R57050,Accepted,"This paper introduces a new patient-mounted CT and MRI guided interventional radiology robot for percutaneous needle interventions. The 5 DOF robot uses ultrasonic motors and pneumatics to position the needle and then insert it progressively. The needle position and inclination can be registered in the images using two strategically placed fiducials visible in both imaging modalities. A first prototype is presented and described in terms of its sterilization, CT and MRI compatibility, and precision. Tests showed that 1) it is entirely sterilizable with hydrogen peroxide gas, 2) no image artifacts or deformations are noticeable in the CT and MRI images, 3) does not affect the SNR of MR images, and 4) its mechanical error is less than 5mm.",MICCAI 14
53e9b38eb7602d9703e6ebc1,Interventional radiology robot for CT and MRI guided percutaneous interventions.,53f42d55dabfaee02ac60445,Ivan Bricault,Poster,Workshop,S23075,MICCAI,2011,Conference,14,RV61873,Mr. David Jenkins II,RV72327,Joshua Juarez,H39046,euzvesnssl,Chair,AR48602,dof robot,R57050,Accepted,"This paper introduces a new patient-mounted CT and MRI guided interventional radiology robot for percutaneous needle interventions. The 5 DOF robot uses ultrasonic motors and pneumatics to position the needle and then insert it progressively. The needle position and inclination can be registered in the images using two strategically placed fiducials visible in both imaging modalities. A first prototype is presented and described in terms of its sterilization, CT and MRI compatibility, and precision. Tests showed that 1) it is entirely sterilizable with hydrogen peroxide gas, 2) no image artifacts or deformations are noticeable in the CT and MRI images, 3) does not affect the SNR of MR images, and 4) its mechanical error is less than 5mm.",MICCAI 14
53e9b38eb7602d9703e6ebc1,Interventional radiology robot for CT and MRI guided percutaneous interventions.,540958a7dabfae8faa683d1d,Philippe Cinquin,Poster,Workshop,S23075,MICCAI,2011,Conference,14,RV61873,Mr. David Jenkins II,RV72327,Joshua Juarez,H39046,euzvesnssl,Chair,AR48602,dof robot,R57050,Accepted,"This paper introduces a new patient-mounted CT and MRI guided interventional radiology robot for percutaneous needle interventions. The 5 DOF robot uses ultrasonic motors and pneumatics to position the needle and then insert it progressively. The needle position and inclination can be registered in the images using two strategically placed fiducials visible in both imaging modalities. A first prototype is presented and described in terms of its sterilization, CT and MRI compatibility, and precision. Tests showed that 1) it is entirely sterilizable with hydrogen peroxide gas, 2) no image artifacts or deformations are noticeable in the CT and MRI images, 3) does not affect the SNR of MR images, and 4) its mechanical error is less than 5mm.",MICCAI 14
53e9b38eb7602d9703e6e906,Comparison of the functional power of APL2 and FORTRAN 90,53f45294dabfaedd74e1ee62,Robert G. Willhoft,Demo Paper,None,S20411,APL,1991,Journal,21,RV62665,Jerry Davis,RV72214,Christina Porter,H30066,ckennunmmw,Editor,AR43336,computer language,R59555,Rejected,"APL and Fortran, although very different, share the challenge of remaining competitive in the light of an onslaught of modern computer languages. To meet this challenge, both have attempted to enhance their position by adding significant new features to their language. For example, APL2 is an extension of APL.Fortran has also attempted to meet the challenges of modern programming by developing a new Fortran standard called Fortran 90. This standard revises many areas of Fortran, but this paper will only concentrate on those that affect the computational power of Fortran. Many of the changes were motivated by the increased use of vector and array supercomputers. Therefore, array features, the ability to act on entire arrays instead of individual elements, are an important part of this new standard. In doing this work, the Fortran community looked to APL as an example of a powerful array language.This paper will answer several questions regarding this new standard. First, from a computational or functional point of view, what are the major features of Fortran 90? Next, how do these features compare with APL2? And finally, what can APL2 learn from the Fortran 90 work?",No
53e9b38eb7602d9703e6eec9,Software and Hardware Cooperate for 1-D FFT Algorithm Optimization on Multicore Processors,542a3a2cdabfae61d495c561,Yongbin Zhou,Full Paper,None,S26802,CIT (1),2009,Journal,1,RV65112,John Williams,RV73068,Susan Willis,H37884,boaspapnrj,Editor,AR40306,matrix transpose conceal,R59972,Rejected,"Multicore architecture is becoming a promise to keep Moore's Law and brings a revolution in both research and industry which results new design space for software and architecture. Fast Fourier transform (FFT), computing intensive and bandwidth intensive, is one of the most popular and important applications in the world. Compared with the computing resource on multicore architecture, the on-chip memory resource is much more expensive because of the limitation of physical chip size. Efficient implementation of FFT algorithm on multicore with good scalability is a challenge for both software and hardware developers. In this paper, supported by the Godson-T architecture, an optimized implementation of 1-D FFT has been developed with matrix transpose conceal and computation/communication overlapping, which achieve more than 30% performance improvement as well as almost 1/3 L2 cache consumption reduce comparing with the base six-step FFT. The limitation of scalability is also analyzed and the conclusion is that on Godson-T when frequency and simultaneous data access happen, the limited access bandwidth of L2 cache is the bottleneck and result in the longer on-chip network latency.",No
53e9b38eb7602d9703e6eec9,Software and Hardware Cooperate for 1-D FFT Algorithm Optimization on Multicore Processors,53f48515dabfaec09f2a8187,Junchao Zhang,Full Paper,None,S26802,CIT (1),2009,Journal,1,RV65112,John Williams,RV73068,Susan Willis,H37884,boaspapnrj,Editor,AR40306,matrix transpose conceal,R59972,Rejected,"Multicore architecture is becoming a promise to keep Moore's Law and brings a revolution in both research and industry which results new design space for software and architecture. Fast Fourier transform (FFT), computing intensive and bandwidth intensive, is one of the most popular and important applications in the world. Compared with the computing resource on multicore architecture, the on-chip memory resource is much more expensive because of the limitation of physical chip size. Efficient implementation of FFT algorithm on multicore with good scalability is a challenge for both software and hardware developers. In this paper, supported by the Godson-T architecture, an optimized implementation of 1-D FFT has been developed with matrix transpose conceal and computation/communication overlapping, which achieve more than 30% performance improvement as well as almost 1/3 L2 cache consumption reduce comparing with the base six-step FFT. The limitation of scalability is also analyzed and the conclusion is that on Godson-T when frequency and simultaneous data access happen, the limited access bandwidth of L2 cache is the bottleneck and result in the longer on-chip network latency.",No
53e9b38eb7602d9703e6eec9,Software and Hardware Cooperate for 1-D FFT Algorithm Optimization on Multicore Processors,56291c7c45cedb339882f2dd,Dongrui Fan,Full Paper,None,S26802,CIT (1),2009,Journal,1,RV65112,John Williams,RV73068,Susan Willis,H37884,boaspapnrj,Editor,AR40306,matrix transpose conceal,R59972,Rejected,"Multicore architecture is becoming a promise to keep Moore's Law and brings a revolution in both research and industry which results new design space for software and architecture. Fast Fourier transform (FFT), computing intensive and bandwidth intensive, is one of the most popular and important applications in the world. Compared with the computing resource on multicore architecture, the on-chip memory resource is much more expensive because of the limitation of physical chip size. Efficient implementation of FFT algorithm on multicore with good scalability is a challenge for both software and hardware developers. In this paper, supported by the Godson-T architecture, an optimized implementation of 1-D FFT has been developed with matrix transpose conceal and computation/communication overlapping, which achieve more than 30% performance improvement as well as almost 1/3 L2 cache consumption reduce comparing with the base six-step FFT. The limitation of scalability is also analyzed and the conclusion is that on Godson-T when frequency and simultaneous data access happen, the limited access bandwidth of L2 cache is the bottleneck and result in the longer on-chip network latency.",No
53e9b38eb7602d9703e6e961,Multivariate time series classification by combining trend-based and value-based approximations,53f47bdcdabfaee43ed47535,Bilal Esmael,Poster,Workshop,S29447,ICCSA (4),2012,Conference,7336,RV61676,Mr. Carl Sheppard MD,RV74526,Brandon Wade,H32929,adgbxbpklr,Chair,AR41088,time series,R55415,Accepted,"Multivariate time series data often have a very high dimensionality. Classifying such high dimensional data poses a challenge because a vast number of features can be extracted. Furthermore, the meaning of the normally intuitive term similar to needs to be precisely defined. Representing the time series data effectively is an essential task for decision-making activities such as prediction, clustering and classification. In this paper we propose a feature-based classification approach to classify real-world multivariate time series generated by drilling rig sensors in the oil and gas industry. Our approach encompasses two main phases: representation and classification. For the representation phase, we propose a novel representation of time series which combines trend-based and value-based approximations (we abbreviate it as TVA). It produces a compact representation of the time series which consists of symbolic strings that represent the trends and the values of each variable in the series. The TVA representation improves both the accuracy and the running time of the classification process by extracting a set of informative features suitable for common classifiers. For the classification phase, we propose a memory-based classifier which takes into account the antecedent results of the classification process. The inputs of the proposed classifier are the TVA features computed from the current segment, as well as the predicted class of the previous segment. Our experimental results on real-world multivariate time series show that our approach enables highly accurate and fast classification of multivariate time series.",ICCSA (4) 7336
53e9b38eb7602d9703e6e961,Multivariate time series classification by combining trend-based and value-based approximations,53f44a0ddabfaee43ec8c4f3,Arghad Arnaout,Poster,Workshop,S29447,ICCSA (4),2012,Conference,7336,RV61676,Mr. Carl Sheppard MD,RV74526,Brandon Wade,H32929,adgbxbpklr,Chair,AR41088,time series,R55415,Accepted,"Multivariate time series data often have a very high dimensionality. Classifying such high dimensional data poses a challenge because a vast number of features can be extracted. Furthermore, the meaning of the normally intuitive term similar to needs to be precisely defined. Representing the time series data effectively is an essential task for decision-making activities such as prediction, clustering and classification. In this paper we propose a feature-based classification approach to classify real-world multivariate time series generated by drilling rig sensors in the oil and gas industry. Our approach encompasses two main phases: representation and classification. For the representation phase, we propose a novel representation of time series which combines trend-based and value-based approximations (we abbreviate it as TVA). It produces a compact representation of the time series which consists of symbolic strings that represent the trends and the values of each variable in the series. The TVA representation improves both the accuracy and the running time of the classification process by extracting a set of informative features suitable for common classifiers. For the classification phase, we propose a memory-based classifier which takes into account the antecedent results of the classification process. The inputs of the proposed classifier are the TVA features computed from the current segment, as well as the predicted class of the previous segment. Our experimental results on real-world multivariate time series show that our approach enables highly accurate and fast classification of multivariate time series.",ICCSA (4) 7336
53e9b38eb7602d9703e6e961,Multivariate time series classification by combining trend-based and value-based approximations,53f4346ddabfaeb2ac04041b,Rudolf K. Fruhwirth,Poster,Workshop,S29447,ICCSA (4),2012,Conference,7336,RV61676,Mr. Carl Sheppard MD,RV74526,Brandon Wade,H32929,adgbxbpklr,Chair,AR41088,time series,R55415,Accepted,"Multivariate time series data often have a very high dimensionality. Classifying such high dimensional data poses a challenge because a vast number of features can be extracted. Furthermore, the meaning of the normally intuitive term similar to needs to be precisely defined. Representing the time series data effectively is an essential task for decision-making activities such as prediction, clustering and classification. In this paper we propose a feature-based classification approach to classify real-world multivariate time series generated by drilling rig sensors in the oil and gas industry. Our approach encompasses two main phases: representation and classification. For the representation phase, we propose a novel representation of time series which combines trend-based and value-based approximations (we abbreviate it as TVA). It produces a compact representation of the time series which consists of symbolic strings that represent the trends and the values of each variable in the series. The TVA representation improves both the accuracy and the running time of the classification process by extracting a set of informative features suitable for common classifiers. For the classification phase, we propose a memory-based classifier which takes into account the antecedent results of the classification process. The inputs of the proposed classifier are the TVA features computed from the current segment, as well as the predicted class of the previous segment. Our experimental results on real-world multivariate time series show that our approach enables highly accurate and fast classification of multivariate time series.",ICCSA (4) 7336
53e9b38eb7602d9703e6e961,Multivariate time series classification by combining trend-based and value-based approximations,53f461f5dabfaec09f22e1e3,Gerhard Thonhauser,Poster,Workshop,S29447,ICCSA (4),2012,Conference,7336,RV61676,Mr. Carl Sheppard MD,RV74526,Brandon Wade,H32929,adgbxbpklr,Chair,AR41088,time series,R55415,Accepted,"Multivariate time series data often have a very high dimensionality. Classifying such high dimensional data poses a challenge because a vast number of features can be extracted. Furthermore, the meaning of the normally intuitive term similar to needs to be precisely defined. Representing the time series data effectively is an essential task for decision-making activities such as prediction, clustering and classification. In this paper we propose a feature-based classification approach to classify real-world multivariate time series generated by drilling rig sensors in the oil and gas industry. Our approach encompasses two main phases: representation and classification. For the representation phase, we propose a novel representation of time series which combines trend-based and value-based approximations (we abbreviate it as TVA). It produces a compact representation of the time series which consists of symbolic strings that represent the trends and the values of each variable in the series. The TVA representation improves both the accuracy and the running time of the classification process by extracting a set of informative features suitable for common classifiers. For the classification phase, we propose a memory-based classifier which takes into account the antecedent results of the classification process. The inputs of the proposed classifier are the TVA features computed from the current segment, as well as the predicted class of the previous segment. Our experimental results on real-world multivariate time series show that our approach enables highly accurate and fast classification of multivariate time series.",ICCSA (4) 7336
53e9b38eb7602d9703e6ea0e,A lie subroutine for computing prehomogeneous spaces associated with real nilpotent orbits,53f42b9ddabfaeb22f3ec752,Steven Glenn Jackson,Poster,Symposium,S20555,ICCSA (3),2005,Conference,3482,RV60601,Steven Gomez,RV77161,John Edwards,H34155,jgfjjowprr,Chair,AR40209,latex statement,R54504,Rejected,We describe an algorithm for decomposing certain modules attached to real nilpotent orbits into their irreducible components. These modules are prehomogeneous spaces in the sense of Sato and Kimura and arise in the study of nilpotent orbits and the representation theory of Lie groups. The output is a set of LATEX statements that can be compiled in a LATEX environment in order to produce tables. Although the algorithm is used to solve the problem in the case of exceptional real reductive Lie groups of inner type it does describe these spaces for the classical cases of inner type also. Complete tables for the exceptional groups can be found at http://www.math.umb.edu/~anoel/publications/tables/.,No
53e9b38eb7602d9703e6ea0e,A lie subroutine for computing prehomogeneous spaces associated with real nilpotent orbits,53f47260dabfaee02adc3ae7,Alfred G. Noël,Poster,Symposium,S20555,ICCSA (3),2005,Conference,3482,RV60601,Steven Gomez,RV77161,John Edwards,H34155,jgfjjowprr,Chair,AR40209,latex statement,R54504,Rejected,We describe an algorithm for decomposing certain modules attached to real nilpotent orbits into their irreducible components. These modules are prehomogeneous spaces in the sense of Sato and Kimura and arise in the study of nilpotent orbits and the representation theory of Lie groups. The output is a set of LATEX statements that can be compiled in a LATEX environment in order to produce tables. Although the algorithm is used to solve the problem in the case of exceptional real reductive Lie groups of inner type it does describe these spaces for the classical cases of inner type also. Complete tables for the exceptional groups can be found at http://www.math.umb.edu/~anoel/publications/tables/.,No
53e9b38eb7602d9703e6f286,A design of the flexible mobile agents based on web,53f44b61dabfaefedbb279f5,Yun Ji Na,Poster,Symposium,S28397,ICCSA (5),2006,Conference,3984,RV63813,Dr. Robin Walls,RV74981,Kristy Weeks,H34983,dipsclwmhz,Chair,AR40414,fixed code part,R56918,Accepted,"The key features of mobile agent systems are mobility, autonomy, and intelligence. On integrity protection within mobile agent technology, mobile agent integrity should be protected against attacks from malicious hosts and other agents. In this paper, we design flexible Mobile Agents (FMA). The traditional mobile agents consist of fixed code parts. But FMA consist of flexibly upgradeable agent code part for that new agent code modules can be added and redundant code modules can be deleted on the executing requirement. FMA prove to be more flexible in agent-based web systems compared to the traditional static-type mobile agent.",ICCSA (5) 3984
53e9b38eb7602d9703e6f286,A design of the flexible mobile agents based on web,53f432a2dabfaee43ec08b89,Il Seok Ko,Poster,Symposium,S28397,ICCSA (5),2006,Conference,3984,RV63813,Dr. Robin Walls,RV74981,Kristy Weeks,H34983,dipsclwmhz,Chair,AR40414,fixed code part,R56918,Accepted,"The key features of mobile agent systems are mobility, autonomy, and intelligence. On integrity protection within mobile agent technology, mobile agent integrity should be protected against attacks from malicious hosts and other agents. In this paper, we design flexible Mobile Agents (FMA). The traditional mobile agents consist of fixed code parts. But FMA consist of flexibly upgradeable agent code part for that new agent code modules can be added and redundant code modules can be deleted on the executing requirement. FMA prove to be more flexible in agent-based web systems compared to the traditional static-type mobile agent.",ICCSA (5) 3984
53e9b38eb7602d9703e6f286,A design of the flexible mobile agents based on web,53f438b7dabfaeb22f4873f8,Gun Heui Han,Poster,Symposium,S28397,ICCSA (5),2006,Conference,3984,RV63813,Dr. Robin Walls,RV74981,Kristy Weeks,H34983,dipsclwmhz,Chair,AR40414,fixed code part,R56918,Accepted,"The key features of mobile agent systems are mobility, autonomy, and intelligence. On integrity protection within mobile agent technology, mobile agent integrity should be protected against attacks from malicious hosts and other agents. In this paper, we design flexible Mobile Agents (FMA). The traditional mobile agents consist of fixed code parts. But FMA consist of flexibly upgradeable agent code part for that new agent code modules can be added and redundant code modules can be deleted on the executing requirement. FMA prove to be more flexible in agent-based web systems compared to the traditional static-type mobile agent.",ICCSA (5) 3984
53e9b38eb7602d9703e6ee28,Formal study of plane delaunay triangulation,53f42aeadabfaeb1a7b6af6b,Jean-François Dufourd,Demo Paper,Expert Group,S22790,Clinical Orthopaedics and Related Research,2010,Conference,abs/1007.3350,RV63511,Kevin Holt,RV78690,Ian Marshall,H36609,rqjbbteoth,Chair,AR42909,delaunay property,R51684,Accepted,"This article presents the formal proof of correctness for a plane Delaunay triangulation algorithm. It consists in repeating a sequence of edge flippings from an initial triangulation until the Delaunay property is achieved. To describe triangulations, we rely on a combinatorial hypermap specification framework we have been developing for years. We embed hypermaps in the plane by attaching coordinates to elements in a consistent way. We then describe what are legal and illegal Delaunay edges and a flipping operation which we show preserves hypermap, triangulation, and embedding invariants. To prove the termination of the algorithm, we use a generic approach expressing that any non-cyclic relation is well-founded when working on a finite set.",Clinical Orthopaedics and Related Research abs/1007.3350
53e9b38eb7602d9703e6ee28,Formal study of plane delaunay triangulation,53f465dddabfaec09f23d877,Yves Bertot,Demo Paper,Expert Group,S22790,Clinical Orthopaedics and Related Research,2010,Conference,abs/1007.3350,RV63511,Kevin Holt,RV78690,Ian Marshall,H36609,rqjbbteoth,Chair,AR42909,delaunay property,R51684,Accepted,"This article presents the formal proof of correctness for a plane Delaunay triangulation algorithm. It consists in repeating a sequence of edge flippings from an initial triangulation until the Delaunay property is achieved. To describe triangulations, we rely on a combinatorial hypermap specification framework we have been developing for years. We embed hypermaps in the plane by attaching coordinates to elements in a consistent way. We then describe what are legal and illegal Delaunay edges and a flipping operation which we show preserves hypermap, triangulation, and embedding invariants. To prove the termination of the algorithm, we use a generic approach expressing that any non-cyclic relation is well-founded when working on a finite set.",Clinical Orthopaedics and Related Research abs/1007.3350
53e9b38eb7602d9703e6ee45,Scribble: closing the book on ad hoc documentation tools,540555c7dabfae92b41c577b,Matthew Flatt,Demo Paper,Regular Conference,S26445,Proceedings of the 18th ACM SIGPLAN international conference on Functional programming,2009,Conference,44,RV64647,Sean Smith,RV77723,Bryan Sexton,H30185,qjiukrqlqw,Chair,AR49150,domain specific languages,R54550,Rejected,"Scribble is a system for writing library documentation, user guides, and tutorials. It builds on PLT Scheme's technology for language extension, and at its heart is a new approach to connecting prose references with library bindings. Besides the base system, we have built Scribble libraries for JavaDoc-style API documentation, literate programming, and conference papers. We have used Scribble to produce thousands of pages of documentation for PLT Scheme; the new documentation is more complete, more accessible, and better organized, thanks in large part to Scribble's flexibility and the ease with which we cross-reference information across levels. This paper reports on the use of Scribble and on its design as both an extension and an extensible part of PLT Scheme.",No
53e9b38eb7602d9703e6ee45,Scribble: closing the book on ad hoc documentation tools,53f4324edabfaee02ac9f4fc,Eli Barzilay,Demo Paper,Regular Conference,S26445,Proceedings of the 18th ACM SIGPLAN international conference on Functional programming,2009,Conference,44,RV64647,Sean Smith,RV77723,Bryan Sexton,H30185,qjiukrqlqw,Chair,AR49150,domain specific languages,R54550,Rejected,"Scribble is a system for writing library documentation, user guides, and tutorials. It builds on PLT Scheme's technology for language extension, and at its heart is a new approach to connecting prose references with library bindings. Besides the base system, we have built Scribble libraries for JavaDoc-style API documentation, literate programming, and conference papers. We have used Scribble to produce thousands of pages of documentation for PLT Scheme; the new documentation is more complete, more accessible, and better organized, thanks in large part to Scribble's flexibility and the ease with which we cross-reference information across levels. This paper reports on the use of Scribble and on its design as both an extension and an extensible part of PLT Scheme.",No
53e9b38eb7602d9703e6ee45,Scribble: closing the book on ad hoc documentation tools,53f42cb8dabfaeb2acfe0b31,Robert Bruce Findler,Demo Paper,Regular Conference,S26445,Proceedings of the 18th ACM SIGPLAN international conference on Functional programming,2009,Conference,44,RV64647,Sean Smith,RV77723,Bryan Sexton,H30185,qjiukrqlqw,Chair,AR49150,domain specific languages,R54550,Rejected,"Scribble is a system for writing library documentation, user guides, and tutorials. It builds on PLT Scheme's technology for language extension, and at its heart is a new approach to connecting prose references with library bindings. Besides the base system, we have built Scribble libraries for JavaDoc-style API documentation, literate programming, and conference papers. We have used Scribble to produce thousands of pages of documentation for PLT Scheme; the new documentation is more complete, more accessible, and better organized, thanks in large part to Scribble's flexibility and the ease with which we cross-reference information across levels. This paper reports on the use of Scribble and on its design as both an extension and an extensible part of PLT Scheme.",No
53e9b38eb7602d9703e6ebbf,Color-Blind specifications for transformations of reactive synchronous programs,53f4b9c2dabfaed83d77b5af,Kim G. Larsen,Demo Paper,None,S21822,FASE,2005,Journal,3442,RV62731,Christina Leon,RV74284,Lauren Kaiser,H30045,yglpgdluwl,Editor,AR40605,reactive synchronous program,R55339,Rejected,"Execution environments are used as specifications for specialization of input-output programs in the derivation of product lines. These environments, formalized as color-blind I/O-alternating transition systems, are tolerant to mutations in a given program's outputs. Execution environments enable new compiler optimizations, vastly exceeding usual reductions. We propose a notion of context-dependent refinement for I/O-alternating transition systems, which supports composition and hierarchical reuse. The framework is demonstrated by discussing adaptations to realistic design languages and by presenting an exampleof a product line.",No
53e9b38eb7602d9703e6ebbf,Color-Blind specifications for transformations of reactive synchronous programs,53f47550dabfaeecd6a46100,Ulrik Larsen,Demo Paper,None,S21822,FASE,2005,Journal,3442,RV62731,Christina Leon,RV74284,Lauren Kaiser,H30045,yglpgdluwl,Editor,AR40605,reactive synchronous program,R55339,Rejected,"Execution environments are used as specifications for specialization of input-output programs in the derivation of product lines. These environments, formalized as color-blind I/O-alternating transition systems, are tolerant to mutations in a given program's outputs. Execution environments enable new compiler optimizations, vastly exceeding usual reductions. We propose a notion of context-dependent refinement for I/O-alternating transition systems, which supports composition and hierarchical reuse. The framework is demonstrated by discussing adaptations to realistic design languages and by presenting an exampleof a product line.",No
53e9b38eb7602d9703e6ebbf,Color-Blind specifications for transformations of reactive synchronous programs,53f47182dabfaec09f26aaa4,Andrzej Wasowski,Demo Paper,None,S21822,FASE,2005,Journal,3442,RV62731,Christina Leon,RV74284,Lauren Kaiser,H30045,yglpgdluwl,Editor,AR40605,reactive synchronous program,R55339,Rejected,"Execution environments are used as specifications for specialization of input-output programs in the derivation of product lines. These environments, formalized as color-blind I/O-alternating transition systems, are tolerant to mutations in a given program's outputs. Execution environments enable new compiler optimizations, vastly exceeding usual reductions. We propose a notion of context-dependent refinement for I/O-alternating transition systems, which supports composition and hierarchical reuse. The framework is demonstrated by discussing adaptations to realistic design languages and by presenting an exampleof a product line.",No
53e9b38eb7602d9703e6f2e7,Volume Reconstruction from Sparse 3D Ultrasonography,53f4c7b4dabfaee57a77ce60,Mark J. Gooding,Demo Paper,Workshop,S25518,Lecture Notes in Computer Science,2003,Conference,2879,RV68820,Regina Mullins,RV78818,Jeremy Richards,H30376,amlthlnxab,Chair,AR47743,surface reconstruction,R53807,Rejected,"3D freehand ultrasound has extensive application for organ volume measurement and has been shown to have better reproducibility than estimates of volume made from 2D measurement followed by interpolation to 3D. One key advantage of free-hand ultrasound is that of image compounding, but this advantage is lost in many automated reconstruction systems. A novel method is presented for the automated segmentation and surface reconstruction of organs from sparse 3D ultrasound data. Preliminary results are demonstrated for simulated data, and two cases of in-vivo data; breast ultrasound and imaging of ovarian follicles.",No
53e9b38eb7602d9703e6f2e7,Volume Reconstruction from Sparse 3D Ultrasonography,54489154dabfae87b7e44d8a,Stephen Kennedy,Demo Paper,Workshop,S25518,Lecture Notes in Computer Science,2003,Conference,2879,RV68820,Regina Mullins,RV78818,Jeremy Richards,H30376,amlthlnxab,Chair,AR47743,surface reconstruction,R53807,Rejected,"3D freehand ultrasound has extensive application for organ volume measurement and has been shown to have better reproducibility than estimates of volume made from 2D measurement followed by interpolation to 3D. One key advantage of free-hand ultrasound is that of image compounding, but this advantage is lost in many automated reconstruction systems. A novel method is presented for the automated segmentation and surface reconstruction of organs from sparse 3D ultrasound data. Preliminary results are demonstrated for simulated data, and two cases of in-vivo data; breast ultrasound and imaging of ovarian follicles.",No
53e9b38eb7602d9703e6f2e7,Volume Reconstruction from Sparse 3D Ultrasonography,53f43a41dabfaedce5553266,J. Alison Noble,Demo Paper,Workshop,S25518,Lecture Notes in Computer Science,2003,Conference,2879,RV68820,Regina Mullins,RV78818,Jeremy Richards,H30376,amlthlnxab,Chair,AR47743,surface reconstruction,R53807,Rejected,"3D freehand ultrasound has extensive application for organ volume measurement and has been shown to have better reproducibility than estimates of volume made from 2D measurement followed by interpolation to 3D. One key advantage of free-hand ultrasound is that of image compounding, but this advantage is lost in many automated reconstruction systems. A novel method is presented for the automated segmentation and surface reconstruction of organs from sparse 3D ultrasound data. Preliminary results are demonstrated for simulated data, and two cases of in-vivo data; breast ultrasound and imaging of ovarian follicles.",No
53e9b38eb7602d9703e6f2ea,A consideration of application of attractor selection to a real-time production scheduling,53f3a1bcdabfae4b34abe4e9,Hiroaki Chujo,Full Paper,Workshop,S20542,BioADIT,2006,Conference,3853,RV66352,Cristina Cruz,RV78189,Lisa Allen,H34508,bneqtlrwpp,Chair,AR48730,scheduling field,R56656,Accepted,"In this research, “attractor selection,” which adopts the concept of “attractor” chiefly defined in biological and physical fields, is applied to a scheduling problem. An attractor is an attraction area that an orbit in space converges on asymptotically, and this area denotes a stable state. The attractor to which an orbit from a certain state of an initial condition is attracted is statistically determined. Attractor selection is an algorithm that searches for a stable state flexibly under changing environments. To apply attractor selection to a scheduling field, a scheduling framework based on scheduling strategy using a dispatching rule is introduced. A scheduling problem solution is scheduled by repeated applications of a prepared dispatching rule with plural strategies. The rule has a parameter that controls scheduling strategies based on the current “environment,” which means kinds, amounts, and remaining to due of jobs, machine conditions, etc. Attracter selection controls the parameters under changing environments. The proposed framework was applied to a real-time production scheduling problem, and the optimality of the parameters of the strategy and followup ability were considerd when environmental changes occur.",BioADIT 3853
53e9b38eb7602d9703e6f2ea,A consideration of application of attractor selection to a real-time production scheduling,53f4288adabfaeb22f3d06d2,Hironori Oka,Full Paper,Workshop,S20542,BioADIT,2006,Conference,3853,RV66352,Cristina Cruz,RV78189,Lisa Allen,H34508,bneqtlrwpp,Chair,AR48730,scheduling field,R56656,Accepted,"In this research, “attractor selection,” which adopts the concept of “attractor” chiefly defined in biological and physical fields, is applied to a scheduling problem. An attractor is an attraction area that an orbit in space converges on asymptotically, and this area denotes a stable state. The attractor to which an orbit from a certain state of an initial condition is attracted is statistically determined. Attractor selection is an algorithm that searches for a stable state flexibly under changing environments. To apply attractor selection to a scheduling field, a scheduling framework based on scheduling strategy using a dispatching rule is introduced. A scheduling problem solution is scheduled by repeated applications of a prepared dispatching rule with plural strategies. The rule has a parameter that controls scheduling strategies based on the current “environment,” which means kinds, amounts, and remaining to due of jobs, machine conditions, etc. Attracter selection controls the parameters under changing environments. The proposed framework was applied to a real-time production scheduling problem, and the optimality of the parameters of the strategy and followup ability were considerd when environmental changes occur.",BioADIT 3853
53e9b38eb7602d9703e6f2ea,A consideration of application of attractor selection to a real-time production scheduling,53f47edddabfaec09f29b40b,Yoshitomo Ikkai,Full Paper,Workshop,S20542,BioADIT,2006,Conference,3853,RV66352,Cristina Cruz,RV78189,Lisa Allen,H34508,bneqtlrwpp,Chair,AR48730,scheduling field,R56656,Accepted,"In this research, “attractor selection,” which adopts the concept of “attractor” chiefly defined in biological and physical fields, is applied to a scheduling problem. An attractor is an attraction area that an orbit in space converges on asymptotically, and this area denotes a stable state. The attractor to which an orbit from a certain state of an initial condition is attracted is statistically determined. Attractor selection is an algorithm that searches for a stable state flexibly under changing environments. To apply attractor selection to a scheduling field, a scheduling framework based on scheduling strategy using a dispatching rule is introduced. A scheduling problem solution is scheduled by repeated applications of a prepared dispatching rule with plural strategies. The rule has a parameter that controls scheduling strategies based on the current “environment,” which means kinds, amounts, and remaining to due of jobs, machine conditions, etc. Attracter selection controls the parameters under changing environments. The proposed framework was applied to a real-time production scheduling problem, and the optimality of the parameters of the strategy and followup ability were considerd when environmental changes occur.",BioADIT 3853
53e9b38eb7602d9703e6f2ea,A consideration of application of attractor selection to a real-time production scheduling,53f42bfddabfaec09f0ff4dc,Norihisa Komoda,Full Paper,Workshop,S20542,BioADIT,2006,Conference,3853,RV66352,Cristina Cruz,RV78189,Lisa Allen,H34508,bneqtlrwpp,Chair,AR48730,scheduling field,R56656,Accepted,"In this research, “attractor selection,” which adopts the concept of “attractor” chiefly defined in biological and physical fields, is applied to a scheduling problem. An attractor is an attraction area that an orbit in space converges on asymptotically, and this area denotes a stable state. The attractor to which an orbit from a certain state of an initial condition is attracted is statistically determined. Attractor selection is an algorithm that searches for a stable state flexibly under changing environments. To apply attractor selection to a scheduling field, a scheduling framework based on scheduling strategy using a dispatching rule is introduced. A scheduling problem solution is scheduled by repeated applications of a prepared dispatching rule with plural strategies. The rule has a parameter that controls scheduling strategies based on the current “environment,” which means kinds, amounts, and remaining to due of jobs, machine conditions, etc. Attracter selection controls the parameters under changing environments. The proposed framework was applied to a real-time production scheduling problem, and the optimality of the parameters of the strategy and followup ability were considerd when environmental changes occur.",BioADIT 3853
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f441ecdabfaee43ec6f820,Martin J. Pearson,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f42c64dabfaeb22f3f96e7,Ian Gilhespy,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f43a57dabfaedce555434c,Kevin N. Gurney,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",54084095dabfae8faa638289,Chris Melhuish,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f42c13dabfaee02ac4d22e,Benjamin Mitchinson,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f427fadabfaeb2acfb025b,Mokhtar Nibouche,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f2f3,"A real-time, FPGA based, biologically plausible neural network processor",53f35617dabfae4b3495c8d2,Anthony G. Pipe,Demo Paper,Workshop,S25635,ICANN (2),2005,Conference,3697,RV65944,David Oliver,RV74854,Emma Young,H36192,gamykiyrzs,Chair,AR49242,feedback control,R55764,Accepted,"A real-time, large scale, leaky-integrate-and-fire neural network processor realized using FPGA is presented. This has been designed, as part of a collaborative project, to investigate and implement biologically plausible models of the rodent vibrissae based somatosensory system to control a robot. An emphasis has been made on hard real-time performance of the processor, as it is to be used as part of a feedback control system. This has led to a revision of some of the established modelling protocols used in other hardware spiking neural network processors. The underlying neuron model has the ability to model synaptic noise and inter-neural propagation delays to provide a greater degree of biological plausibility. The processor has been demonstrated modelling real neural circuitry in real-time, independent of the underlying neural network activity.",ICANN (2) 3697
53e9b38eb7602d9703e6f375,A fast and tolerant voting mechanism in wireless sensor networks,542a43addabfae646d5463c4,Jian-Hua Huang,Demo Paper,None,S25075,WISM (1),2011,Journal,6987,RV65539,Jessica Cole,RV79177,Daniel Gray,H30033,fopvqpaxdp,Editor,AR44512,voting technique,R54482,Rejected,"In wireless sensor networks, the voting technique is used to extract a reliable result from redundant information. This paper proposes a fast and tolerant voting mechanism (FTVM), which greatly reduces the polling delay at one election with the CDMA technique. This work also adopts intrusion detection techniques to identity the compromised nodes and replaces them with backup nodes, such that the tolerance of the network is improved by a large degree.",No
53e9b38eb7602d9703e6f375,A fast and tolerant voting mechanism in wireless sensor networks,53f43347dabfaee02acaa0c9,Yong-Hong Shi,Demo Paper,None,S25075,WISM (1),2011,Journal,6987,RV65539,Jessica Cole,RV79177,Daniel Gray,H30033,fopvqpaxdp,Editor,AR44512,voting technique,R54482,Rejected,"In wireless sensor networks, the voting technique is used to extract a reliable result from redundant information. This paper proposes a fast and tolerant voting mechanism (FTVM), which greatly reduces the polling delay at one election with the CDMA technique. This work also adopts intrusion detection techniques to identity the compromised nodes and replaces them with backup nodes, such that the tolerance of the network is improved by a large degree.",No
53e9b38eb7602d9703e6f06d,A particle swarm optimized fuzzy neural network control for acrobot,542d2fc3dabfae498ae19cac,Dong-bin Zhao,Full Paper,None,S21162,ISNN (2),2006,Journal,3972,RV69349,Miss Alison Fletcher MD,RV76052,Michael Wright,H30060,udiggdqfbg,Editor,AR49558,system state,R54688,Accepted,"This paper addresses the problem of controlling an acrobot, an under-actuated robotic systems, using fuzzy neural network approach. A five-layer Takagi-Sugeno fuzzy neural network control (TSFNNC) is proposed to swing up the acrobot from the low stable equilibrium to approach and balance around its top unstable equilibrium position. By analyzing the system dynamics, total energy and potential energy of the system are introduced in the second layer, with the system states as the inputs to the first layer. Fuzzy membership functions and rules are depicted in the third and fourth layers respectively. The fifth layer works as the final output. A modified particle swarm optimizer (PSO) is adopted to train the consequents in the fourth layer. Simulation results indicate that the integrated TSFNNC approach can control the acrobot system from upswing to balance process effectively. This approach provides an easy and feasible solution for similar control problems.",ISNN (2) 3972
53e9b38eb7602d9703e6f06d,A particle swarm optimized fuzzy neural network control for acrobot,56cb18b0c35f4f3c6565beb5,Jian-qiang Yi,Full Paper,None,S21162,ISNN (2),2006,Journal,3972,RV69349,Miss Alison Fletcher MD,RV76052,Michael Wright,H30060,udiggdqfbg,Editor,AR49558,system state,R54688,Accepted,"This paper addresses the problem of controlling an acrobot, an under-actuated robotic systems, using fuzzy neural network approach. A five-layer Takagi-Sugeno fuzzy neural network control (TSFNNC) is proposed to swing up the acrobot from the low stable equilibrium to approach and balance around its top unstable equilibrium position. By analyzing the system dynamics, total energy and potential energy of the system are introduced in the second layer, with the system states as the inputs to the first layer. Fuzzy membership functions and rules are depicted in the third and fourth layers respectively. The fifth layer works as the final output. A modified particle swarm optimizer (PSO) is adopted to train the consequents in the fourth layer. Simulation results indicate that the integrated TSFNNC approach can control the acrobot system from upswing to balance process effectively. This approach provides an easy and feasible solution for similar control problems.",ISNN (2) 3972
53e9b38eb7602d9703e6f3d4,Level of detail concepts in data-intensive web applications,53f56c60dabfae7059f80493,Sara Comai,Demo Paper,Regular Conference,S23139,ICWE,2005,Conference,3579,RV67173,Jeremy Lee,RV76670,Rachel Ross,H34599,pbzhdxlkkq,Chair,AR43182,navigational path,R51280,Rejected,"Current data-intensive Web applications, such as on-line trading, e-commerce, corporate portals and so on, are becoming more and more complex, both in terms of density of information and in terms of navigational paths. At this aim different techniques have been proposed in literature for optimizing the information to be shown to the user. In this paper we present a technique for tuning the amount of data presented to the user, directly inspired to the concept of Levels of Details (LoD), commonly used in computer graphics. Like in computer graphics the idea is to simplify the original model, without loosing the main characteristics of the objects to be shown. The approach is based on the application of LoD operators to the compositional and navigational structure of a Web application, expressed through an hypertextual model.",No
53e9b38eb7602d9703e6f3da,Mediation = Information Revelation + Analogical Reasoning,53f59ad7dabfaeec0af8045b,Simeon J. Simoff,Full Paper,Symposium,S25536,KRAMAS,2008,Conference,5605,RV63375,Jason Powers,RV79782,Amanda Hill,H36425,qtvvavgvok,Chair,AR48989,conflict resolution,R52121,Rejected,This paper presents an initial study of the relevant issues on the development of an automated mediation agent. The work is conducted within the `curious negotiator' framework [1] . The paper demonstrates that mediation is a knowledge intensive process that integrates information revelation and analogical reasoning. The introduced formalism is used to demonstrate how via revealing the appropriate information and reshaping the set of issues of the disputing parties mediation can succeed. The paper presents MediaThor - a mediating agent that utilises past experiences and information from negotiating parties to mediate disputes and change the positions of negotiating parties.,No
53e9b38eb7602d9703e6f3da,Mediation = Information Revelation + Analogical Reasoning,562ce15b45cedb3398cf11fd,Carles Sierra,Full Paper,Symposium,S25536,KRAMAS,2008,Conference,5605,RV63375,Jason Powers,RV79782,Amanda Hill,H36425,qtvvavgvok,Chair,AR48989,conflict resolution,R52121,Rejected,This paper presents an initial study of the relevant issues on the development of an automated mediation agent. The work is conducted within the `curious negotiator' framework [1] . The paper demonstrates that mediation is a knowledge intensive process that integrates information revelation and analogical reasoning. The introduced formalism is used to demonstrate how via revealing the appropriate information and reshaping the set of issues of the disputing parties mediation can succeed. The paper presents MediaThor - a mediating agent that utilises past experiences and information from negotiating parties to mediate disputes and change the positions of negotiating parties.,No
53e9b38eb7602d9703e6f3da,Mediation = Information Revelation + Analogical Reasoning,53f4281adabfaeb2acfb0b52,Ramon López de Mántaras,Full Paper,Symposium,S25536,KRAMAS,2008,Conference,5605,RV63375,Jason Powers,RV79782,Amanda Hill,H36425,qtvvavgvok,Chair,AR48989,conflict resolution,R52121,Rejected,This paper presents an initial study of the relevant issues on the development of an automated mediation agent. The work is conducted within the `curious negotiator' framework [1] . The paper demonstrates that mediation is a knowledge intensive process that integrates information revelation and analogical reasoning. The introduced formalism is used to demonstrate how via revealing the appropriate information and reshaping the set of issues of the disputing parties mediation can succeed. The paper presents MediaThor - a mediating agent that utilises past experiences and information from negotiating parties to mediate disputes and change the positions of negotiating parties.,No
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,53f44e89dabfaedd74e0ef43,Hyun-Ho Kang,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,53f430f4dabfaee0d9b2a3ab,Brian M. Kurkoski,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,53f450dcdabfaedf435f5d5f,Young-Ran Park,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,53f47120dabfaee4dc878b78,Sanguk Shin,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,5602998845cedb3395fee2ad,Kazuhiko Yamaguchi,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b38eb7602d9703e6f421,A viable system for tracing illegal users of video,53f42dc5dabfaedf4351be67,Kingo Kobayashi,Short Paper,None,S23970,WISI,2006,Journal,3917,RV67909,Chad Tran,RV78539,Sarah Perez,H35161,scbhqigwiu,Editor,AR49683,distributed system,R59384,Accepted,"Typical uses of watermarks include copyright protection and disabling unauthorized access to content. Especially, copyright protection watermarks embed some information in the data to identify the copyright holder or content provider, while receiver-identifying watermarking, commonly referred to as fingerprinting, embeds information to identify the receiver of that copy of the content. Thus, if an unauthorized copy of the content is recovered, extracting the fingerprint will show who the initial receiver was [1][2]. In this paper we generalize our previous work [3] of a video fingerprinting system to identify the source of illegal copies. This includes a logo embedding technique, generalization of the distribution system and detailed investigation of the robustness against collusion attacks.",WISI 3917
53e9b388b7602d9703e6d468,Improved assignment with ant colony optimization for multi-target tracking,53f43421dabfaee1c0a87b9b,Ali Onder Bozdogan,Short Paper,None,S24376,Expert Syst. Appl.,2011,Journal,38,RV62811,Susan Conrad,RV76928,Joseph Smith,H32871,ttjjsmceau,Editor,AR43932,multiple target tracking,R53573,Accepted,"Detecting and tracking ground targets is crucial in military intelligence in battlefield surveillance. Once targets have been detected, the system used can proceed to track them where tracking can be done using Ground Moving Target Indicator (GMTI) type indicators that can observe objects moving in the area of interest. However, when targets move close to each other in formation as a convoy, then the problem of assigning measurements to targets has to be addressed first, as it is an important step in target tracking. With the increasing computational power, it became possible to use more complex association logic in tracking algorithms. Although its optimal solution can be proved to be an NP hard problem, the multidimensional assignment enjoyed a renewed interest mostly due to Lagrangian relaxation approaches to its solution. Recently, it has been reported that randomized heuristic approaches surpassed the performance of Lagrangian relaxation algorithm especially in dense problems. In this paper, impelled from the success of randomized heuristic methods, we investigate a different stochastic approach, namely, the biologically inspired ant colony optimization to solve the NP hard multidimensional assignment problem for tracking multiple ground targets.",Expert Syst. Appl. 38
53e9b388b7602d9703e6d468,Improved assignment with ant colony optimization for multi-target tracking,53f44b05dabfaee0d9bc6800,Murat Efe,Short Paper,None,S24376,Expert Syst. Appl.,2011,Journal,38,RV62811,Susan Conrad,RV76928,Joseph Smith,H32871,ttjjsmceau,Editor,AR43932,multiple target tracking,R53573,Accepted,"Detecting and tracking ground targets is crucial in military intelligence in battlefield surveillance. Once targets have been detected, the system used can proceed to track them where tracking can be done using Ground Moving Target Indicator (GMTI) type indicators that can observe objects moving in the area of interest. However, when targets move close to each other in formation as a convoy, then the problem of assigning measurements to targets has to be addressed first, as it is an important step in target tracking. With the increasing computational power, it became possible to use more complex association logic in tracking algorithms. Although its optimal solution can be proved to be an NP hard problem, the multidimensional assignment enjoyed a renewed interest mostly due to Lagrangian relaxation approaches to its solution. Recently, it has been reported that randomized heuristic approaches surpassed the performance of Lagrangian relaxation algorithm especially in dense problems. In this paper, impelled from the success of randomized heuristic methods, we investigate a different stochastic approach, namely, the biologically inspired ant colony optimization to solve the NP hard multidimensional assignment problem for tracking multiple ground targets.",Expert Syst. Appl. 38
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,53f43b94dabfaee2a1d19007,Alexis C. Kaporis,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,543526cfdabfaebba58a91f2,Christos Makris,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,53fa1320dabfae7f97b05c78,Spyros Sioutas,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,53f79a9fdabfae8faa49e7aa,Athanasios K. Tsakalidis,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,53f4c864dabfaee57e77d3aa,Kostas Tsichlas,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d3dc,Improved Bounds for Finger Search on a RAM,53f44cb2dabfaee1c0b0636a,Christos D. Zaroliagis,Poster,Regular Conference,S22486,european symposium on algorithms,2013,Conference,66,RV61335,Christopher Lopez,RV76935,Kenneth Curry PhD,H37639,noyvewbtoy,Chair,AR42786,Data structures,R54047,Accepted,"We present a new finger search tree with O(1) worst-case update time and O(log log d) expected search time with high probability in the Random Access Machine (RAM) model of computation for a large class of input distributions. The parameter d represents the number of elements (distance) between the search element and an element pointed to by a finger, in a finger search tree that stores n elements. For the need of the analysis we model the updates by a balls and bins combinatorial game that is interesting in its own right as it involves insertions and deletions of balls according to an unknown distribution.",european symposium on algorithms 66
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,53f43617dabfaee43ec2d8fb,Davi Correia,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,560c23cc45ce1e596055aa06,H Petra Kok,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,53f46f3cdabfaeecd6a2ee33,Martijn de Greef,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,53f46e99dabfaeecd6a2c7b9,Arjan Bel,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,53f449abdabfaedd74dfbbee,Niek van Wieringen,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,53f45866dabfaefedbb57c94,Johannes Crezee,Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d185,Body Conformal Antennas for Superficial Hyperthermia: The Impact of Bending Contact Flexible Microstrip Applicators on Their Electromagnetic Behavior,5630156e45cedb339990a42e,"de Greef, M.",Short Paper,Workshop,S22611,"Biomedical Engineering, IEEE Transactions",2009,Conference,56,RV69385,Ryan Cohen,RV71057,Toni Williams DDS,H30583,jghwxbhfvv,Chair,AR44090,biological effects of fields,R59184,Rejected,"Hyperthermia is a powerful radiosensitizer for treatment of superficial tumors. This requires body conformal antennas with a power distribution as homogeneous as possible over the skin area. The contact flexible microstrip applicators (CFMA) operating at 434 MHz exist in several sizes, including the large size 3H and 5H. This paper investigates the behavior of the electromagnetic fields for the 3H and 5H CFMA in both flat and curved configurations, and the impact on performance parameters like the penetration depth (PD) and the effective heating depth (EHD). The underlying theory behind the electromagnetic behavior in curved situations is presented as well as numerical simulations of both flat and curved configurations. The results are compared to measurements of the electromagnetic field distributions in a cylindrical patient model. Due to their large size multimode solutions may exist, and our results confirm their existence. These multimode solutions affect both the power distribution and PD/EHD, with a dependence on applicator curvature. Therefore, the performance parameters like PD and EHD need to be carefully assessed when bending large size CFMA applicators to conform to the patient body. This conclusion also holds for other types of large size surface current applicators.",No
53e9b388b7602d9703e6d557,The multi-terminal maximum-flow network-interdiction problem,53f352e8dabfae4b349503f9,Ibrahim Akgün,Short Paper,None,S27934,European Journal of Operational Research,2011,Journal,211,RV63128,Eric Brown,RV75076,Dr. Charles Rivera,H37276,xqptvtqtal,Editor,AR42351,OR in military,R56504,Rejected,"This paper defines and studies the multi-terminal maximum-flow network-interdiction problem (MTNIP) in which a network user attempts to maximize flow in a network among K⩾3 pre-specified node groups while an interdictor uses limited resources to interdict network arcs to minimize this maximum flow. The paper proposes an exact (MTNIP-E) and an approximating model (MPNIM) to solve this NP-hard problem and presents computational results to compare the models. MTNIP-E is obtained by first formulating MTNIP as bi-level min–max program and then converting it into a mixed integer program where the flow is explicitly minimized. MPNIM is binary-integer program that does not minimize the flow directly. It partitions the node set into disjoint subsets such that each node group is in a different subset and minimizes the sum of the arc capacities crossing between different subsets. Computational results show that MPNIM can solve all instances in a few seconds while MTNIP-E cannot solve about one third of the problems in 24hour. The optimal objective function values of both models are equal to each other for some problems while they differ from each other as much as 46.2% in the worst case. However, when the post-interdiction flow capacity incurred by the solution of MPNIM is computed and compared to the objective value of MTNIP-E, the largest difference is only 7.90% implying that MPNIM may be a very good approximation to MTNIP-E.",No
53e9b388b7602d9703e6d557,The multi-terminal maximum-flow network-interdiction problem,53f46975dabfaee43ed02d7a,Barbaros Ç. Tansel,Short Paper,None,S27934,European Journal of Operational Research,2011,Journal,211,RV63128,Eric Brown,RV75076,Dr. Charles Rivera,H37276,xqptvtqtal,Editor,AR42351,OR in military,R56504,Rejected,"This paper defines and studies the multi-terminal maximum-flow network-interdiction problem (MTNIP) in which a network user attempts to maximize flow in a network among K⩾3 pre-specified node groups while an interdictor uses limited resources to interdict network arcs to minimize this maximum flow. The paper proposes an exact (MTNIP-E) and an approximating model (MPNIM) to solve this NP-hard problem and presents computational results to compare the models. MTNIP-E is obtained by first formulating MTNIP as bi-level min–max program and then converting it into a mixed integer program where the flow is explicitly minimized. MPNIM is binary-integer program that does not minimize the flow directly. It partitions the node set into disjoint subsets such that each node group is in a different subset and minimizes the sum of the arc capacities crossing between different subsets. Computational results show that MPNIM can solve all instances in a few seconds while MTNIP-E cannot solve about one third of the problems in 24hour. The optimal objective function values of both models are equal to each other for some problems while they differ from each other as much as 46.2% in the worst case. However, when the post-interdiction flow capacity incurred by the solution of MPNIM is computed and compared to the objective value of MTNIP-E, the largest difference is only 7.90% implying that MPNIM may be a very good approximation to MTNIP-E.",No
53e9b388b7602d9703e6d557,The multi-terminal maximum-flow network-interdiction problem,5405ee25dabfae91d301dabd,R. Kevin Wood,Short Paper,None,S27934,European Journal of Operational Research,2011,Journal,211,RV63128,Eric Brown,RV75076,Dr. Charles Rivera,H37276,xqptvtqtal,Editor,AR42351,OR in military,R56504,Rejected,"This paper defines and studies the multi-terminal maximum-flow network-interdiction problem (MTNIP) in which a network user attempts to maximize flow in a network among K⩾3 pre-specified node groups while an interdictor uses limited resources to interdict network arcs to minimize this maximum flow. The paper proposes an exact (MTNIP-E) and an approximating model (MPNIM) to solve this NP-hard problem and presents computational results to compare the models. MTNIP-E is obtained by first formulating MTNIP as bi-level min–max program and then converting it into a mixed integer program where the flow is explicitly minimized. MPNIM is binary-integer program that does not minimize the flow directly. It partitions the node set into disjoint subsets such that each node group is in a different subset and minimizes the sum of the arc capacities crossing between different subsets. Computational results show that MPNIM can solve all instances in a few seconds while MTNIP-E cannot solve about one third of the problems in 24hour. The optimal objective function values of both models are equal to each other for some problems while they differ from each other as much as 46.2% in the worst case. However, when the post-interdiction flow capacity incurred by the solution of MPNIM is computed and compared to the objective value of MTNIP-E, the largest difference is only 7.90% implying that MPNIM may be a very good approximation to MTNIP-E.",No
53e9b388b7602d9703e6d478,"Self-Consistency and MDL: A Paradigm for Evaluating Point-Correspondence Algorithms, and Its Application to Detecting Changes in Surface Elevation",53f43493dabfaeee22999b8b,Yvan G. Leclerc,Short Paper,None,S22922,International Journal of Computer Vision,2003,Journal,51,RV68666,Mike Morton,RV74364,Patrick Robinson,H39410,qozreioets,Editor,AR45513,consistency,R56923,Accepted,The self-consistency methodology is a new paradigm for evaluating certain vision algorithms without relying extensively on ground truth. We demonstrate its effectiveness in the case of point-correspondence algorithms and use our approach to predict their accuracy.,International Journal of Computer Vision 51
53e9b388b7602d9703e6d478,"Self-Consistency and MDL: A Paradigm for Evaluating Point-Correspondence Algorithms, and Its Application to Detecting Changes in Surface Elevation",53f4316edabfaee02ac941b3,Q.-Tuan Luong,Short Paper,None,S22922,International Journal of Computer Vision,2003,Journal,51,RV68666,Mike Morton,RV74364,Patrick Robinson,H39410,qozreioets,Editor,AR45513,consistency,R56923,Accepted,The self-consistency methodology is a new paradigm for evaluating certain vision algorithms without relying extensively on ground truth. We demonstrate its effectiveness in the case of point-correspondence algorithms and use our approach to predict their accuracy.,International Journal of Computer Vision 51
53e9b388b7602d9703e6d478,"Self-Consistency and MDL: A Paradigm for Evaluating Point-Correspondence Algorithms, and Its Application to Detecting Changes in Surface Elevation",53f4ec7fdabfae0354f8045b,P. Fua,Short Paper,None,S22922,International Journal of Computer Vision,2003,Journal,51,RV68666,Mike Morton,RV74364,Patrick Robinson,H39410,qozreioets,Editor,AR45513,consistency,R56923,Accepted,The self-consistency methodology is a new paradigm for evaluating certain vision algorithms without relying extensively on ground truth. We demonstrate its effectiveness in the case of point-correspondence algorithms and use our approach to predict their accuracy.,International Journal of Computer Vision 51
53e9b388b7602d9703e6d760,Recovery of upper body poses in static images based on joints detection,53f47789dabfaee4dc892407,Zhilan Hu,Short Paper,None,S23469,Pattern Recognition Letters,2009,Journal,30,RV69528,Lauren Simmons,RV71393,Christine Cabrera,H38457,nybzpxawpm,Editor,AR44132,continuous likelihood function,R56712,Rejected,"Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.",No
53e9b388b7602d9703e6d760,Recovery of upper body poses in static images based on joints detection,5405db4edabfae44f082eed2,Guijin Wang,Short Paper,None,S23469,Pattern Recognition Letters,2009,Journal,30,RV69528,Lauren Simmons,RV71393,Christine Cabrera,H38457,nybzpxawpm,Editor,AR44132,continuous likelihood function,R56712,Rejected,"Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.",No
53e9b388b7602d9703e6d760,Recovery of upper body poses in static images based on joints detection,53f7d373dabfae90ec121ade,Xinggang Lin,Short Paper,None,S23469,Pattern Recognition Letters,2009,Journal,30,RV69528,Lauren Simmons,RV71393,Christine Cabrera,H38457,nybzpxawpm,Editor,AR44132,continuous likelihood function,R56712,Rejected,"Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.",No
53e9b388b7602d9703e6d760,Recovery of upper body poses in static images based on joints detection,56205e3f45cedb33982672a5,Hong Yan,Short Paper,None,S23469,Pattern Recognition Letters,2009,Journal,30,RV69528,Lauren Simmons,RV71393,Christine Cabrera,H38457,nybzpxawpm,Editor,AR44132,continuous likelihood function,R56712,Rejected,"Recovering human body poses from static images is challenging without prior knowledge of pose, appearance, background and clothing. In this paper, we propose a novel model-based upper poses recovery method via effective joints detection. In our research, three observables are firstly detected: face, skin, and torso. Then the joints are properly initialized according to the observables and some heuristic configuration constraints. Finally the sample-based Markov chain Monte Carlo (MCMC) method is employed to determine the final pose. The main contributions of this paper include a robust torso detector through maximizing a posterior estimation, effective joints initialization, and two continuous likelihood functions developed for effective pose inference. Experiments on 250 real world images show that our method can accurately recover upper body poses from images with a variety of individuals, poses, backgrounds and clothing.",No
53e9b388b7602d9703e6d492,Electrostatic energy analysis of 8-oxoguanine DNA lesion-molecular dynamics study.,53f4b5b4dabfaedce564a9c0,Miroslav Pinak,Full Paper,Workshop,S21083,Computational Biology and Chemistry,2003,Conference,27,RV65176,Matthew Young,RV74320,Yolanda Barr,H32944,nctlifhehl,Chair,AR40891,electrostatic potential,R54054,Rejected,"One nanosecond molecular dynamics (MD) simulation was performed for two DNA segments each composed of 30 base pairs. In one DNA segment the native guanines at nucleotides positions 17 and 19 were replaced with two 8-oxoguanines (8-oxoG) (8-oxoG is mutagenic DNA oxo-lesion). The analysis of results was focused on the electrostatic energy that is supposed to be significant factor causing the disruption of DNA base stacking in DNA duplex and may also serve as a signal toward the repair enzyme informing the presence of the lesion. The repulsive interaction between 8-oxoG and the entire DNA molecule was observed, which caused the extrahelical position of 8-oxoG (position 19). The repulsive electrostatic interaction between both 8-oxoG lesions contributed to the flipping out of one 8-oxoG and to the local instability of the lesioned DNA region. The electrostatic potential at the surface of DNA close to the lesions has more negative value than the same region on the native DNA. This electrostatic potential may signal presence of the lesion to the repair enzyme. In the simulation of native DNA segment, no significant structural changes were observed and B-DNA structure was well preserved throughout the MD simulation.",No
53e9b388b7602d9703e6d576,QoS multicast routing using a quantum-behaved particle swarm optimization algorithm,5408e2e3dabfae8faa661c9c,Jun Sun,Short Paper,Workshop,S29971,Eng. Appl. of AI,2011,Conference,24,RV69254,James Massey,RV71407,James May,H38873,fufnxvkcvq,Chair,AR47072,routing problem,R52858,Accepted,"QoS multicast routing in networks is a very important research issue in networks and distributed systems. It is also a challenging and hard problem for high-performance networks of the next generation. Due to its NP-completeness, many heuristic methods have been employed to solve the problem. This paper proposes the modified quantum-behaved particle swarm optimization (QPSO) method for QoS multicast routing. In the proposed method, QoS multicast routing is converted into an integer programming problem with QoS constraints and is solved by the QPSO algorithm combined with loop deletion operation. The QPSO-based routing method, along with the routing algorithms based on particle swarm optimization (PSO) and genetic algorithm (GA), is tested on randomly generated network topologies for the purpose of performance evaluation. The simulation results show the efficiency of the proposed method on QoS the routing problem and its superiority to the methods based on PSO and GA.",Eng. Appl. of AI 24
53e9b388b7602d9703e6d576,QoS multicast routing using a quantum-behaved particle swarm optimization algorithm,53f470d1dabfaee43ed1f290,Wei Fang,Short Paper,Workshop,S29971,Eng. Appl. of AI,2011,Conference,24,RV69254,James Massey,RV71407,James May,H38873,fufnxvkcvq,Chair,AR47072,routing problem,R52858,Accepted,"QoS multicast routing in networks is a very important research issue in networks and distributed systems. It is also a challenging and hard problem for high-performance networks of the next generation. Due to its NP-completeness, many heuristic methods have been employed to solve the problem. This paper proposes the modified quantum-behaved particle swarm optimization (QPSO) method for QoS multicast routing. In the proposed method, QoS multicast routing is converted into an integer programming problem with QoS constraints and is solved by the QPSO algorithm combined with loop deletion operation. The QPSO-based routing method, along with the routing algorithms based on particle swarm optimization (PSO) and genetic algorithm (GA), is tested on randomly generated network topologies for the purpose of performance evaluation. The simulation results show the efficiency of the proposed method on QoS the routing problem and its superiority to the methods based on PSO and GA.",Eng. Appl. of AI 24
53e9b388b7602d9703e6d576,QoS multicast routing using a quantum-behaved particle swarm optimization algorithm,5406a57bdabfae8faa616a07,Xiaojun Wu,Short Paper,Workshop,S29971,Eng. Appl. of AI,2011,Conference,24,RV69254,James Massey,RV71407,James May,H38873,fufnxvkcvq,Chair,AR47072,routing problem,R52858,Accepted,"QoS multicast routing in networks is a very important research issue in networks and distributed systems. It is also a challenging and hard problem for high-performance networks of the next generation. Due to its NP-completeness, many heuristic methods have been employed to solve the problem. This paper proposes the modified quantum-behaved particle swarm optimization (QPSO) method for QoS multicast routing. In the proposed method, QoS multicast routing is converted into an integer programming problem with QoS constraints and is solved by the QPSO algorithm combined with loop deletion operation. The QPSO-based routing method, along with the routing algorithms based on particle swarm optimization (PSO) and genetic algorithm (GA), is tested on randomly generated network topologies for the purpose of performance evaluation. The simulation results show the efficiency of the proposed method on QoS the routing problem and its superiority to the methods based on PSO and GA.",Eng. Appl. of AI 24
53e9b388b7602d9703e6d576,QoS multicast routing using a quantum-behaved particle swarm optimization algorithm,54481539dabfae809f4d2299,Zhenping Xie,Short Paper,Workshop,S29971,Eng. Appl. of AI,2011,Conference,24,RV69254,James Massey,RV71407,James May,H38873,fufnxvkcvq,Chair,AR47072,routing problem,R52858,Accepted,"QoS multicast routing in networks is a very important research issue in networks and distributed systems. It is also a challenging and hard problem for high-performance networks of the next generation. Due to its NP-completeness, many heuristic methods have been employed to solve the problem. This paper proposes the modified quantum-behaved particle swarm optimization (QPSO) method for QoS multicast routing. In the proposed method, QoS multicast routing is converted into an integer programming problem with QoS constraints and is solved by the QPSO algorithm combined with loop deletion operation. The QPSO-based routing method, along with the routing algorithms based on particle swarm optimization (PSO) and genetic algorithm (GA), is tested on randomly generated network topologies for the purpose of performance evaluation. The simulation results show the efficiency of the proposed method on QoS the routing problem and its superiority to the methods based on PSO and GA.",Eng. Appl. of AI 24
53e9b388b7602d9703e6d576,QoS multicast routing using a quantum-behaved particle swarm optimization algorithm,54484ccddabfae87b7e026e8,Wenbo Xu,Short Paper,Workshop,S29971,Eng. Appl. of AI,2011,Conference,24,RV69254,James Massey,RV71407,James May,H38873,fufnxvkcvq,Chair,AR47072,routing problem,R52858,Accepted,"QoS multicast routing in networks is a very important research issue in networks and distributed systems. It is also a challenging and hard problem for high-performance networks of the next generation. Due to its NP-completeness, many heuristic methods have been employed to solve the problem. This paper proposes the modified quantum-behaved particle swarm optimization (QPSO) method for QoS multicast routing. In the proposed method, QoS multicast routing is converted into an integer programming problem with QoS constraints and is solved by the QPSO algorithm combined with loop deletion operation. The QPSO-based routing method, along with the routing algorithms based on particle swarm optimization (PSO) and genetic algorithm (GA), is tested on randomly generated network topologies for the purpose of performance evaluation. The simulation results show the efficiency of the proposed method on QoS the routing problem and its superiority to the methods based on PSO and GA.",Eng. Appl. of AI 24
53e9b388b7602d9703e6d7f2,VLSI systolic binary tree-searched vector quantizer for image compression,53f4530ddabfaee2a1d67114,Wai-Chi Fang,Short Paper,None,S29104,IEEE Trans. VLSI Syst.,1994,Journal,2,RV60411,Daniel Mckay,RV75055,Tiffany Carter,H35366,zmjenwwedn,Editor,AR40785,cmos integrated circuits,R58994,Accepted,"A high-speed image compression VLSI processor based on the systolic architecture of difference-codebook binary tree-searched vector quantization has been developed to meet the increasing demands on large-volume data communication and storage requirements. Simulation results show that this design is applicable to many types of image data and capable of producing good reconstructed data quality at high compression ratios. Various design aspects of the binary tree-searched vector quantizer including the algorithm, architecture, and detailed functional design are thoroughly investigated for VLSI implementation. An 8-level difference-codebook binary tree-searched vector quantizer can be implemented on a custom VLSI chip that includes a systolic array of eight identical processors and a hierarchical memory of eight subcodebook memory banks. The total transistor count is about 300000 and the die size is about 8.67/spl times/7.72 mm/sup 2/ in a 1.0 /spl mu/m CMOS technology. The throughput rate of this high-speed VLSI compression system is approximately 25 Mpixels per second and its equivalent computation power is 600 million instructions per second.<>",IEEE Trans. VLSI Syst. 2
53e9b388b7602d9703e6d7f2,VLSI systolic binary tree-searched vector quantizer for image compression,53f43aa2dabfaee02acf360e,Chi-Yung Chang,Short Paper,None,S29104,IEEE Trans. VLSI Syst.,1994,Journal,2,RV60411,Daniel Mckay,RV75055,Tiffany Carter,H35366,zmjenwwedn,Editor,AR40785,cmos integrated circuits,R58994,Accepted,"A high-speed image compression VLSI processor based on the systolic architecture of difference-codebook binary tree-searched vector quantization has been developed to meet the increasing demands on large-volume data communication and storage requirements. Simulation results show that this design is applicable to many types of image data and capable of producing good reconstructed data quality at high compression ratios. Various design aspects of the binary tree-searched vector quantizer including the algorithm, architecture, and detailed functional design are thoroughly investigated for VLSI implementation. An 8-level difference-codebook binary tree-searched vector quantizer can be implemented on a custom VLSI chip that includes a systolic array of eight identical processors and a hierarchical memory of eight subcodebook memory banks. The total transistor count is about 300000 and the die size is about 8.67/spl times/7.72 mm/sup 2/ in a 1.0 /spl mu/m CMOS technology. The throughput rate of this high-speed VLSI compression system is approximately 25 Mpixels per second and its equivalent computation power is 600 million instructions per second.<>",IEEE Trans. VLSI Syst. 2
53e9b388b7602d9703e6d7f2,VLSI systolic binary tree-searched vector quantizer for image compression,54883c30dabfae8a11fb415d,Bing J. Sheu,Short Paper,None,S29104,IEEE Trans. VLSI Syst.,1994,Journal,2,RV60411,Daniel Mckay,RV75055,Tiffany Carter,H35366,zmjenwwedn,Editor,AR40785,cmos integrated circuits,R58994,Accepted,"A high-speed image compression VLSI processor based on the systolic architecture of difference-codebook binary tree-searched vector quantization has been developed to meet the increasing demands on large-volume data communication and storage requirements. Simulation results show that this design is applicable to many types of image data and capable of producing good reconstructed data quality at high compression ratios. Various design aspects of the binary tree-searched vector quantizer including the algorithm, architecture, and detailed functional design are thoroughly investigated for VLSI implementation. An 8-level difference-codebook binary tree-searched vector quantizer can be implemented on a custom VLSI chip that includes a systolic array of eight identical processors and a hierarchical memory of eight subcodebook memory banks. The total transistor count is about 300000 and the die size is about 8.67/spl times/7.72 mm/sup 2/ in a 1.0 /spl mu/m CMOS technology. The throughput rate of this high-speed VLSI compression system is approximately 25 Mpixels per second and its equivalent computation power is 600 million instructions per second.<>",IEEE Trans. VLSI Syst. 2
53e9b388b7602d9703e6d7f2,VLSI systolic binary tree-searched vector quantizer for image compression,54087fdedabfae450f421bab,Oscal T.-C. Chen,Short Paper,None,S29104,IEEE Trans. VLSI Syst.,1994,Journal,2,RV60411,Daniel Mckay,RV75055,Tiffany Carter,H35366,zmjenwwedn,Editor,AR40785,cmos integrated circuits,R58994,Accepted,"A high-speed image compression VLSI processor based on the systolic architecture of difference-codebook binary tree-searched vector quantization has been developed to meet the increasing demands on large-volume data communication and storage requirements. Simulation results show that this design is applicable to many types of image data and capable of producing good reconstructed data quality at high compression ratios. Various design aspects of the binary tree-searched vector quantizer including the algorithm, architecture, and detailed functional design are thoroughly investigated for VLSI implementation. An 8-level difference-codebook binary tree-searched vector quantizer can be implemented on a custom VLSI chip that includes a systolic array of eight identical processors and a hierarchical memory of eight subcodebook memory banks. The total transistor count is about 300000 and the die size is about 8.67/spl times/7.72 mm/sup 2/ in a 1.0 /spl mu/m CMOS technology. The throughput rate of this high-speed VLSI compression system is approximately 25 Mpixels per second and its equivalent computation power is 600 million instructions per second.<>",IEEE Trans. VLSI Syst. 2
53e9b388b7602d9703e6d7f2,VLSI systolic binary tree-searched vector quantizer for image compression,53f43937dabfaeee229c6041,John C. Curlander,Short Paper,None,S29104,IEEE Trans. VLSI Syst.,1994,Journal,2,RV60411,Daniel Mckay,RV75055,Tiffany Carter,H35366,zmjenwwedn,Editor,AR40785,cmos integrated circuits,R58994,Accepted,"A high-speed image compression VLSI processor based on the systolic architecture of difference-codebook binary tree-searched vector quantization has been developed to meet the increasing demands on large-volume data communication and storage requirements. Simulation results show that this design is applicable to many types of image data and capable of producing good reconstructed data quality at high compression ratios. Various design aspects of the binary tree-searched vector quantizer including the algorithm, architecture, and detailed functional design are thoroughly investigated for VLSI implementation. An 8-level difference-codebook binary tree-searched vector quantizer can be implemented on a custom VLSI chip that includes a systolic array of eight identical processors and a hierarchical memory of eight subcodebook memory banks. The total transistor count is about 300000 and the die size is about 8.67/spl times/7.72 mm/sup 2/ in a 1.0 /spl mu/m CMOS technology. The throughput rate of this high-speed VLSI compression system is approximately 25 Mpixels per second and its equivalent computation power is 600 million instructions per second.<>",IEEE Trans. VLSI Syst. 2
53e9b388b7602d9703e6d7f7,Mean-dispersion preferences and constant absolute uncertainty aversion.,53f631a9dabfae906a9bf11f,Simon Grant,Demo Paper,None,S28546,Journal of Economic Theory,2013,Journal,148,RV67070,Juan Cross,RV71441,Mrs. Maureen Dennis,H37742,banfylupib,Editor,AR46626,D81,R56337,Rejected,"We axiomatize, in an Anscombe–Aumann framework, the class of preferences that admit a representation of the form V(f)=μ−ρ(d), where μ is the mean utility of the act f with respect to a given probability, d is the vector of state-by-state utility deviations from the mean, and ρ(d) is a measure of (aversion to) dispersion that corresponds to an uncertainty premium. The key feature of these mean-dispersion preferences is that they exhibit constant absolute uncertainty aversion. This class includes many well-known models of preferences from the literature on ambiguity. We show what properties of the dispersion function ρ(⋅) correspond to known models, to probabilistic sophistication, and to some new notions of uncertainty aversion.",No
53e9b388b7602d9703e6d7f7,Mean-dispersion preferences and constant absolute uncertainty aversion.,53f39b42dabfae4b34a94e7f,Ben Polak,Demo Paper,None,S28546,Journal of Economic Theory,2013,Journal,148,RV67070,Juan Cross,RV71441,Mrs. Maureen Dennis,H37742,banfylupib,Editor,AR46626,D81,R56337,Rejected,"We axiomatize, in an Anscombe–Aumann framework, the class of preferences that admit a representation of the form V(f)=μ−ρ(d), where μ is the mean utility of the act f with respect to a given probability, d is the vector of state-by-state utility deviations from the mean, and ρ(d) is a measure of (aversion to) dispersion that corresponds to an uncertainty premium. The key feature of these mean-dispersion preferences is that they exhibit constant absolute uncertainty aversion. This class includes many well-known models of preferences from the literature on ambiguity. We show what properties of the dispersion function ρ(⋅) correspond to known models, to probabilistic sophistication, and to some new notions of uncertainty aversion.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f42f46dabfaedd74d4ee25,Carl J. Yeoman,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,5406eb82dabfae44f086afb7,Nicholas Chia,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f430eddabfaee4dc7465aa,Suleyman Yildirim,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f43173dabfaee43ebf9723,Margret E. Berg Miller,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f3adf0dabfae4b34b0943e,Angela Kent,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f4326bdabfaee0d9b3d00f,Rebecca Stumpf,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f46daadabfaedf4366234c,Steven R. Leigh,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f47483dabfaee43ed2dfec,Karen E. Nelson,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,54325c7ddabfaeb4ea4eccc0,Bryan A. White,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d29f,Towards an Evolutionary Model of Animal-Associated Microbiomes.,53f4402adabfaeb22f4ad916,Brenda A. Wilson,Poster,Regular Conference,S29791,ENTROPY,2011,Conference,13,RV63069,Diamond Hayes,RV75107,Ariel Carpenter,H33883,hyfadtykev,Chair,AR40483,microbiome,R59853,Rejected,"Second-generation sequencing technologies have granted us greater access to the diversity and genetics of microbial communities that naturally reside endo-and ecto-symbiotically with animal hosts. Substantial research has emerged describing the diversity and broader trends that exist within and between host species and their associated microbial ecosystems, yet the application of these data to our evolutionary understanding of microbiomes appears fragmented. For the most part biological perspectives are based on limited observations of oversimplified communities, while mathematical and/or computational modeling of these concepts often lack biological precedence. In recognition of this disconnect, both fields have attempted to incorporate ecological theories, although their applicability is currently a subject of debate because most ecological theories were developed based on observations of macro-organisms and their ecosystems. For the purposes of this review, we attempt to transcend the biological, ecological and computational realms, drawing on extensive literature, to forge a useful framework that can, at a minimum be built upon, but ideally will shape the hypotheses of each field as they move forward. In evaluating the top-down selection pressures that are exerted on a microbiome we find cause to warrant reconsideration of the much-maligned theory of multi-level selection and reason that complexity must be underscored by modularity.",No
53e9b388b7602d9703e6d2a6,Wireless commons perils in the common good,53f3a78bdabfae4b34ae286c,Jan Damsgaard,Poster,Regular Conference,S28025,Commun. ACM,2006,Conference,49,RV62195,Melinda Santos,RV71378,Michelle Wolf,H36523,taaqeeixru,Chair,AR46254,manufacturing sector,R53921,Accepted,"In the last few years, high-speed wireless access to the Internet has grown rapidly. Surprisingly, this growth has not come through cellular phone networks as many had expected, but through IEEE 802.11 standards-based wireless local area networks (WLANs). This rise of WLANs can be partly linked to the creation of a series of open standards, a precipitous fall in the costs of related hardware, and the explosive growth of home networking. WLANs have become commonplace in the education, transportation, and manufacturing sectors and are rapidly embraced in the retail, hospitality, and government sectors.",Commun. ACM 49
53e9b388b7602d9703e6d2a6,Wireless commons perils in the common good,53f46859dabfaee43ecfe3f0,Mihir A. Parikh,Poster,Regular Conference,S28025,Commun. ACM,2006,Conference,49,RV62195,Melinda Santos,RV71378,Michelle Wolf,H36523,taaqeeixru,Chair,AR46254,manufacturing sector,R53921,Accepted,"In the last few years, high-speed wireless access to the Internet has grown rapidly. Surprisingly, this growth has not come through cellular phone networks as many had expected, but through IEEE 802.11 standards-based wireless local area networks (WLANs). This rise of WLANs can be partly linked to the creation of a series of open standards, a precipitous fall in the costs of related hardware, and the explosive growth of home networking. WLANs have become commonplace in the education, transportation, and manufacturing sectors and are rapidly embraced in the retail, hospitality, and government sectors.",Commun. ACM 49
53e9b388b7602d9703e6d2a6,Wireless commons perils in the common good,53f46816dabfaedd74e706de,Bharat Rao,Poster,Regular Conference,S28025,Commun. ACM,2006,Conference,49,RV62195,Melinda Santos,RV71378,Michelle Wolf,H36523,taaqeeixru,Chair,AR46254,manufacturing sector,R53921,Accepted,"In the last few years, high-speed wireless access to the Internet has grown rapidly. Surprisingly, this growth has not come through cellular phone networks as many had expected, but through IEEE 802.11 standards-based wireless local area networks (WLANs). This rise of WLANs can be partly linked to the creation of a series of open standards, a precipitous fall in the costs of related hardware, and the explosive growth of home networking. WLANs have become commonplace in the education, transportation, and manufacturing sectors and are rapidly embraced in the retail, hospitality, and government sectors.",Commun. ACM 49
53e9b388b7602d9703e6d2af,A Domain-Based Routing Protocol for SSM Source Mobility in Mobile IPv6 Networks,54453eb1dabfae862da0ee98,Ping Wang,Short Paper,None,S25816,Wireless Personal Communications,2007,Journal,41,RV66807,Andre Fitzpatrick,RV74378,Rachel Jacobs,H35891,mekppckfag,Editor,AR42646,Mobile IP,R57333,Rejected,"In Mobile IP, the signaling traffic overhead will be too high since the new Care-of-Address (CoA) of the mobile node (MN) is registered all the way with the home agent (HA) whenever the MN has moved into a new foreign network. To complement Mobile IP in better handling local movement, several IP micro protocols have been proposed. These protocols introduce a hierarchical mobility management scheme, which divides the mobility into micro mobility and macro mobility according to whether the host's movement is intra-domain or inter-domain. Thus, the requirements on performance and flexibility are achieved, especially for frequently moving hosts. This paper introduces a routing protocol for multicast source mobility on the basis of the hierarchical mobile management scheme, which provides a unified global architecture for both uni- and multicast routing in mobile networks. The implementation of multicast services adopts an improved SSM (Source Specific Multicast) model, which combines the advantages of the existing protocols in scalability and mobility transparency. Simulation results show that the proposed protocol has better performance than the existing routing protocols for SSM source mobility.",No
53e9b388b7602d9703e6d2af,A Domain-Based Routing Protocol for SSM Source Mobility in Mobile IPv6 Networks,54486217dabfae87b7e17775,Yunze Cai,Short Paper,None,S25816,Wireless Personal Communications,2007,Journal,41,RV66807,Andre Fitzpatrick,RV74378,Rachel Jacobs,H35891,mekppckfag,Editor,AR42646,Mobile IP,R57333,Rejected,"In Mobile IP, the signaling traffic overhead will be too high since the new Care-of-Address (CoA) of the mobile node (MN) is registered all the way with the home agent (HA) whenever the MN has moved into a new foreign network. To complement Mobile IP in better handling local movement, several IP micro protocols have been proposed. These protocols introduce a hierarchical mobility management scheme, which divides the mobility into micro mobility and macro mobility according to whether the host's movement is intra-domain or inter-domain. Thus, the requirements on performance and flexibility are achieved, especially for frequently moving hosts. This paper introduces a routing protocol for multicast source mobility on the basis of the hierarchical mobile management scheme, which provides a unified global architecture for both uni- and multicast routing in mobile networks. The implementation of multicast services adopts an improved SSM (Source Specific Multicast) model, which combines the advantages of the existing protocols in scalability and mobility transparency. Simulation results show that the proposed protocol has better performance than the existing routing protocols for SSM source mobility.",No
53e9b388b7602d9703e6d2af,A Domain-Based Routing Protocol for SSM Source Mobility in Mobile IPv6 Networks,53f43abcdabfaee2a1d10606,Jinjie Huang,Short Paper,None,S25816,Wireless Personal Communications,2007,Journal,41,RV66807,Andre Fitzpatrick,RV74378,Rachel Jacobs,H35891,mekppckfag,Editor,AR42646,Mobile IP,R57333,Rejected,"In Mobile IP, the signaling traffic overhead will be too high since the new Care-of-Address (CoA) of the mobile node (MN) is registered all the way with the home agent (HA) whenever the MN has moved into a new foreign network. To complement Mobile IP in better handling local movement, several IP micro protocols have been proposed. These protocols introduce a hierarchical mobility management scheme, which divides the mobility into micro mobility and macro mobility according to whether the host's movement is intra-domain or inter-domain. Thus, the requirements on performance and flexibility are achieved, especially for frequently moving hosts. This paper introduces a routing protocol for multicast source mobility on the basis of the hierarchical mobile management scheme, which provides a unified global architecture for both uni- and multicast routing in mobile networks. The implementation of multicast services adopts an improved SSM (Source Specific Multicast) model, which combines the advantages of the existing protocols in scalability and mobility transparency. Simulation results show that the proposed protocol has better performance than the existing routing protocols for SSM source mobility.",No
53e9b388b7602d9703e6d2af,A Domain-Based Routing Protocol for SSM Source Mobility in Mobile IPv6 Networks,53f43133dabfaee43ebf647b,Xiaoming Xu,Short Paper,None,S25816,Wireless Personal Communications,2007,Journal,41,RV66807,Andre Fitzpatrick,RV74378,Rachel Jacobs,H35891,mekppckfag,Editor,AR42646,Mobile IP,R57333,Rejected,"In Mobile IP, the signaling traffic overhead will be too high since the new Care-of-Address (CoA) of the mobile node (MN) is registered all the way with the home agent (HA) whenever the MN has moved into a new foreign network. To complement Mobile IP in better handling local movement, several IP micro protocols have been proposed. These protocols introduce a hierarchical mobility management scheme, which divides the mobility into micro mobility and macro mobility according to whether the host's movement is intra-domain or inter-domain. Thus, the requirements on performance and flexibility are achieved, especially for frequently moving hosts. This paper introduces a routing protocol for multicast source mobility on the basis of the hierarchical mobile management scheme, which provides a unified global architecture for both uni- and multicast routing in mobile networks. The implementation of multicast services adopts an improved SSM (Source Specific Multicast) model, which combines the advantages of the existing protocols in scalability and mobility transparency. Simulation results show that the proposed protocol has better performance than the existing routing protocols for SSM source mobility.",No
53e9b388b7602d9703e6d2bd,Virtual reality for collaborative e-learning,5440f35fdabfae7d84bcadae,Teresa Monahan,Demo Paper,None,S29292,Computers & Education,2008,Journal,50,RV66404,Christina Roberts,RV79080,Justin Singh,H32129,sfnttpyoza,Editor,AR43148,cooperative/collaborative learning,R54481,Rejected,"In the past, the term e-learning referred to any method of learning that used electronic delivery methods. With the advent of the Internet however, e-learning has evolved and the term is now most commonly used to refer to online courses. A multitude of systems are now available to manage and deliver learning content online. While these have proved popular, they are often single-user learning environments which provide little in the way of interaction or stimulation for the student. As the concept of lifelong learning now becomes a reality and thus more and more people are partaking in online courses, researchers are constantly exploring innovative techniques to motivate online students and enhance the e-learning experience. This article presents our research in this area and the resulting development of CLEV-R, a Collaborative Learning Environment with Virtual Reality. This web-based system uses Virtual Reality (VR) and multimedia and provides communication tools to support collaboration among students. In this article, we describe the features of CLEV-R, its adaptation for mobile devices and present the findings from an initial evaluation.",No
53e9b388b7602d9703e6d2bd,Virtual reality for collaborative e-learning,53f45546dabfaee1c0b2912c,Gavin McArdle,Demo Paper,None,S29292,Computers & Education,2008,Journal,50,RV66404,Christina Roberts,RV79080,Justin Singh,H32129,sfnttpyoza,Editor,AR43148,cooperative/collaborative learning,R54481,Rejected,"In the past, the term e-learning referred to any method of learning that used electronic delivery methods. With the advent of the Internet however, e-learning has evolved and the term is now most commonly used to refer to online courses. A multitude of systems are now available to manage and deliver learning content online. While these have proved popular, they are often single-user learning environments which provide little in the way of interaction or stimulation for the student. As the concept of lifelong learning now becomes a reality and thus more and more people are partaking in online courses, researchers are constantly exploring innovative techniques to motivate online students and enhance the e-learning experience. This article presents our research in this area and the resulting development of CLEV-R, a Collaborative Learning Environment with Virtual Reality. This web-based system uses Virtual Reality (VR) and multimedia and provides communication tools to support collaboration among students. In this article, we describe the features of CLEV-R, its adaptation for mobile devices and present the findings from an initial evaluation.",No
53e9b388b7602d9703e6d2bd,Virtual reality for collaborative e-learning,54330181dabfaeb4c6aa577c,Michela Bertolotto,Demo Paper,None,S29292,Computers & Education,2008,Journal,50,RV66404,Christina Roberts,RV79080,Justin Singh,H32129,sfnttpyoza,Editor,AR43148,cooperative/collaborative learning,R54481,Rejected,"In the past, the term e-learning referred to any method of learning that used electronic delivery methods. With the advent of the Internet however, e-learning has evolved and the term is now most commonly used to refer to online courses. A multitude of systems are now available to manage and deliver learning content online. While these have proved popular, they are often single-user learning environments which provide little in the way of interaction or stimulation for the student. As the concept of lifelong learning now becomes a reality and thus more and more people are partaking in online courses, researchers are constantly exploring innovative techniques to motivate online students and enhance the e-learning experience. This article presents our research in this area and the resulting development of CLEV-R, a Collaborative Learning Environment with Virtual Reality. This web-based system uses Virtual Reality (VR) and multimedia and provides communication tools to support collaboration among students. In this article, we describe the features of CLEV-R, its adaptation for mobile devices and present the findings from an initial evaluation.",No
53e9b388b7602d9703e6d2c5,Visual Difficulties Reported by Low-Vision and Nonimpaired Older Adult Drivers.,53f43429dabfaee0d9b4ff75,Loretta Neal McGregor,Demo Paper,Symposium,S23324,HUMAN FACTORS,2005,Conference,47,RV64655,Matthew Santiago,RV77067,Douglas Thompson,H39447,vuqkmtmmph,Chair,AR45423,factor analysis,R59118,Accepted,"Nonimpaired and low-vision older adults responded to a questionnaire regarding the types of visual difficulties experienced while performing daily tasks and while driving. Using the factors produced from a factor analysis as predictors, a discriminant analysis was performed to determine whether significant differences in visual problems existed between the groups. The majority of participants reported that they currently required more time than in the past to perform tasks that depended on their vision, regardless of their visual status. All participants reported experiencing significant difficulties with static and dynamic acuity, peripheral vision, illumination problems, and contrast sensitivity. Both nonimpaired and low-vision older adult drivers reported experiencing difficulty with glare, peripheral vision, and night driving. Low-vision drivers reported experiencing unique difficulties with near acuity, distant acuity, and physical obstructions. Potential applications of this research include suggestions for redesigning automobiles and highway signs for safer use and viewing.",HUMAN FACTORS 47
53e9b388b7602d9703e6d2c5,Visual Difficulties Reported by Low-Vision and Nonimpaired Older Adult Drivers.,53f4387bdabfaec22ba94b9c,Alex Chaparro,Demo Paper,Symposium,S23324,HUMAN FACTORS,2005,Conference,47,RV64655,Matthew Santiago,RV77067,Douglas Thompson,H39447,vuqkmtmmph,Chair,AR45423,factor analysis,R59118,Accepted,"Nonimpaired and low-vision older adults responded to a questionnaire regarding the types of visual difficulties experienced while performing daily tasks and while driving. Using the factors produced from a factor analysis as predictors, a discriminant analysis was performed to determine whether significant differences in visual problems existed between the groups. The majority of participants reported that they currently required more time than in the past to perform tasks that depended on their vision, regardless of their visual status. All participants reported experiencing significant difficulties with static and dynamic acuity, peripheral vision, illumination problems, and contrast sensitivity. Both nonimpaired and low-vision older adult drivers reported experiencing difficulty with glare, peripheral vision, and night driving. Low-vision drivers reported experiencing unique difficulties with near acuity, distant acuity, and physical obstructions. Potential applications of this research include suggestions for redesigning automobiles and highway signs for safer use and viewing.",HUMAN FACTORS 47
53e9b388b7602d9703e6d669,Factors affecting the adoption of open systems: an exploratory study,53f63321dabfae7cd03fa731,Patrick Y. K. Chau,Short Paper,None,S22076,MIS Quarterly,1997,Journal,21,RV60803,Peter Moreno,RV77158,Anthony Mason,H31441,qhlxxnvnid,Editor,AR42852,major ramification,R56063,Rejected,"Advocates of open systems believe that problems related to compatibility, interoperability, scalability, and efficient use of IT resources can be resolved by setting software and hardware standards and strictly adhering to these standards in systems development and management. Representing a major departure from the traditional way of running an IS operation, the adoption of open systems has major ramifications on the IT infrastructure with long-lasting effects. Unfortunately, little research has been done to study this ubiquitous phenomenon despite its impacts on organizational computing worldwide. To fill this research gap, a model that incorporates seven factors perceived to affect the adoption is developed and tested. In-depth interviews with senior executives responsible for managing corporate IS functions from 89 organizations were conducted to collect data for empirical analysis. The findings suggest that organizations tend to (1) focus more on their ability to adopt than on the benefits from adoption, and (2) take a reactive rather than proactive attitude in adopting open systems technology. Managerial implications are also discussed.",No
53e9b388b7602d9703e6d669,Factors affecting the adoption of open systems: an exploratory study,53f435bfdabfaee0d9b601a4,Kar Yan Tam,Short Paper,None,S22076,MIS Quarterly,1997,Journal,21,RV60803,Peter Moreno,RV77158,Anthony Mason,H31441,qhlxxnvnid,Editor,AR42852,major ramification,R56063,Rejected,"Advocates of open systems believe that problems related to compatibility, interoperability, scalability, and efficient use of IT resources can be resolved by setting software and hardware standards and strictly adhering to these standards in systems development and management. Representing a major departure from the traditional way of running an IS operation, the adoption of open systems has major ramifications on the IT infrastructure with long-lasting effects. Unfortunately, little research has been done to study this ubiquitous phenomenon despite its impacts on organizational computing worldwide. To fill this research gap, a model that incorporates seven factors perceived to affect the adoption is developed and tested. In-depth interviews with senior executives responsible for managing corporate IS functions from 89 organizations were conducted to collect data for empirical analysis. The findings suggest that organizations tend to (1) focus more on their ability to adopt than on the benefits from adoption, and (2) take a reactive rather than proactive attitude in adopting open systems technology. Managerial implications are also discussed.",No
53e9b388b7602d9703e6d953,A discriminative HMM/N-gram-based retrieval approach for mandarin spoken documents,56077f8d45cedb3396ade08d,Berlin Chen,Full Paper,Expert Group,S26809,ACM Trans. Asian Lang. Inf. Process.,2004,Conference,3,RV60326,Jennifer Taylor,RV71380,Karen Salazar,H34597,qcdtxibgrg,Chair,AR49449,indexing word,R57173,Accepted,"In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.",ACM Trans. Asian Lang. Inf. Process. 3
53e9b388b7602d9703e6d953,A discriminative HMM/N-gram-based retrieval approach for mandarin spoken documents,54352264dabfaebba58a61f5,Hsin-Min Wang,Full Paper,Expert Group,S26809,ACM Trans. Asian Lang. Inf. Process.,2004,Conference,3,RV60326,Jennifer Taylor,RV71380,Karen Salazar,H34597,qcdtxibgrg,Chair,AR49449,indexing word,R57173,Accepted,"In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.",ACM Trans. Asian Lang. Inf. Process. 3
53e9b388b7602d9703e6d953,A discriminative HMM/N-gram-based retrieval approach for mandarin spoken documents,53f46211dabfaee4dc83c485,Lin-Shan Lee,Full Paper,Expert Group,S26809,ACM Trans. Asian Lang. Inf. Process.,2004,Conference,3,RV60326,Jennifer Taylor,RV71380,Karen Salazar,H34597,qcdtxibgrg,Chair,AR49449,indexing word,R57173,Accepted,"In recent years, statistical modeling approaches have steadily gained in popularity in the field of information retrieval. This article presents an HMM/N-gram-based retrieval approach for Mandarin spoken documents. The underlying characteristics and the various structures of this approach were extensively investigated and analyzed. The retrieval capabilities were verified by tests with word- and syllable-level indexing features and comparisons to the conventional vector-space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. Fusion of information via indexing word- and syllable-level features was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.",ACM Trans. Asian Lang. Inf. Process. 3
53e9b388b7602d9703e6d2dd,Establishment of a hyperspectral evaluation model of ocean color satellite-measured reflectance.,53f4474adabfaec09f1cca8c,Zhihua Mao,Full Paper,Regular Conference,S24890,SCIENCE CHINA Information Sciences,2010,Conference,53,RV69760,Cynthia Bean,RV72821,Robert Harris,H37058,pjolccnvhz,Chair,AR43468,data processing,R56873,Accepted,"The accuracy of the satellite-measured reflectance is a key question for data processing and oceanographic applications. A
 hyperspectral satellite remote sensing reflectance evaluation model (HRSREM) was developed to evaluate the accuracy of the
 satellite measured reflectance. The model can compute the total reflectance at the top of the atmosphere (TOA) according to
 observation conditions of satellites, based on a radiative transfer model with the consideration of multiple scattering effects
 and atmospheric absorption effects. The performance of the HRSREM model was examined by Gordon’s algorithms, showing that
 the relative errors of the Rayleigh scattering reflectance and the aerosol scattering reflectance are less than 2%. The model
 can also compute the sky reflectance which can be validated by in-situ measurements. The two sky reflectances match well with
 a spectral average error of 5.4%. The relative error of the total reflectance of the model, verified by sea-viewing wide field-of-view
 sensor (SeaWiFS) data, is about 3.5%. Therefore, the total reflectances at TOA, computed by the model, can be taken as reference
 values to evaluate the accuracy of satellite reflectances. The model was used to evaluate the accuracy of the hypersptral
 satellite (Hyperion) remote sensing data. The Hyperion reflectance matches the total reflectance of HRSREM very well at visible
 and near-infrared bands with an average error of 7.3%, while the status of calibration coefficients at shortwave infrared
 bands are not stable with a large spectral average error of 63.5%. The reflectance evaluation of a moderate resolution imaging
 spectrometer (CMODIS) data indicated that relative errors are large, especially at near-infrared bands with relative errors
 more than 100%. The calibration coefficients of CMODIS, obtained from laboratory measurements, are not reliable. The CMODIS
 data should be recalibrated for oceanographic applications. The performance of the HRSREM model is effective in evaluating
 satellite data and its algorithms can be easily modified in order to evaluate the accuracy of other ocean color satellite
 sensors.",SCIENCE CHINA Information Sciences 53
53e9b388b7602d9703e6d2dd,Establishment of a hyperspectral evaluation model of ocean color satellite-measured reflectance.,53f432b3dabfaec22ba5be30,Jianyu Chen,Full Paper,Regular Conference,S24890,SCIENCE CHINA Information Sciences,2010,Conference,53,RV69760,Cynthia Bean,RV72821,Robert Harris,H37058,pjolccnvhz,Chair,AR43468,data processing,R56873,Accepted,"The accuracy of the satellite-measured reflectance is a key question for data processing and oceanographic applications. A
 hyperspectral satellite remote sensing reflectance evaluation model (HRSREM) was developed to evaluate the accuracy of the
 satellite measured reflectance. The model can compute the total reflectance at the top of the atmosphere (TOA) according to
 observation conditions of satellites, based on a radiative transfer model with the consideration of multiple scattering effects
 and atmospheric absorption effects. The performance of the HRSREM model was examined by Gordon’s algorithms, showing that
 the relative errors of the Rayleigh scattering reflectance and the aerosol scattering reflectance are less than 2%. The model
 can also compute the sky reflectance which can be validated by in-situ measurements. The two sky reflectances match well with
 a spectral average error of 5.4%. The relative error of the total reflectance of the model, verified by sea-viewing wide field-of-view
 sensor (SeaWiFS) data, is about 3.5%. Therefore, the total reflectances at TOA, computed by the model, can be taken as reference
 values to evaluate the accuracy of satellite reflectances. The model was used to evaluate the accuracy of the hypersptral
 satellite (Hyperion) remote sensing data. The Hyperion reflectance matches the total reflectance of HRSREM very well at visible
 and near-infrared bands with an average error of 7.3%, while the status of calibration coefficients at shortwave infrared
 bands are not stable with a large spectral average error of 63.5%. The reflectance evaluation of a moderate resolution imaging
 spectrometer (CMODIS) data indicated that relative errors are large, especially at near-infrared bands with relative errors
 more than 100%. The calibration coefficients of CMODIS, obtained from laboratory measurements, are not reliable. The CMODIS
 data should be recalibrated for oceanographic applications. The performance of the HRSREM model is effective in evaluating
 satellite data and its algorithms can be easily modified in order to evaluate the accuracy of other ocean color satellite
 sensors.",SCIENCE CHINA Information Sciences 53
53e9b388b7602d9703e6d2dd,Establishment of a hyperspectral evaluation model of ocean color satellite-measured reflectance.,53f46e02dabfaee4dc86c855,Haiqing Huang,Full Paper,Regular Conference,S24890,SCIENCE CHINA Information Sciences,2010,Conference,53,RV69760,Cynthia Bean,RV72821,Robert Harris,H37058,pjolccnvhz,Chair,AR43468,data processing,R56873,Accepted,"The accuracy of the satellite-measured reflectance is a key question for data processing and oceanographic applications. A
 hyperspectral satellite remote sensing reflectance evaluation model (HRSREM) was developed to evaluate the accuracy of the
 satellite measured reflectance. The model can compute the total reflectance at the top of the atmosphere (TOA) according to
 observation conditions of satellites, based on a radiative transfer model with the consideration of multiple scattering effects
 and atmospheric absorption effects. The performance of the HRSREM model was examined by Gordon’s algorithms, showing that
 the relative errors of the Rayleigh scattering reflectance and the aerosol scattering reflectance are less than 2%. The model
 can also compute the sky reflectance which can be validated by in-situ measurements. The two sky reflectances match well with
 a spectral average error of 5.4%. The relative error of the total reflectance of the model, verified by sea-viewing wide field-of-view
 sensor (SeaWiFS) data, is about 3.5%. Therefore, the total reflectances at TOA, computed by the model, can be taken as reference
 values to evaluate the accuracy of satellite reflectances. The model was used to evaluate the accuracy of the hypersptral
 satellite (Hyperion) remote sensing data. The Hyperion reflectance matches the total reflectance of HRSREM very well at visible
 and near-infrared bands with an average error of 7.3%, while the status of calibration coefficients at shortwave infrared
 bands are not stable with a large spectral average error of 63.5%. The reflectance evaluation of a moderate resolution imaging
 spectrometer (CMODIS) data indicated that relative errors are large, especially at near-infrared bands with relative errors
 more than 100%. The calibration coefficients of CMODIS, obtained from laboratory measurements, are not reliable. The CMODIS
 data should be recalibrated for oceanographic applications. The performance of the HRSREM model is effective in evaluating
 satellite data and its algorithms can be easily modified in order to evaluate the accuracy of other ocean color satellite
 sensors.",SCIENCE CHINA Information Sciences 53
53e9b388b7602d9703e6d2dd,Establishment of a hyperspectral evaluation model of ocean color satellite-measured reflectance.,56cb18c0c35f4f3c65660566,Xianqiang He,Full Paper,Regular Conference,S24890,SCIENCE CHINA Information Sciences,2010,Conference,53,RV69760,Cynthia Bean,RV72821,Robert Harris,H37058,pjolccnvhz,Chair,AR43468,data processing,R56873,Accepted,"The accuracy of the satellite-measured reflectance is a key question for data processing and oceanographic applications. A
 hyperspectral satellite remote sensing reflectance evaluation model (HRSREM) was developed to evaluate the accuracy of the
 satellite measured reflectance. The model can compute the total reflectance at the top of the atmosphere (TOA) according to
 observation conditions of satellites, based on a radiative transfer model with the consideration of multiple scattering effects
 and atmospheric absorption effects. The performance of the HRSREM model was examined by Gordon’s algorithms, showing that
 the relative errors of the Rayleigh scattering reflectance and the aerosol scattering reflectance are less than 2%. The model
 can also compute the sky reflectance which can be validated by in-situ measurements. The two sky reflectances match well with
 a spectral average error of 5.4%. The relative error of the total reflectance of the model, verified by sea-viewing wide field-of-view
 sensor (SeaWiFS) data, is about 3.5%. Therefore, the total reflectances at TOA, computed by the model, can be taken as reference
 values to evaluate the accuracy of satellite reflectances. The model was used to evaluate the accuracy of the hypersptral
 satellite (Hyperion) remote sensing data. The Hyperion reflectance matches the total reflectance of HRSREM very well at visible
 and near-infrared bands with an average error of 7.3%, while the status of calibration coefficients at shortwave infrared
 bands are not stable with a large spectral average error of 63.5%. The reflectance evaluation of a moderate resolution imaging
 spectrometer (CMODIS) data indicated that relative errors are large, especially at near-infrared bands with relative errors
 more than 100%. The calibration coefficients of CMODIS, obtained from laboratory measurements, are not reliable. The CMODIS
 data should be recalibrated for oceanographic applications. The performance of the HRSREM model is effective in evaluating
 satellite data and its algorithms can be easily modified in order to evaluate the accuracy of other ocean color satellite
 sensors.",SCIENCE CHINA Information Sciences 53
53e9b388b7602d9703e6d2dd,Establishment of a hyperspectral evaluation model of ocean color satellite-measured reflectance.,542ac45bdabfae646d58678b,Fang Gong,Full Paper,Regular Conference,S24890,SCIENCE CHINA Information Sciences,2010,Conference,53,RV69760,Cynthia Bean,RV72821,Robert Harris,H37058,pjolccnvhz,Chair,AR43468,data processing,R56873,Accepted,"The accuracy of the satellite-measured reflectance is a key question for data processing and oceanographic applications. A
 hyperspectral satellite remote sensing reflectance evaluation model (HRSREM) was developed to evaluate the accuracy of the
 satellite measured reflectance. The model can compute the total reflectance at the top of the atmosphere (TOA) according to
 observation conditions of satellites, based on a radiative transfer model with the consideration of multiple scattering effects
 and atmospheric absorption effects. The performance of the HRSREM model was examined by Gordon’s algorithms, showing that
 the relative errors of the Rayleigh scattering reflectance and the aerosol scattering reflectance are less than 2%. The model
 can also compute the sky reflectance which can be validated by in-situ measurements. The two sky reflectances match well with
 a spectral average error of 5.4%. The relative error of the total reflectance of the model, verified by sea-viewing wide field-of-view
 sensor (SeaWiFS) data, is about 3.5%. Therefore, the total reflectances at TOA, computed by the model, can be taken as reference
 values to evaluate the accuracy of satellite reflectances. The model was used to evaluate the accuracy of the hypersptral
 satellite (Hyperion) remote sensing data. The Hyperion reflectance matches the total reflectance of HRSREM very well at visible
 and near-infrared bands with an average error of 7.3%, while the status of calibration coefficients at shortwave infrared
 bands are not stable with a large spectral average error of 63.5%. The reflectance evaluation of a moderate resolution imaging
 spectrometer (CMODIS) data indicated that relative errors are large, especially at near-infrared bands with relative errors
 more than 100%. The calibration coefficients of CMODIS, obtained from laboratory measurements, are not reliable. The CMODIS
 data should be recalibrated for oceanographic applications. The performance of the HRSREM model is effective in evaluating
 satellite data and its algorithms can be easily modified in order to evaluate the accuracy of other ocean color satellite
 sensors.",SCIENCE CHINA Information Sciences 53
53e9b388b7602d9703e6d590,A hybrid intelligent system for 3D reconstruction from a single line drawing,53f47c7bdabfaee43ed49ce0,Yuan Sun,Full Paper,Expert Group,S20504,IJCAT,2012,Conference,45,RV62561,Gina Harrison,RV79685,William Golden,H32152,opvrhmmryw,Chair,AR47243,general solution,R56150,Accepted,"In automatic reconstruction of 3D objects from single line drawings, existing systems are all single-track, containing one general solution for all drawings. This paper proposes a method in which an input drawing is first classified based on dominant features which exist in the drawing, including symmetry, orthogonality and parallelism. The reconstruction is then performed by experts to deal with each class specifically. Drawing classification is done using the technique of support vector machine classification. A specific set of features are selected to form an optimal regularity set for each class, and used in the formulation of the objective function for effective reconstruction. Experimental results show that the proposed system can improve the reconstruction accuracy and efficiency than that of a single-track general 3D reconstruction system.",IJCAT 45
53e9b388b7602d9703e6d590,A hybrid intelligent system for 3D reconstruction from a single line drawing,54487a8bdabfae87b7e2dcf5,Jie Sun,Full Paper,Expert Group,S20504,IJCAT,2012,Conference,45,RV62561,Gina Harrison,RV79685,William Golden,H32152,opvrhmmryw,Chair,AR47243,general solution,R56150,Accepted,"In automatic reconstruction of 3D objects from single line drawings, existing systems are all single-track, containing one general solution for all drawings. This paper proposes a method in which an input drawing is first classified based on dominant features which exist in the drawing, including symmetry, orthogonality and parallelism. The reconstruction is then performed by experts to deal with each class specifically. Drawing classification is done using the technique of support vector machine classification. A specific set of features are selected to form an optimal regularity set for each class, and used in the formulation of the objective function for effective reconstruction. Experimental results show that the proposed system can improve the reconstruction accuracy and efficiency than that of a single-track general 3D reconstruction system.",IJCAT 45
53e9b388b7602d9703e6d590,A hybrid intelligent system for 3D reconstruction from a single line drawing,53f43ad6dabfaec09f1a7caa,Yong Tsue Lee,Full Paper,Expert Group,S20504,IJCAT,2012,Conference,45,RV62561,Gina Harrison,RV79685,William Golden,H32152,opvrhmmryw,Chair,AR47243,general solution,R56150,Accepted,"In automatic reconstruction of 3D objects from single line drawings, existing systems are all single-track, containing one general solution for all drawings. This paper proposes a method in which an input drawing is first classified based on dominant features which exist in the drawing, including symmetry, orthogonality and parallelism. The reconstruction is then performed by experts to deal with each class specifically. Drawing classification is done using the technique of support vector machine classification. A specific set of features are selected to form an optimal regularity set for each class, and used in the formulation of the objective function for effective reconstruction. Experimental results show that the proposed system can improve the reconstruction accuracy and efficiency than that of a single-track general 3D reconstruction system.",IJCAT 45
53e9b388b7602d9703e6d97b,Performance results of the simplex algorithm for a set of real-world linear programming models,53f45366dabfaee02ad4d6e0,Edward H. McCall,Full Paper,None,S28025,Commun. ACM,1982,Journal,25,RV67885,Timothy Greene,RV74503,Thomas Clark,H32994,sqppiqusjs,Editor,AR41764,commercial simplex algorithm,R59145,Accepted,"This paper provides performance results using the SPERRY UNIVAC 1100 Series linear programming product FMPS to solve a set of 16 real-world linear programming problems. As such, this paper provides a data point for the actual performance of a commercial simplex algorithm on real-world linear programming problems and shows that the simplex algorithm is a linear time algorithm in actual performance. Correlations and performance relationships not previously available are also provided.",Commun. ACM 25
53e9b388b7602d9703e6d6e3,A Time-Domain Based Approach for Short-Term Fading Depth Evaluation in Wideband Mobile Communication Systems,53f42ba9dabfaedf434fc234,Filipe D. Cardoso,Poster,Symposium,S25816,Wireless Personal Communications,2005,Conference,35,RV68602,Paula Bailey,RV78528,Lisa Kelly,H35090,ioxfzkegec,Chair,AR41856,short-term fading,R53651,Rejected,"In this paper a time-domain technique for wideband fading depth evaluation, is proposed, which is applicable to both Line-of-Sight and Non-Line-of-Sight cases, and the probability functions of the wideband received power are derived, contributing for filling in the gap of short-term fading characterisation in wideband systems. Application examples for different systems, namely GSM, UMTS and HIPERLAN/2, working in different standard reference environments, are shown. As expected, it is observed that Rayleigh and Rice distributions are appropriate for evaluating the fading margins for GSM in some environments; nevertheless, for UMTS and HIPERLAN/2, fading margins are usually well below the ones obtained from considering these narrowband distributions, the differences reaching up to around 10 and 13 dB, respectively.A simple relationship between the physical and the geometrical environment properties, and the rms delay spread of the propagation channel is also proposed, establishing a relationship between two models independently derived by different authors. Using the proposed relationship, fading depth results from the proposed time-domain technique are compared with the ones for an environment-geometry based one, and a good agreement is verified. The difference in fading depth between both approaches is roughly below 2 dB.",No
53e9b388b7602d9703e6d6e3,A Time-Domain Based Approach for Short-Term Fading Depth Evaluation in Wideband Mobile Communication Systems,53f43aeadabfaee43ec5bb15,Luis M. Correia,Poster,Symposium,S25816,Wireless Personal Communications,2005,Conference,35,RV68602,Paula Bailey,RV78528,Lisa Kelly,H35090,ioxfzkegec,Chair,AR41856,short-term fading,R53651,Rejected,"In this paper a time-domain technique for wideband fading depth evaluation, is proposed, which is applicable to both Line-of-Sight and Non-Line-of-Sight cases, and the probability functions of the wideband received power are derived, contributing for filling in the gap of short-term fading characterisation in wideband systems. Application examples for different systems, namely GSM, UMTS and HIPERLAN/2, working in different standard reference environments, are shown. As expected, it is observed that Rayleigh and Rice distributions are appropriate for evaluating the fading margins for GSM in some environments; nevertheless, for UMTS and HIPERLAN/2, fading margins are usually well below the ones obtained from considering these narrowband distributions, the differences reaching up to around 10 and 13 dB, respectively.A simple relationship between the physical and the geometrical environment properties, and the rms delay spread of the propagation channel is also proposed, establishing a relationship between two models independently derived by different authors. Using the proposed relationship, fading depth results from the proposed time-domain technique are compared with the ones for an environment-geometry based one, and a good agreement is verified. The difference in fading depth between both approaches is roughly below 2 dB.",No
53e9b388b7602d9703e6d5ae,A Delay-Efficient Algorithm for Data Aggregation in Multihop Wireless Sensor Networks,5609324745cedb3396e4f0b3,XiaoHua Xu,Full Paper,None,S20012,IEEE Trans. Parallel Distrib. Syst.,2011,Journal,22,RV65142,Andrea Robinson,RV70994,Robert Moore,H37369,hbzyvhrpzj,Editor,AR41105,distributed algorithms,R56326,Rejected,"Data aggregation is a key functionality in wireless sensor networks (WSNs). This paper focuses on data aggregation scheduling problem to minimize the delay (or latency). We propose an efficient distributed algorithm that produces a collision-free schedule for data aggregation in WSNs. We theoretically prove that the delay of the aggregation schedule generated by our algorithm is at most 16R + Δ - 14 time slots. Here, R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our algorithm significantly improves the previously known best data aggregation algorithm with an upper bound of delay of 24D + 6Δ + 16 time slots, where D is the network diameter (note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation algorithm. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower bound of delay for data aggregation under any interference model is max{log n,R}, where n is the network size. We provide an example to show that the lower bound is (approximately) tight under the protocol interference model when rI = r, where rI is the interference range and r is the transmission range. We also derive the lower bound of delay under the protocol interference model when r <; rI <; 3r and rI ≥ 3r.",No
53e9b388b7602d9703e6d5ae,A Delay-Efficient Algorithm for Data Aggregation in Multihop Wireless Sensor Networks,542a4ae1dabfae61d496511f,Xiang Yang Li,Full Paper,None,S20012,IEEE Trans. Parallel Distrib. Syst.,2011,Journal,22,RV65142,Andrea Robinson,RV70994,Robert Moore,H37369,hbzyvhrpzj,Editor,AR41105,distributed algorithms,R56326,Rejected,"Data aggregation is a key functionality in wireless sensor networks (WSNs). This paper focuses on data aggregation scheduling problem to minimize the delay (or latency). We propose an efficient distributed algorithm that produces a collision-free schedule for data aggregation in WSNs. We theoretically prove that the delay of the aggregation schedule generated by our algorithm is at most 16R + Δ - 14 time slots. Here, R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our algorithm significantly improves the previously known best data aggregation algorithm with an upper bound of delay of 24D + 6Δ + 16 time slots, where D is the network diameter (note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation algorithm. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower bound of delay for data aggregation under any interference model is max{log n,R}, where n is the network size. We provide an example to show that the lower bound is (approximately) tight under the protocol interference model when rI = r, where rI is the interference range and r is the transmission range. We also derive the lower bound of delay under the protocol interference model when r <; rI <; 3r and rI ≥ 3r.",No
53e9b388b7602d9703e6d5ae,A Delay-Efficient Algorithm for Data Aggregation in Multihop Wireless Sensor Networks,53f42ff5dabfaee43ebe6064,XuFei Mao,Full Paper,None,S20012,IEEE Trans. Parallel Distrib. Syst.,2011,Journal,22,RV65142,Andrea Robinson,RV70994,Robert Moore,H37369,hbzyvhrpzj,Editor,AR41105,distributed algorithms,R56326,Rejected,"Data aggregation is a key functionality in wireless sensor networks (WSNs). This paper focuses on data aggregation scheduling problem to minimize the delay (or latency). We propose an efficient distributed algorithm that produces a collision-free schedule for data aggregation in WSNs. We theoretically prove that the delay of the aggregation schedule generated by our algorithm is at most 16R + Δ - 14 time slots. Here, R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our algorithm significantly improves the previously known best data aggregation algorithm with an upper bound of delay of 24D + 6Δ + 16 time slots, where D is the network diameter (note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation algorithm. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower bound of delay for data aggregation under any interference model is max{log n,R}, where n is the network size. We provide an example to show that the lower bound is (approximately) tight under the protocol interference model when rI = r, where rI is the interference range and r is the transmission range. We also derive the lower bound of delay under the protocol interference model when r <; rI <; 3r and rI ≥ 3r.",No
53e9b388b7602d9703e6d5ae,A Delay-Efficient Algorithm for Data Aggregation in Multihop Wireless Sensor Networks,5408a9dddabfae450f43462b,Shaojie Tang,Full Paper,None,S20012,IEEE Trans. Parallel Distrib. Syst.,2011,Journal,22,RV65142,Andrea Robinson,RV70994,Robert Moore,H37369,hbzyvhrpzj,Editor,AR41105,distributed algorithms,R56326,Rejected,"Data aggregation is a key functionality in wireless sensor networks (WSNs). This paper focuses on data aggregation scheduling problem to minimize the delay (or latency). We propose an efficient distributed algorithm that produces a collision-free schedule for data aggregation in WSNs. We theoretically prove that the delay of the aggregation schedule generated by our algorithm is at most 16R + Δ - 14 time slots. Here, R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our algorithm significantly improves the previously known best data aggregation algorithm with an upper bound of delay of 24D + 6Δ + 16 time slots, where D is the network diameter (note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation algorithm. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower bound of delay for data aggregation under any interference model is max{log n,R}, where n is the network size. We provide an example to show that the lower bound is (approximately) tight under the protocol interference model when rI = r, where rI is the interference range and r is the transmission range. We also derive the lower bound of delay under the protocol interference model when r <; rI <; 3r and rI ≥ 3r.",No
53e9b388b7602d9703e6d5ae,A Delay-Efficient Algorithm for Data Aggregation in Multihop Wireless Sensor Networks,53f435aadabfaeee229a4ed2,ShiGuang Wang,Full Paper,None,S20012,IEEE Trans. Parallel Distrib. Syst.,2011,Journal,22,RV65142,Andrea Robinson,RV70994,Robert Moore,H37369,hbzyvhrpzj,Editor,AR41105,distributed algorithms,R56326,Rejected,"Data aggregation is a key functionality in wireless sensor networks (WSNs). This paper focuses on data aggregation scheduling problem to minimize the delay (or latency). We propose an efficient distributed algorithm that produces a collision-free schedule for data aggregation in WSNs. We theoretically prove that the delay of the aggregation schedule generated by our algorithm is at most 16R + Δ - 14 time slots. Here, R is the network radius and Δ is the maximum node degree in the communication graph of the original network. Our algorithm significantly improves the previously known best data aggregation algorithm with an upper bound of delay of 24D + 6Δ + 16 time slots, where D is the network diameter (note that D can be as large as 2R). We conduct extensive simulations to study the practical performances of our proposed data aggregation algorithm. Our simulation results corroborate our theoretical results and show that our algorithms perform better in practice. We prove that the overall lower bound of delay for data aggregation under any interference model is max{log n,R}, where n is the network size. We provide an example to show that the lower bound is (approximately) tight under the protocol interference model when rI = r, where rI is the interference range and r is the transmission range. We also derive the lower bound of delay under the protocol interference model when r <; rI <; 3r and rI ≥ 3r.",No
53e9b388b7602d9703e6d6fb,Minimizing drop cost for SONET/WDM networks with wavelength requirements,54057dc0dabfae8faa5d8415,Charles J. Colbourn,Short Paper,None,S22055,NETWORKS,2001,Journal,37,RV61929,Rhonda Davis,RV73230,Dylan Flores,H31146,mcekcacxjg,Editor,AR48865,ring grooming,R50930,Accepted,"SONET/WDM networks using wavelength add-drop multiplexing can be constructed using certain graph decompositions used to form a grooming, consisting of unions of certain primitive rings, The existence of such decompositions when every pair of sites employs no more than 1/8 of the wavelength capacity is determined, with few possible exceptions, when the ring size is a multiple of four. The techniques developed rely heavily on tools from combinatorial design theory. (C) 2001 John Wiley & Sons, Inc.",NETWORKS 37
53e9b388b7602d9703e6d6fb,Minimizing drop cost for SONET/WDM networks with wavelength requirements,542ad9c9dabfae646d58ca4e,Peng-jun Wan,Short Paper,None,S22055,NETWORKS,2001,Journal,37,RV61929,Rhonda Davis,RV73230,Dylan Flores,H31146,mcekcacxjg,Editor,AR48865,ring grooming,R50930,Accepted,"SONET/WDM networks using wavelength add-drop multiplexing can be constructed using certain graph decompositions used to form a grooming, consisting of unions of certain primitive rings, The existence of such decompositions when every pair of sites employs no more than 1/8 of the wavelength capacity is determined, with few possible exceptions, when the ring size is a multiple of four. The techniques developed rely heavily on tools from combinatorial design theory. (C) 2001 John Wiley & Sons, Inc.",NETWORKS 37
53e9b388b7602d9703e6da98,Virtual reality for future workforce preparation,53f3a1b5dabfae4b34abe211,Shana S. Smith,Short Paper,Symposium,S28499,COMPUTER APPLICATIONS IN ENGINEERING EDUCATION,2009,Conference,17.0,RV67884,Angela Stone,RV79998,Matthew Atkins,H31467,xoycnehccy,Chair,AR44507,virtual reality,R58913,Rejected,"This study discusses how the use of virtual reality technology impacts academic and industry needs. Results and recommendations from interviews concerning engineering design curricula are presented for two major computer-aided design companies. The results described show that virtual reality technology has significant impacts oil academia and industry and that there are also significant interactions and relationships between the two entities. Implementation techniques and responses from students, faculty, and industry are presented. The article concludes with recommendations for current practice and areas for future research, which will promote effective use of virtual reality in engineering education. (C) 2009 Wiley Periodicals. Inc. Comput Appl Eng Educ 17: 429-434, 2009; Published online in Wiley InterScience (www.interscience.wiley.com) DOI 10.1002/cae.20211",No
53e9b388b7602d9703e6da98,Virtual reality for future workforce preparation,53f43699dabfaee02accb9a0,Kevin P. Saunders,Short Paper,Symposium,S28499,COMPUTER APPLICATIONS IN ENGINEERING EDUCATION,2009,Conference,17.0,RV67884,Angela Stone,RV79998,Matthew Atkins,H31467,xoycnehccy,Chair,AR44507,virtual reality,R58913,Rejected,"This study discusses how the use of virtual reality technology impacts academic and industry needs. Results and recommendations from interviews concerning engineering design curricula are presented for two major computer-aided design companies. The results described show that virtual reality technology has significant impacts oil academia and industry and that there are also significant interactions and relationships between the two entities. Implementation techniques and responses from students, faculty, and industry are presented. The article concludes with recommendations for current practice and areas for future research, which will promote effective use of virtual reality in engineering education. (C) 2009 Wiley Periodicals. Inc. Comput Appl Eng Educ 17: 429-434, 2009; Published online in Wiley InterScience (www.interscience.wiley.com) DOI 10.1002/cae.20211",No
53e9b388b7602d9703e6dad6,Interference mitigation in WSN by means of directional antennas and duty cycle control,53f43a13dabfaec22baa3169,Kamil Staniec,Demo Paper,Workshop,S26090,Wireless Communications and Mobile Computing,2012,Conference,12,RV65184,Mary Mack,RV75957,Joe Martinez,H39769,pjcieplqaq,Chair,AR42146,sensor network,R59919,Rejected,"Network spanning algorithms, such as ZigBee-native and Stojmenovič, constitute a crucial element in the wireless sensor network design, by determining its potential for reliability and fault-tolerance. The interconnections between nodes have a great impact on the radio interference level present in such a network and may create a serious electromagnetic compatibility issue in some cases. It can be proved that the total interference incurred by a statistical node can be diminished in two ways: either by using directional antennas or by setting an upper limit on the duty cycle of each network node. Copyright © 2010 John Wiley & Sons, Ltd.",No
53e9b388b7602d9703e6dad6,Interference mitigation in WSN by means of directional antennas and duty cycle control,53f443e8dabfaedd74de45dc,Grzegorz Debita,Demo Paper,Workshop,S26090,Wireless Communications and Mobile Computing,2012,Conference,12,RV65184,Mary Mack,RV75957,Joe Martinez,H39769,pjcieplqaq,Chair,AR42146,sensor network,R59919,Rejected,"Network spanning algorithms, such as ZigBee-native and Stojmenovič, constitute a crucial element in the wireless sensor network design, by determining its potential for reliability and fault-tolerance. The interconnections between nodes have a great impact on the radio interference level present in such a network and may create a serious electromagnetic compatibility issue in some cases. It can be proved that the total interference incurred by a statistical node can be diminished in two ways: either by using directional antennas or by setting an upper limit on the duty cycle of each network node. Copyright © 2010 John Wiley & Sons, Ltd.",No
53e9b388b7602d9703e6d68b,Incorporating Codebook and Utterance Information in Cepstral Statistics Normalization Techniques for Robust Speech Recognition in Additive Noise Environments,548cfffedabfaed7b5fa47a7,Jeih-weih Hung,Full Paper,Regular Conference,S24548,IEEE Signal Process. Lett.,2009,Conference,16,RV67872,Brenda Johnson,RV76909,Christopher Cruz,H39533,lrhtybzwax,Chair,AR41276,cepstral statistics normalization techniques,R53893,Rejected,"Cepstral statistics normalization techniques have been shown to be very successful at improving the noise robustness of speech features. This letter proposes a hybrid-based scheme to achieve a more accurate estimate of the statistical information of features in these techniques. By properly integrating codebook and utterance knowledge, the resulting hybrid-based approach significantly outperforms conventional utterance-based, segment-based and codebook-based approaches in additive noise environments. Furthermore, the high-performance CS-HEQ can be implemented with a short delay and can thus be applied in real-time online systems.",No
53e9b388b7602d9703e6d68b,Incorporating Codebook and Utterance Information in Cepstral Statistics Normalization Techniques for Robust Speech Recognition in Additive Noise Environments,53f438abdabfaee4dc7982a1,Wen-hsiang Tu,Full Paper,Regular Conference,S24548,IEEE Signal Process. Lett.,2009,Conference,16,RV67872,Brenda Johnson,RV76909,Christopher Cruz,H39533,lrhtybzwax,Chair,AR41276,cepstral statistics normalization techniques,R53893,Rejected,"Cepstral statistics normalization techniques have been shown to be very successful at improving the noise robustness of speech features. This letter proposes a hybrid-based scheme to achieve a more accurate estimate of the statistical information of features in these techniques. By properly integrating codebook and utterance knowledge, the resulting hybrid-based approach significantly outperforms conventional utterance-based, segment-based and codebook-based approaches in additive noise environments. Furthermore, the high-performance CS-HEQ can be implemented with a short delay and can thus be applied in real-time online systems.",No
53e9b388b7602d9703e6d886,A Hierarchical Reliability Simulation Methodology For Ams Integrated Circuits And Systems,562d6e8645cedb3398e0087e,Hao Cai,Full Paper,None,S21493,JOURNAL OF LOW POWER ELECTRONICS,2012,Journal,8,RV60886,Valerie Harrington,RV72199,Adrienne Hill,H35059,ibdbixyazp,Editor,AR46585,Reliability,R57194,Accepted,"In this paper, we propose a methodology for simultaneously analyzing the ageing effects and process variations. Nominal ageing simulation and statistical methods are applied to reliability simulation of AMS integrated circuits and systems. Response surface modeling (RSM) is used to build direct relationship between process parameters and circuit/system performances. With Varied/fixed RSMs, designers can have reliability information of designed circuit/system. Also, this methodology has been developed with behavioral modeling for reliability consideration of large AMS circuits and systems (e.g., Sigma Delta modulator, RF front-end). Conventional Monte-Carlo (MC) method is infeasible in these complex circuits and systems. The methodology is validated with a series of circuits and systems in 65 nm CMOS technology: simple current mirrors, a dynamic comparator and a 2nd order continuous-time Sigma Delta analog-to-digital modulator. It is shown that this methodology can provide designers with reliability information graphically with a general perspective. It can achieve better simulation efficiency than traditional Monte-Carlo analysis, while still guaranteeing good simulation accuracy.",JOURNAL OF LOW POWER ELECTRONICS 8
53e9b388b7602d9703e6d886,A Hierarchical Reliability Simulation Methodology For Ams Integrated Circuits And Systems,53f474d0dabfaefedbba78f2,Hervé Petit,Full Paper,None,S21493,JOURNAL OF LOW POWER ELECTRONICS,2012,Journal,8,RV60886,Valerie Harrington,RV72199,Adrienne Hill,H35059,ibdbixyazp,Editor,AR46585,Reliability,R57194,Accepted,"In this paper, we propose a methodology for simultaneously analyzing the ageing effects and process variations. Nominal ageing simulation and statistical methods are applied to reliability simulation of AMS integrated circuits and systems. Response surface modeling (RSM) is used to build direct relationship between process parameters and circuit/system performances. With Varied/fixed RSMs, designers can have reliability information of designed circuit/system. Also, this methodology has been developed with behavioral modeling for reliability consideration of large AMS circuits and systems (e.g., Sigma Delta modulator, RF front-end). Conventional Monte-Carlo (MC) method is infeasible in these complex circuits and systems. The methodology is validated with a series of circuits and systems in 65 nm CMOS technology: simple current mirrors, a dynamic comparator and a 2nd order continuous-time Sigma Delta analog-to-digital modulator. It is shown that this methodology can provide designers with reliability information graphically with a general perspective. It can achieve better simulation efficiency than traditional Monte-Carlo analysis, while still guaranteeing good simulation accuracy.",JOURNAL OF LOW POWER ELECTRONICS 8
53e9b388b7602d9703e6d886,A Hierarchical Reliability Simulation Methodology For Ams Integrated Circuits And Systems,53f3a467dabfae4b34acf4fc,Jean-François Naviner,Full Paper,None,S21493,JOURNAL OF LOW POWER ELECTRONICS,2012,Journal,8,RV60886,Valerie Harrington,RV72199,Adrienne Hill,H35059,ibdbixyazp,Editor,AR46585,Reliability,R57194,Accepted,"In this paper, we propose a methodology for simultaneously analyzing the ageing effects and process variations. Nominal ageing simulation and statistical methods are applied to reliability simulation of AMS integrated circuits and systems. Response surface modeling (RSM) is used to build direct relationship between process parameters and circuit/system performances. With Varied/fixed RSMs, designers can have reliability information of designed circuit/system. Also, this methodology has been developed with behavioral modeling for reliability consideration of large AMS circuits and systems (e.g., Sigma Delta modulator, RF front-end). Conventional Monte-Carlo (MC) method is infeasible in these complex circuits and systems. The methodology is validated with a series of circuits and systems in 65 nm CMOS technology: simple current mirrors, a dynamic comparator and a 2nd order continuous-time Sigma Delta analog-to-digital modulator. It is shown that this methodology can provide designers with reliability information graphically with a general perspective. It can achieve better simulation efficiency than traditional Monte-Carlo analysis, while still guaranteeing good simulation accuracy.",JOURNAL OF LOW POWER ELECTRONICS 8
53e9b388b7602d9703e6d883,Post-placement power optimization with multi-bit flip-flops,53f47d2adabfaec09f29552c,Yao-Tsung Chang,Demo Paper,Symposium,S20472,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2011,Conference,30,RV61753,Kelly Rodriguez,RV72769,Jessica Sosa,H31618,pqqgvjxyqq,Chair,AR45931,design objective,R53149,Rejected,"Optimization for power is always one of the most important design objectives in modern nanometer IC design. Recent studies have shown the effectiveness of applying multi-bit flip-flops to save the power consumption of the clock network. However, all the previous works applied multi-bit flip-flops at earlier design stages, which could be very difficult to carry out the trade-off among power, timing, and other design objectives. This paper presents a novel power optimization method by incrementally applying more multi-bit flip-flops at the post-placement stage to gain more clock power saving while considering the placement density and timing slack constraints, and simultaneously minimizing interconnecting wirelength. Experimental results based on the industry benchmark circuits show that our approach is very effective and efficient, which can be seamlessly integrated in modern design flow.

",No
53e9b388b7602d9703e6d883,Post-placement power optimization with multi-bit flip-flops,54878fafdabfaed7b5fa30e2,Chih-Cheng Hsu,Demo Paper,Symposium,S20472,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2011,Conference,30,RV61753,Kelly Rodriguez,RV72769,Jessica Sosa,H31618,pqqgvjxyqq,Chair,AR45931,design objective,R53149,Rejected,"Optimization for power is always one of the most important design objectives in modern nanometer IC design. Recent studies have shown the effectiveness of applying multi-bit flip-flops to save the power consumption of the clock network. However, all the previous works applied multi-bit flip-flops at earlier design stages, which could be very difficult to carry out the trade-off among power, timing, and other design objectives. This paper presents a novel power optimization method by incrementally applying more multi-bit flip-flops at the post-placement stage to gain more clock power saving while considering the placement density and timing slack constraints, and simultaneously minimizing interconnecting wirelength. Experimental results based on the industry benchmark circuits show that our approach is very effective and efficient, which can be seamlessly integrated in modern design flow.

",No
53e9b388b7602d9703e6d883,Post-placement power optimization with multi-bit flip-flops,53f466aadabfaee02ad958ed,Mark Po-Hung Lin,Demo Paper,Symposium,S20472,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2011,Conference,30,RV61753,Kelly Rodriguez,RV72769,Jessica Sosa,H31618,pqqgvjxyqq,Chair,AR45931,design objective,R53149,Rejected,"Optimization for power is always one of the most important design objectives in modern nanometer IC design. Recent studies have shown the effectiveness of applying multi-bit flip-flops to save the power consumption of the clock network. However, all the previous works applied multi-bit flip-flops at earlier design stages, which could be very difficult to carry out the trade-off among power, timing, and other design objectives. This paper presents a novel power optimization method by incrementally applying more multi-bit flip-flops at the post-placement stage to gain more clock power saving while considering the placement density and timing slack constraints, and simultaneously minimizing interconnecting wirelength. Experimental results based on the industry benchmark circuits show that our approach is very effective and efficient, which can be seamlessly integrated in modern design flow.

",No
53e9b388b7602d9703e6d883,Post-placement power optimization with multi-bit flip-flops,53f44a9adabfaee2a1d45c15,Yu-Wen Tsai,Demo Paper,Symposium,S20472,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2011,Conference,30,RV61753,Kelly Rodriguez,RV72769,Jessica Sosa,H31618,pqqgvjxyqq,Chair,AR45931,design objective,R53149,Rejected,"Optimization for power is always one of the most important design objectives in modern nanometer IC design. Recent studies have shown the effectiveness of applying multi-bit flip-flops to save the power consumption of the clock network. However, all the previous works applied multi-bit flip-flops at earlier design stages, which could be very difficult to carry out the trade-off among power, timing, and other design objectives. This paper presents a novel power optimization method by incrementally applying more multi-bit flip-flops at the post-placement stage to gain more clock power saving while considering the placement density and timing slack constraints, and simultaneously minimizing interconnecting wirelength. Experimental results based on the industry benchmark circuits show that our approach is very effective and efficient, which can be seamlessly integrated in modern design flow.

",No
53e9b388b7602d9703e6d883,Post-placement power optimization with multi-bit flip-flops,53f437b9dabfaee2a1cf2a48,Sheng-Fong Chen,Demo Paper,Symposium,S20472,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2011,Conference,30,RV61753,Kelly Rodriguez,RV72769,Jessica Sosa,H31618,pqqgvjxyqq,Chair,AR45931,design objective,R53149,Rejected,"Optimization for power is always one of the most important design objectives in modern nanometer IC design. Recent studies have shown the effectiveness of applying multi-bit flip-flops to save the power consumption of the clock network. However, all the previous works applied multi-bit flip-flops at earlier design stages, which could be very difficult to carry out the trade-off among power, timing, and other design objectives. This paper presents a novel power optimization method by incrementally applying more multi-bit flip-flops at the post-placement stage to gain more clock power saving while considering the placement density and timing slack constraints, and simultaneously minimizing interconnecting wirelength. Experimental results based on the industry benchmark circuits show that our approach is very effective and efficient, which can be seamlessly integrated in modern design flow.

",No
53e9b388b7602d9703e6db24,Moment-based spectral analysis of large-scale networks using local structural information,56070f6945cedb33969e855d,Victor M. Preciado,Full Paper,None,S21110,IEEE/ACM Trans. Netw.,2013,Journal,21,RV68826,Mr. Mark Edwards,RV76104,Suzanne Turner,H35661,rrgkguwlkw,Editor,AR48934,Eigenvalues and eigenfunctions,R50323,Rejected,"The eigenvalues of matrices representing the structure of large-scale complex networks present a wide range of applications, fromthe analysis of dynamical processes taking place in the network to spectral techniques aiming to rank the importance of nodes in the network. A common approach to study the relationship between the structure of a network and its eigenvalues is to use synthetic random networks in which structural properties of interest, such as degree distributions, are prescribed. Although very common, synthetic models present two major flaws: 1) These models are only suitable to study a very limited range of structural properties; and 2) they implicitly induce structural properties that are not directly controlled and can deceivingly influence the network eigenvalue spectrum. In this paper, we propose an alternative approach to overcome these limitations. Our approach is not based on synthetic models. Instead, we use algebraic graph theory and convex optimization to study how structural properties influence the spectrum of eigenvalues of the network. Using our approach, we can compute, with low computational overhead, global spectral properties of a network from its local structural properties. We illustrate our approach by studying how structural properties of online social networks influence their eigenvalue spectra.",No
53e9b388b7602d9703e6db24,Moment-based spectral analysis of large-scale networks using local structural information,548808d9dabfaed7b5fa3466,Ali Jadbabaie,Full Paper,None,S21110,IEEE/ACM Trans. Netw.,2013,Journal,21,RV68826,Mr. Mark Edwards,RV76104,Suzanne Turner,H35661,rrgkguwlkw,Editor,AR48934,Eigenvalues and eigenfunctions,R50323,Rejected,"The eigenvalues of matrices representing the structure of large-scale complex networks present a wide range of applications, fromthe analysis of dynamical processes taking place in the network to spectral techniques aiming to rank the importance of nodes in the network. A common approach to study the relationship between the structure of a network and its eigenvalues is to use synthetic random networks in which structural properties of interest, such as degree distributions, are prescribed. Although very common, synthetic models present two major flaws: 1) These models are only suitable to study a very limited range of structural properties; and 2) they implicitly induce structural properties that are not directly controlled and can deceivingly influence the network eigenvalue spectrum. In this paper, we propose an alternative approach to overcome these limitations. Our approach is not based on synthetic models. Instead, we use algebraic graph theory and convex optimization to study how structural properties influence the spectrum of eigenvalues of the network. Using our approach, we can compute, with low computational overhead, global spectral properties of a network from its local structural properties. We illustrate our approach by studying how structural properties of online social networks influence their eigenvalue spectra.",No
53e9b388b7602d9703e6d6ba,Imaging Arterial Fibres Using Diffusion Tensor Imaging - Feasibility Study and Preliminary Results,562c80fb45cedb3398c4214c,Vittoria Flamini,Full Paper,Expert Group,S29956,EURASIP J. Adv. Sig. Proc.,2010,Conference,2010,RV60546,Francisco Love,RV72315,Toni Johnson,H31149,frmtragoft,Chair,AR42379,bioengineering,R51998,Accepted,"MR diffusion tensor imaging (DTI) was used to analyze the fibrous structure of aortic tissue. A fresh porcine aorta was imaged at 7T using a spin echo sequence with the following parameters: matrix 128  Open image in new window  128 pixel; slice thickness 0.5 mm; interslice spacing 0.1 mm; number of slices 16; echo time 20.3 s; field of view 28 mm  Open image in new window  28 mm. Eigenvectors from the diffusion tensor images were calculated for the central image slice and the averaged tensors and the eigenvector corresponding to the largest eigenvalue showed two distinct angles corresponding to near  Open image in new window  and  Open image in new window  to the transverse plane of the aorta. Fibre tractography within the aortic volume imaged confirmed that fibre angles were oriented helically with lead angles of  Open image in new window  and  Open image in new window . The findings correspond to current histological and microscopy data on the fibrous structure of aortic tissue, and therefore the eigenvector maps and fibre tractography appear to reflect the alignment of the fibers in the aorta. In view of current efforts to develop noninvasive diagnostic tools for cardiovascular diseases, DTI may offer a technique to assess the structural properties of arterial tissue and hence any changes or degradation in arterial tissue.",EURASIP J. Adv. Sig. Proc. 2010
53e9b388b7602d9703e6d6ba,Imaging Arterial Fibres Using Diffusion Tensor Imaging - Feasibility Study and Preliminary Results,53f475eadabfaec09f27c92e,Christian M. Kerskens,Full Paper,Expert Group,S29956,EURASIP J. Adv. Sig. Proc.,2010,Conference,2010,RV60546,Francisco Love,RV72315,Toni Johnson,H31149,frmtragoft,Chair,AR42379,bioengineering,R51998,Accepted,"MR diffusion tensor imaging (DTI) was used to analyze the fibrous structure of aortic tissue. A fresh porcine aorta was imaged at 7T using a spin echo sequence with the following parameters: matrix 128  Open image in new window  128 pixel; slice thickness 0.5 mm; interslice spacing 0.1 mm; number of slices 16; echo time 20.3 s; field of view 28 mm  Open image in new window  28 mm. Eigenvectors from the diffusion tensor images were calculated for the central image slice and the averaged tensors and the eigenvector corresponding to the largest eigenvalue showed two distinct angles corresponding to near  Open image in new window  and  Open image in new window  to the transverse plane of the aorta. Fibre tractography within the aortic volume imaged confirmed that fibre angles were oriented helically with lead angles of  Open image in new window  and  Open image in new window . The findings correspond to current histological and microscopy data on the fibrous structure of aortic tissue, and therefore the eigenvector maps and fibre tractography appear to reflect the alignment of the fibers in the aorta. In view of current efforts to develop noninvasive diagnostic tools for cardiovascular diseases, DTI may offer a technique to assess the structural properties of arterial tissue and hence any changes or degradation in arterial tissue.",EURASIP J. Adv. Sig. Proc. 2010
53e9b388b7602d9703e6d6ba,Imaging Arterial Fibres Using Diffusion Tensor Imaging - Feasibility Study and Preliminary Results,53f4d190dabfaef010f8150d,Kevin Mattheus Moerman,Full Paper,Expert Group,S29956,EURASIP J. Adv. Sig. Proc.,2010,Conference,2010,RV60546,Francisco Love,RV72315,Toni Johnson,H31149,frmtragoft,Chair,AR42379,bioengineering,R51998,Accepted,"MR diffusion tensor imaging (DTI) was used to analyze the fibrous structure of aortic tissue. A fresh porcine aorta was imaged at 7T using a spin echo sequence with the following parameters: matrix 128  Open image in new window  128 pixel; slice thickness 0.5 mm; interslice spacing 0.1 mm; number of slices 16; echo time 20.3 s; field of view 28 mm  Open image in new window  28 mm. Eigenvectors from the diffusion tensor images were calculated for the central image slice and the averaged tensors and the eigenvector corresponding to the largest eigenvalue showed two distinct angles corresponding to near  Open image in new window  and  Open image in new window  to the transverse plane of the aorta. Fibre tractography within the aortic volume imaged confirmed that fibre angles were oriented helically with lead angles of  Open image in new window  and  Open image in new window . The findings correspond to current histological and microscopy data on the fibrous structure of aortic tissue, and therefore the eigenvector maps and fibre tractography appear to reflect the alignment of the fibers in the aorta. In view of current efforts to develop noninvasive diagnostic tools for cardiovascular diseases, DTI may offer a technique to assess the structural properties of arterial tissue and hence any changes or degradation in arterial tissue.",EURASIP J. Adv. Sig. Proc. 2010
53e9b388b7602d9703e6d6ba,Imaging Arterial Fibres Using Diffusion Tensor Imaging - Feasibility Study and Preliminary Results,53f44914dabfaee4dc7dc005,Ciaran K. Simms,Full Paper,Expert Group,S29956,EURASIP J. Adv. Sig. Proc.,2010,Conference,2010,RV60546,Francisco Love,RV72315,Toni Johnson,H31149,frmtragoft,Chair,AR42379,bioengineering,R51998,Accepted,"MR diffusion tensor imaging (DTI) was used to analyze the fibrous structure of aortic tissue. A fresh porcine aorta was imaged at 7T using a spin echo sequence with the following parameters: matrix 128  Open image in new window  128 pixel; slice thickness 0.5 mm; interslice spacing 0.1 mm; number of slices 16; echo time 20.3 s; field of view 28 mm  Open image in new window  28 mm. Eigenvectors from the diffusion tensor images were calculated for the central image slice and the averaged tensors and the eigenvector corresponding to the largest eigenvalue showed two distinct angles corresponding to near  Open image in new window  and  Open image in new window  to the transverse plane of the aorta. Fibre tractography within the aortic volume imaged confirmed that fibre angles were oriented helically with lead angles of  Open image in new window  and  Open image in new window . The findings correspond to current histological and microscopy data on the fibrous structure of aortic tissue, and therefore the eigenvector maps and fibre tractography appear to reflect the alignment of the fibers in the aorta. In view of current efforts to develop noninvasive diagnostic tools for cardiovascular diseases, DTI may offer a technique to assess the structural properties of arterial tissue and hence any changes or degradation in arterial tissue.",EURASIP J. Adv. Sig. Proc. 2010
53e9b388b7602d9703e6d6ba,Imaging Arterial Fibres Using Diffusion Tensor Imaging - Feasibility Study and Preliminary Results,53f43762dabfaeb2ac05bdbd,Caitríona Lally,Full Paper,Expert Group,S29956,EURASIP J. Adv. Sig. Proc.,2010,Conference,2010,RV60546,Francisco Love,RV72315,Toni Johnson,H31149,frmtragoft,Chair,AR42379,bioengineering,R51998,Accepted,"MR diffusion tensor imaging (DTI) was used to analyze the fibrous structure of aortic tissue. A fresh porcine aorta was imaged at 7T using a spin echo sequence with the following parameters: matrix 128  Open image in new window  128 pixel; slice thickness 0.5 mm; interslice spacing 0.1 mm; number of slices 16; echo time 20.3 s; field of view 28 mm  Open image in new window  28 mm. Eigenvectors from the diffusion tensor images were calculated for the central image slice and the averaged tensors and the eigenvector corresponding to the largest eigenvalue showed two distinct angles corresponding to near  Open image in new window  and  Open image in new window  to the transverse plane of the aorta. Fibre tractography within the aortic volume imaged confirmed that fibre angles were oriented helically with lead angles of  Open image in new window  and  Open image in new window . The findings correspond to current histological and microscopy data on the fibrous structure of aortic tissue, and therefore the eigenvector maps and fibre tractography appear to reflect the alignment of the fibers in the aorta. In view of current efforts to develop noninvasive diagnostic tools for cardiovascular diseases, DTI may offer a technique to assess the structural properties of arterial tissue and hence any changes or degradation in arterial tissue.",EURASIP J. Adv. Sig. Proc. 2010
53e9b388b7602d9703e6d705,The emerging discourse of knowledge management: a new dawn for information science research?,53f4331bdabfaee0d9b44407,Ashok Jashapara,Demo Paper,Expert Group,S28698,J. Information Science,2005,Conference,31,RV67174,Mark Miller,RV76630,Wesley Pearson,H33359,ddfillovdc,Chair,AR40442,resituating knowledge management,R55225,Rejected,"Information science has played a limited role in providing fresh insights into the emerging interdisciplinary discourse of knowledge management. There are opportunities and challenges posed by the new discourse. An analysis of the knowledge management literature within information science journals shows a need for a wider academic perspective and a more philosophically grounded one. As the current knowledge management discourse is fragmented, we propose an integrative, interdisciplinary framework that would be useful for resituating knowledge management among scholars and practitioners. The principal pillars of this theoretical framework are organizational learning; systems and technology; and culture and strategy. Current criticisms of the knowledge management discourse are closely examined. The notions of knowledge sharing, social capital and organizational learning processes provide fundamental insights into knowledge management. These processes are explored from a social, cognitive and technological perspective.",No
53e9b388b7602d9703e6d707,A design pattern coupling role and component concepts: Application to medical software,53f464d3dabfaec09f2396f9,Jean-Baptiste Fasquel,Demo Paper,Expert Group,S20235,Journal of Systems and Software,2011,Conference,84,RV68566,Shane Hickman,RV77166,Kevin Ortega,H34934,totmmayjwa,Chair,AR49576,code element,R55417,Accepted,"Abstract: One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components.",Journal of Systems and Software 84
53e9b388b7602d9703e6d707,A design pattern coupling role and component concepts: Application to medical software,53f45204dabfaee4dc80069a,Johan Moreau,Demo Paper,Expert Group,S20235,Journal of Systems and Software,2011,Conference,84,RV68566,Shane Hickman,RV77166,Kevin Ortega,H34934,totmmayjwa,Chair,AR49576,code element,R55417,Accepted,"Abstract: One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components.",Journal of Systems and Software 84
53e9b388b7602d9703e6dbcb,Controllability and stabilizability of a networked control system with periodic communication constraints.,53f80f4edabfae938c6fa307,Tatsuo Suzuki,Full Paper,None,S24889,Systems & Control Letters,2011,Journal,60,RV61403,Jacob Miles,RV70268,Thomas Cruz,H30925,tvxmgjehtd,Editor,AR42990,Networked control system,R52494,Accepted,"This paper proposes a new model for a networked control system and considers its controllability and stabilizability. To control a linear time-invariant discrete-time plant via a bus with limited capacity, we introduce a hold device and a communication sequence which follows a given ω-periodic pattern. Incorporating the communication sequences and hold device into the original plant amounts to extending the original time-invariant state equation to a ω-periodic one which has the higher order. We assume that the communication sequence is admissible. It is shown that controllability and stabilizability of the plant are preserved in the periodic extended system under the assumption that the zeros of the communication sequence characteristic polynomial do not coincide with the eigenvalues of the plant.",Systems & Control Letters 60
53e9b388b7602d9703e6dbcb,Controllability and stabilizability of a networked control system with periodic communication constraints.,5609325745cedb3396e4f3d7,Michio Kono,Full Paper,None,S24889,Systems & Control Letters,2011,Journal,60,RV61403,Jacob Miles,RV70268,Thomas Cruz,H30925,tvxmgjehtd,Editor,AR42990,Networked control system,R52494,Accepted,"This paper proposes a new model for a networked control system and considers its controllability and stabilizability. To control a linear time-invariant discrete-time plant via a bus with limited capacity, we introduce a hold device and a communication sequence which follows a given ω-periodic pattern. Incorporating the communication sequences and hold device into the original plant amounts to extending the original time-invariant state equation to a ω-periodic one which has the higher order. We assume that the communication sequence is admissible. It is shown that controllability and stabilizability of the plant are preserved in the periodic extended system under the assumption that the zeros of the communication sequence characteristic polynomial do not coincide with the eigenvalues of the plant.",Systems & Control Letters 60
53e9b388b7602d9703e6dbcb,Controllability and stabilizability of a networked control system with periodic communication constraints.,53f476bbdabfaedf436865cb,Nobuya Takahashi,Full Paper,None,S24889,Systems & Control Letters,2011,Journal,60,RV61403,Jacob Miles,RV70268,Thomas Cruz,H30925,tvxmgjehtd,Editor,AR42990,Networked control system,R52494,Accepted,"This paper proposes a new model for a networked control system and considers its controllability and stabilizability. To control a linear time-invariant discrete-time plant via a bus with limited capacity, we introduce a hold device and a communication sequence which follows a given ω-periodic pattern. Incorporating the communication sequences and hold device into the original plant amounts to extending the original time-invariant state equation to a ω-periodic one which has the higher order. We assume that the communication sequence is admissible. It is shown that controllability and stabilizability of the plant are preserved in the periodic extended system under the assumption that the zeros of the communication sequence characteristic polynomial do not coincide with the eigenvalues of the plant.",Systems & Control Letters 60
53e9b388b7602d9703e6dbcb,Controllability and stabilizability of a networked control system with periodic communication constraints.,53f430e7dabfaee1c0a60f3b,Osamu Sato,Full Paper,None,S24889,Systems & Control Letters,2011,Journal,60,RV61403,Jacob Miles,RV70268,Thomas Cruz,H30925,tvxmgjehtd,Editor,AR42990,Networked control system,R52494,Accepted,"This paper proposes a new model for a networked control system and considers its controllability and stabilizability. To control a linear time-invariant discrete-time plant via a bus with limited capacity, we introduce a hold device and a communication sequence which follows a given ω-periodic pattern. Incorporating the communication sequences and hold device into the original plant amounts to extending the original time-invariant state equation to a ω-periodic one which has the higher order. We assume that the communication sequence is admissible. It is shown that controllability and stabilizability of the plant are preserved in the periodic extended system under the assumption that the zeros of the communication sequence characteristic polynomial do not coincide with the eigenvalues of the plant.",Systems & Control Letters 60
53e9b388b7602d9703e6d73f,Eastern promise? east london transformations and the state of surveillance,53f47c48dabfaee4dc8a17a8,Pete Fussey,Full Paper,None,S22881,Information Polity,2012,Journal,17,RV69988,Suzanne Clark,RV78662,Laura Hansen,H38807,qwwnvcaqlz,Editor,AR41230,urban regeneration scheme,R50575,Accepted,"Largely catalysed by hosting the XXX Summer Olympic Games, East London is currently experiencing significant urban regeneration at a rate not seen since the period of post-war reconstruction. In doing so, a series of processes that serve to heighten the intensity of cameras in this already saturated video surveillance landscape are occurring. At the same time, these developments, whilst affecting East London, demonstrate a number of key issues, debates and crises germane to the dissemination and operation of video surveillance across the UK as a whole. These include the intensification and cohesion of video surveillance networks; the role of CCTV in urban regeneration schemes; tensions between disparate applications of CCTV and aspirations for a coherent regulatory framework; and, crucially, how CCTV can be justified at a time of severe economic crisis. The paper explores these issues via the identification and analysis of three broad processes operating in East London: the 'additionality' of Olympic-related surveillance measures; the centripetal surveillance-pull of Olympic-related regeneration programmes; and the co-option and integration of extant CCTV facilities. The strong emphasis on surveillant economies of scale and the integration of existing surveillance infrastructures invite reflection on post-Foucauldian theorisations of networked 'societies of control'.",Information Polity 17
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f38646dabfae4b34a14fde,Catherine Quantin,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f45d7fdabfaee43ecd5b92,David-Olivier Jaquet-Chiffelle,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f434d8dabfaee1c0a8f8ee,Gouenou Coatrieux,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f4492edabfaedf435d7c69,Eric Benzenine,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f3a11edabfae4b34abab04,Bertrand Auverlot,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b38eb7602d9703e6dc03,Medical record: systematic centralization versus secure on demand aggregation.,53f45249dabfaedd74e1dcf9,François-André Allaert,Demo Paper,None,S22730,BMC Med. Inf. & Decision Making,2011,Journal,11,RV61727,Amy Hughes,RV70461,Joshua Clarke Jr.,H37456,utujwstmtd,Editor,AR45012,computer security,R55573,Rejected,"As patients often see the data of their medical histories scattered among various medical records hosted in several health-care establishments, the purpose of our multidisciplinary study was to define a pragmatic and secure on-demand based system able to gather this information, with no risk of breaching confidentiality, and to relay it to a medical professional who asked for the information via a specific search engine.Scattered data are often heterogeneous, which makes the task of gathering information very hard. Two methods can be compared: trying to solve the problem by standardizing and centralizing all the information about every patient in a single Medical Record system or trying to use the data as is and find a way to obtain the most complete and the most accurate information. Given the failure of the first approach, due to the lack of standardization or privacy and security problems, for example, we propose an alternative that relies on the current state of affairs: an on-demand system, using a specific search engine that is able to retrieve information from the different medical records of a single patient.We describe the function of Medical Record Search Engines (MRSE), which are able to retrieve all the available information regarding a patient who has been hospitalized in different hospitals and to provide this information to health professionals upon request. MRSEs use pseudonymized patient identities and thus never have access to the patient's identity. However, though the system would be easy to implement as it by-passes many of the difficulties associated with a centralized architecture, the health professional would have to validate the information, i.e. read all of the information and create his own synthesis and possibly reject extra data, which could be a drawback. We thus propose various feasible improvements, based on the implementation of several tools in our on-demand based system.A system that gathers all of the currently available information regarding a patient on the request of health-care professionals could be of great interest. This low-cost pragmatic alternative to centralized medical records could be developed quickly and easily. It could also be designed to include extra features and should thus be considered by health authorities.",No
53e9b388b7602d9703e6d908,Category-specific occipitotemporal activation during face perception in dyslexic individuals: an MEG study.,53f42ad5dabfaeb22f3dd1d5,A Tarkiainen,Full Paper,None,S24632,NeuroImage,2003,Journal,19,RV62663,Melissa Long,RV76443,Lisa Baird PhD,H39106,tclfzbvwag,Editor,AR40610,Magnetoencephalography,R54971,Accepted,"In dyslexia, it is consistently found that letter strings produce an abnormally weak or no response in the left occipitotemporal cortex. Time-sensitive imaging techniques have located this deficit to the category-specific processing stage at about 150 ms after stimulus presentation. The typically reported behavioral impairments in dyslexia suggest that the lack of occipitotemporal activation is specific to reading. It could, however, also reflect a more general dysfunction in the left inferior occipitotemporal cortex or in the time window of category-specific activation (150 to 200 ms). As early cortical processing of faces follows a sequence practically identical to that for letter strings, both in location and in timing, we investigated these possibilities by comparing face-specific occipitotemporal activations in dyslexic and non-reading-impaired subjects. We found that both the stage of general visual feature analysis at about 100 ms and the earliest face-specific activation at about 150 ms were essentially normal in the dyslexic individuals. The present results emphasize the special nature of the occipitotemporal abnormality to letter strings in dyslexia. However, in behavioral tests dyslexic subjects were slower and more error-prone than non-reading-impaired subjects in judging the similarity of faces and geometrical shapes. This effect may be related to reduced activation of the right parietotemporal cortex at about 250 ms after stimulus onset.",NeuroImage 19
53e9b388b7602d9703e6d908,Category-specific occipitotemporal activation during face perception in dyslexic individuals: an MEG study.,53f42b56dabfaeb22f3e795f,P Helenius,Full Paper,None,S24632,NeuroImage,2003,Journal,19,RV62663,Melissa Long,RV76443,Lisa Baird PhD,H39106,tclfzbvwag,Editor,AR40610,Magnetoencephalography,R54971,Accepted,"In dyslexia, it is consistently found that letter strings produce an abnormally weak or no response in the left occipitotemporal cortex. Time-sensitive imaging techniques have located this deficit to the category-specific processing stage at about 150 ms after stimulus presentation. The typically reported behavioral impairments in dyslexia suggest that the lack of occipitotemporal activation is specific to reading. It could, however, also reflect a more general dysfunction in the left inferior occipitotemporal cortex or in the time window of category-specific activation (150 to 200 ms). As early cortical processing of faces follows a sequence practically identical to that for letter strings, both in location and in timing, we investigated these possibilities by comparing face-specific occipitotemporal activations in dyslexic and non-reading-impaired subjects. We found that both the stage of general visual feature analysis at about 100 ms and the earliest face-specific activation at about 150 ms were essentially normal in the dyslexic individuals. The present results emphasize the special nature of the occipitotemporal abnormality to letter strings in dyslexia. However, in behavioral tests dyslexic subjects were slower and more error-prone than non-reading-impaired subjects in judging the similarity of faces and geometrical shapes. This effect may be related to reduced activation of the right parietotemporal cortex at about 250 ms after stimulus onset.",NeuroImage 19
53e9b388b7602d9703e6d908,Category-specific occipitotemporal activation during face perception in dyslexic individuals: an MEG study.,5405beaadabfae450f3cd31a,R Salmelin,Full Paper,None,S24632,NeuroImage,2003,Journal,19,RV62663,Melissa Long,RV76443,Lisa Baird PhD,H39106,tclfzbvwag,Editor,AR40610,Magnetoencephalography,R54971,Accepted,"In dyslexia, it is consistently found that letter strings produce an abnormally weak or no response in the left occipitotemporal cortex. Time-sensitive imaging techniques have located this deficit to the category-specific processing stage at about 150 ms after stimulus presentation. The typically reported behavioral impairments in dyslexia suggest that the lack of occipitotemporal activation is specific to reading. It could, however, also reflect a more general dysfunction in the left inferior occipitotemporal cortex or in the time window of category-specific activation (150 to 200 ms). As early cortical processing of faces follows a sequence practically identical to that for letter strings, both in location and in timing, we investigated these possibilities by comparing face-specific occipitotemporal activations in dyslexic and non-reading-impaired subjects. We found that both the stage of general visual feature analysis at about 100 ms and the earliest face-specific activation at about 150 ms were essentially normal in the dyslexic individuals. The present results emphasize the special nature of the occipitotemporal abnormality to letter strings in dyslexia. However, in behavioral tests dyslexic subjects were slower and more error-prone than non-reading-impaired subjects in judging the similarity of faces and geometrical shapes. This effect may be related to reduced activation of the right parietotemporal cortex at about 250 ms after stimulus onset.",NeuroImage 19
53e9b388b7602d9703e6d76e,Thermal Consideration In Led Array Design For Lcd Backlight Unit Applications,5631683845cedb3399ddbd9a,Bong-Ryeol Park,Demo Paper,Expert Group,S24528,IEICE ELECTRONICS EXPRESS,2010,Conference,7,RV68829,Tiffany Hardin,RV71001,Holly Wolfe,H32708,xcporifwgm,Chair,AR43262,backlight unit,R53305,Rejected,"Thermal effects were investigated in a liquid crystal display (LCD) backlight unit (BLU) that was composed of light emitting diode (LED) arrays. The driving power for a single LED in a BLU is determined by the desired brightness of the LCD panel and the pitch distance between adjacent LEDs. As the pitch distance increases, individual LEDs need to be driven by a higher power in order to maintain the same brightness. Since the junction temperature of an LED plays an important role in LED performance, such as optical efficiency, intensity degradation, and lifetime, the maximum driving power for an LED should be limited by a critical level of the junction temperature. A guideline for designing LED BLU panels taking account of the junction temperature is presented using various parameters of LED arrays.",No
53e9b388b7602d9703e6d76e,Thermal Consideration In Led Array Design For Lcd Backlight Unit Applications,53f42cd9dabfaee1c0a2bbf3,Ho-Young Cha,Demo Paper,Expert Group,S24528,IEICE ELECTRONICS EXPRESS,2010,Conference,7,RV68829,Tiffany Hardin,RV71001,Holly Wolfe,H32708,xcporifwgm,Chair,AR43262,backlight unit,R53305,Rejected,"Thermal effects were investigated in a liquid crystal display (LCD) backlight unit (BLU) that was composed of light emitting diode (LED) arrays. The driving power for a single LED in a BLU is determined by the desired brightness of the LCD panel and the pitch distance between adjacent LEDs. As the pitch distance increases, individual LEDs need to be driven by a higher power in order to maintain the same brightness. Since the junction temperature of an LED plays an important role in LED performance, such as optical efficiency, intensity degradation, and lifetime, the maximum driving power for an LED should be limited by a critical level of the junction temperature. A guideline for designing LED BLU panels taking account of the junction temperature is presented using various parameters of LED arrays.",No
53e9b388b7602d9703e6d773,Improving Head Movement Tolerance of Cross-Ratio Based Eye Trackers,53f43b64dabfaee1c0acf0a1,Flavio L. Coutinho,Demo Paper,None,S22922,International Journal of Computer Vision,2013,Journal,101,RV64512,Sarah Williams,RV74231,Javier Lopez,H31388,hvmhnctses,Editor,AR45590,Eye tracking,R51965,Accepted,"When first introduced, the cross-ratio (CR) based remote eye tracking method offered many attractive features for natural human gaze-based interaction, such as simple camera setup, no user calibration, and invariance to head motion. However, due to many simplification assumptions, current CR-based methods are still sensitive to head movements. In this paper, we revisit the CR-based method and introduce two new extensions to improve the robustness of the method to head motion. The first method dynamically compensates for scale changes in the corneal reflection pattern, and the second method estimates true coplanar eye features so that the cross-ratio can be applied. We present real-time implementations of both systems, and compare the performance of these new methods using simulations and user experiments. Our results show a significant improvement in robustness to head motion and, for the user experiments in particular, an average reduction of up to 40 % in gaze estimation error was observed.",International Journal of Computer Vision 101
53e9b388b7602d9703e6d773,Improving Head Movement Tolerance of Cross-Ratio Based Eye Trackers,53f4307adabfaee2a1ca3c2f,Carlos H. Morimoto,Demo Paper,None,S22922,International Journal of Computer Vision,2013,Journal,101,RV64512,Sarah Williams,RV74231,Javier Lopez,H31388,hvmhnctses,Editor,AR45590,Eye tracking,R51965,Accepted,"When first introduced, the cross-ratio (CR) based remote eye tracking method offered many attractive features for natural human gaze-based interaction, such as simple camera setup, no user calibration, and invariance to head motion. However, due to many simplification assumptions, current CR-based methods are still sensitive to head movements. In this paper, we revisit the CR-based method and introduce two new extensions to improve the robustness of the method to head motion. The first method dynamically compensates for scale changes in the corneal reflection pattern, and the second method estimates true coplanar eye features so that the cross-ratio can be applied. We present real-time implementations of both systems, and compare the performance of these new methods using simulations and user experiments. Our results show a significant improvement in robustness to head motion and, for the user experiments in particular, an average reduction of up to 40 % in gaze estimation error was observed.",International Journal of Computer Vision 101
53e9b38eb7602d9703e6dc41,Applying a usage control model in an operating system kernel,53f36b4adabfae4b349acf3b,Rafael Teigão,Full Paper,Expert Group,S25657,J. Network and Computer Applications,2011,Conference,34,RV63542,Megan Carr,RV76072,Tyler Norman,H37960,okevqrlosf,Chair,AR45686,traditional access control,R52980,Rejected,"Operating systems traditionally use access control mechanisms to manage access to system resources like files, network connections, and memory areas. However, classic access control models are not suitable for regulating access to the diversity of ways data is available and used today. Modern usage control models go beyond traditional access control, addressing its limitations related to attribute mutability and continuous usage permission validation. The recently proposed UCONABC model establishes a predicate-based framework to satisfy the new access/usage control needs in computing systems. This paper defines a usage control model based on UCONABC and describes a framework to implement it in an operating system kernel, on top of the existing DAC mechanism. A language for representing usage control entities and rules is also proposed, and some typical access/usage control scenarios are represented using it, to show its usefulness. Finally, a prototype of the proposed framework was built in an operating system kernel, to control the usage of local files. The prototype evaluation shows that the proposed model is feasible, straightforward, and may serve as a basis for more complex usage control frameworks.",No
53e9b38eb7602d9703e6dc41,Applying a usage control model in an operating system kernel,5484e84cdabfae8a11fb241e,Carlos Maziero,Full Paper,Expert Group,S25657,J. Network and Computer Applications,2011,Conference,34,RV63542,Megan Carr,RV76072,Tyler Norman,H37960,okevqrlosf,Chair,AR45686,traditional access control,R52980,Rejected,"Operating systems traditionally use access control mechanisms to manage access to system resources like files, network connections, and memory areas. However, classic access control models are not suitable for regulating access to the diversity of ways data is available and used today. Modern usage control models go beyond traditional access control, addressing its limitations related to attribute mutability and continuous usage permission validation. The recently proposed UCONABC model establishes a predicate-based framework to satisfy the new access/usage control needs in computing systems. This paper defines a usage control model based on UCONABC and describes a framework to implement it in an operating system kernel, on top of the existing DAC mechanism. A language for representing usage control entities and rules is also proposed, and some typical access/usage control scenarios are represented using it, to show its usefulness. Finally, a prototype of the proposed framework was built in an operating system kernel, to control the usage of local files. The prototype evaluation shows that the proposed model is feasible, straightforward, and may serve as a basis for more complex usage control frameworks.",No
53e9b38eb7602d9703e6dc41,Applying a usage control model in an operating system kernel,53f452d5dabfaee2a1d662ff,Altair Santin,Full Paper,Expert Group,S25657,J. Network and Computer Applications,2011,Conference,34,RV63542,Megan Carr,RV76072,Tyler Norman,H37960,okevqrlosf,Chair,AR45686,traditional access control,R52980,Rejected,"Operating systems traditionally use access control mechanisms to manage access to system resources like files, network connections, and memory areas. However, classic access control models are not suitable for regulating access to the diversity of ways data is available and used today. Modern usage control models go beyond traditional access control, addressing its limitations related to attribute mutability and continuous usage permission validation. The recently proposed UCONABC model establishes a predicate-based framework to satisfy the new access/usage control needs in computing systems. This paper defines a usage control model based on UCONABC and describes a framework to implement it in an operating system kernel, on top of the existing DAC mechanism. A language for representing usage control entities and rules is also proposed, and some typical access/usage control scenarios are represented using it, to show its usefulness. Finally, a prototype of the proposed framework was built in an operating system kernel, to control the usage of local files. The prototype evaluation shows that the proposed model is feasible, straightforward, and may serve as a basis for more complex usage control frameworks.",No
53e9b388b7602d9703e6d9b5,How fractional counting of citations affects the impact factor: Normalization in terms of differences in citation potentials among fields of science,5448dbd7dabfae87b7e884bf,Loet Leydesdorff,Short Paper,None,S22165,JASIST,2011,Journal,62,RV63000,Katie Gilbert,RV78026,Catherine George,H35048,inadqsijnh,Editor,AR48622,engineering indicators,R51837,Rejected,"The Impact Factors (IFs) of the Institute for Scientific Information suffer from a number of drawbacks, among them the statistics—Why should one use the mean and not the median?—and the incomparability among fields of science because of systematic differences in citation behavior among fields. Can these drawbacks be counteracted by fractionally counting citation weights instead of using whole numbers in the numerators? (a) Fractional citation counts are normalized in terms of the citing sources and thus would take into account differences in citation behavior among fields of science. (b) Differences in the resulting distributions can be tested statistically for their significance at different levels of aggregation. (c) Fractional counting can be generalized to any document set including journals or groups of journals, and thus the significance of differences among both small and large sets can be tested. A list of fractionally counted IFs for 2008 is available online at The between-group variance among the 13 fields of science identified in the U.S. Science and Engineering Indicators is no longer statistically significant after this normalization. Although citation behavior differs largely between disciplines, the reflection of these differences in fractionally counted citation distributions can not be used as a reliable instrument for the classification. © 2011 Wiley Periodicals, Inc.",No
53e9b388b7602d9703e6d9b5,How fractional counting of citations affects the impact factor: Normalization in terms of differences in citation potentials among fields of science,5485b7cfdabfae8a11fb2b5d,Lutz Bornmann,Short Paper,None,S22165,JASIST,2011,Journal,62,RV63000,Katie Gilbert,RV78026,Catherine George,H35048,inadqsijnh,Editor,AR48622,engineering indicators,R51837,Rejected,"The Impact Factors (IFs) of the Institute for Scientific Information suffer from a number of drawbacks, among them the statistics—Why should one use the mean and not the median?—and the incomparability among fields of science because of systematic differences in citation behavior among fields. Can these drawbacks be counteracted by fractionally counting citation weights instead of using whole numbers in the numerators? (a) Fractional citation counts are normalized in terms of the citing sources and thus would take into account differences in citation behavior among fields of science. (b) Differences in the resulting distributions can be tested statistically for their significance at different levels of aggregation. (c) Fractional counting can be generalized to any document set including journals or groups of journals, and thus the significance of differences among both small and large sets can be tested. A list of fractionally counted IFs for 2008 is available online at The between-group variance among the 13 fields of science identified in the U.S. Science and Engineering Indicators is no longer statistically significant after this normalization. Although citation behavior differs largely between disciplines, the reflection of these differences in fractionally counted citation distributions can not be used as a reliable instrument for the classification. © 2011 Wiley Periodicals, Inc.",No
53e9b388b7602d9703e6d9c7,"Process energy systems: Control, economic, and sustainability objectives.",53f43af7dabfaedf435ab817,Jeffrey J. Siirola,Full Paper,None,S28513,Computers & Chemical Engineering,2012,Journal,47,RV64859,Dana Hill,RV78004,David Bean,H36700,zvnhilbeyy,Editor,AR45746,Sustainability,R52244,Rejected,"Economic, energy, and sustainability metrics are key performance indicators for process operations. The relative importance of these metrics varies from plant to plant, and often some metrics are in conflict with each other (sustainability vs. profitability). In this paper we discuss the current plant environment and how various metrics can be aligned by focusing on energy efficiency. Power-steam systems are the major energy drivers for most plants, and we discuss possible operational changes that might improve energy efficiency, as well as the role of process control. Managing the interplay of real-time optimization and regulatory control is a challenge for the future, as well as interfacing with the implementation of smart power grids by the utility industry. Combined heat and power along with energy storage presents interesting control and optimization opportunities to maximize energy efficiency. (C) 2012 Elsevier Ltd. All rights reserved.",No
53e9b388b7602d9703e6d9c7,"Process energy systems: Control, economic, and sustainability objectives.",54324117dabfaeb4c6a7b475,Thomas F. Edgar,Full Paper,None,S28513,Computers & Chemical Engineering,2012,Journal,47,RV64859,Dana Hill,RV78004,David Bean,H36700,zvnhilbeyy,Editor,AR45746,Sustainability,R52244,Rejected,"Economic, energy, and sustainability metrics are key performance indicators for process operations. The relative importance of these metrics varies from plant to plant, and often some metrics are in conflict with each other (sustainability vs. profitability). In this paper we discuss the current plant environment and how various metrics can be aligned by focusing on energy efficiency. Power-steam systems are the major energy drivers for most plants, and we discuss possible operational changes that might improve energy efficiency, as well as the role of process control. Managing the interplay of real-time optimization and regulatory control is a challenge for the future, as well as interfacing with the implementation of smart power grids by the utility industry. Combined heat and power along with energy storage presents interesting control and optimization opportunities to maximize energy efficiency. (C) 2012 Elsevier Ltd. All rights reserved.",No
53e9b388b7602d9703e6d9e0,Multivariable Harmonic Balance for Central Pattern Generators.,54484515dabfae87b7dfa487,Tetsuya Iwasaki,Demo Paper,Workshop,S29408,Automatica,2008,Conference,44,RV69259,Robert York,RV77515,Annette Smith,H32957,qomdbyrlaq,Chair,AR42398,Oscillators,R58073,Accepted,"The central pattern generator (CPG) is a nonlinear oscillator formed by a group of neurons, providing a fundamental control mechanism underlying rhythmic movements in animal locomotion. We consider a class of CPGs modeled by a set of interconnected identical neurons. Based on the idea of multivariable harmonic balance, we show how the oscillation profile is related to the connectivity matrix that specifies the architecture and strengths of the interconnections. Specifically, the frequency, amplitudes, and phases are essentially encoded in terms of a pair of eigenvalue and eigenvector. This basic principle is used to estimate the oscillation profile of a given CPG model. Moreover, a systematic method is proposed for designing a CPG-based nonlinear oscillator that achieves a prescribed oscillation profile.",Automatica 44
53e9b38eb7602d9703e6dc5e,Dielectric plug-loaded two-port transmission line measurement technique for dielectric property characterization of granular and liquid materials,53f48069dabfaee4dc8aed43,Karl J. Bois,Full Paper,Workshop,S25432,IEEE Transactions on Instrumentation and Measurement,1999,Conference,48,RV62233,Dylan Harvey,RV73351,Danielle Norris,H35238,iwtihlhqmw,Chair,AR48076,Transmission line measurements,R59215,Accepted,"There are numerous dielectric property characterization techniques available in the microwave regime each with its own uniqueness, advantages and disadvantages. The two-port completely-filled waveguide (transmission line) technique is a robust measurement approach which is well suited for solid dielectric materials. In this case, the dielectric material can be relatively easily machined to fit ins...",IEEE Transactions on Instrumentation and Measurement 48
53e9b38eb7602d9703e6dc5e,Dielectric plug-loaded two-port transmission line measurement technique for dielectric property characterization of granular and liquid materials,53f3288bdabfae9a8447f788,Larry F. Handjojo,Full Paper,Workshop,S25432,IEEE Transactions on Instrumentation and Measurement,1999,Conference,48,RV62233,Dylan Harvey,RV73351,Danielle Norris,H35238,iwtihlhqmw,Chair,AR48076,Transmission line measurements,R59215,Accepted,"There are numerous dielectric property characterization techniques available in the microwave regime each with its own uniqueness, advantages and disadvantages. The two-port completely-filled waveguide (transmission line) technique is a robust measurement approach which is well suited for solid dielectric materials. In this case, the dielectric material can be relatively easily machined to fit ins...",IEEE Transactions on Instrumentation and Measurement 48
53e9b38eb7602d9703e6dc5e,Dielectric plug-loaded two-port transmission line measurement technique for dielectric property characterization of granular and liquid materials,53f42f74dabfaee1c0a4e15a,Aaron D. Benally,Full Paper,Workshop,S25432,IEEE Transactions on Instrumentation and Measurement,1999,Conference,48,RV62233,Dylan Harvey,RV73351,Danielle Norris,H35238,iwtihlhqmw,Chair,AR48076,Transmission line measurements,R59215,Accepted,"There are numerous dielectric property characterization techniques available in the microwave regime each with its own uniqueness, advantages and disadvantages. The two-port completely-filled waveguide (transmission line) technique is a robust measurement approach which is well suited for solid dielectric materials. In this case, the dielectric material can be relatively easily machined to fit ins...",IEEE Transactions on Instrumentation and Measurement 48
53e9b38eb7602d9703e6dc5e,Dielectric plug-loaded two-port transmission line measurement technique for dielectric property characterization of granular and liquid materials,53f4330ddabfaeb2ac0313ba,Khalid Mubarak,Full Paper,Workshop,S25432,IEEE Transactions on Instrumentation and Measurement,1999,Conference,48,RV62233,Dylan Harvey,RV73351,Danielle Norris,H35238,iwtihlhqmw,Chair,AR48076,Transmission line measurements,R59215,Accepted,"There are numerous dielectric property characterization techniques available in the microwave regime each with its own uniqueness, advantages and disadvantages. The two-port completely-filled waveguide (transmission line) technique is a robust measurement approach which is well suited for solid dielectric materials. In this case, the dielectric material can be relatively easily machined to fit ins...",IEEE Transactions on Instrumentation and Measurement 48
53e9b38eb7602d9703e6dc5e,Dielectric plug-loaded two-port transmission line measurement technique for dielectric property characterization of granular and liquid materials,53f42c81dabfaec22ba0a4cb,Reza Zoughi,Full Paper,Workshop,S25432,IEEE Transactions on Instrumentation and Measurement,1999,Conference,48,RV62233,Dylan Harvey,RV73351,Danielle Norris,H35238,iwtihlhqmw,Chair,AR48076,Transmission line measurements,R59215,Accepted,"There are numerous dielectric property characterization techniques available in the microwave regime each with its own uniqueness, advantages and disadvantages. The two-port completely-filled waveguide (transmission line) technique is a robust measurement approach which is well suited for solid dielectric materials. In this case, the dielectric material can be relatively easily machined to fit ins...",IEEE Transactions on Instrumentation and Measurement 48
53e9b388b7602d9703e6d857,What is What it's Like? Introducing Perceptual Modes of Presentation,53f4309ddabfaeb2ac012c91,John Kulvicki,Full Paper,Expert Group,S27695,Synthese,2007,Conference,156,RV63298,Jill Phillips,RV71425,Adam Wilkerson,H31860,snknkkmzqr,Chair,AR46360,Perceptual System,R51881,Rejected,"The central claim of this paper is that what it is like to see green or any other perceptible property is just the perceptual mode of presentation of that property. Perceptual modes of presentation are important because they help resolve a tension in current work on consciousness.
 Philosophers are pulled by three mutually inconsistent theses: representational externalism, representationalism, and phenomenal
 internalism. I throw my hat in with defenders of the first two: the externalist representationalists. We are faced with the
 problem of explaining away intuitions that favor phenomenal internalism. Perceptual modes of presentation account for what
 it is like to see properties in a way that accommodates those intuitions without vindicating phenomenal internalism itself.
 Perceptual MoPs therefore provide a new way of being an externalist representationalist.",No
53e9b388b7602d9703e6da68,Imprecise Answers In Distributed Environments: Estimation Of Information Loss For Multi-Ontology Based Query Processing,53f443e3dabfaefedbb0b479,Eduardo Mena,Short Paper,None,S25945,INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS,2000,Journal,9,RV62707,Troy Hall,RV78990,Nicholas Morales,H35379,kxctaudtsy,Editor,AR42490,world wide web,R51226,Rejected,"The World Wide Web is fast becoming a ubiquitous computing environment. Prevalent keyword-based search techniques are scalable, but are incapable of accessing information based on concepts. We investigate the use of concepts from multiple, real-world pre-existing, domain ontologies to describe the underlying data content and support information access at a higher level of abstraction. It is not practical to have a single domain ontology to describe the vast amounts of data on the Web. In fact, we expect multiple ontologies to be used as different world views and present an approach to browse ontologies as a paradigm for information access. A critical challenge in this approach is the vocabulary heterogeneity problem. Queries are rewritten using interontology relationships to obtain translations across ontologies. However, some translations may not be semantics preserving, leading to uncertainty or loss in the information retrieved. We present a novel approach for estimating loss of information based on the navigation of ontological terms. We define measures for loss of information based on intensional information as well as on well established metrics like precision and recall based on extensional information. These measures are used to select results having the desired quality of information.",No
53e9b388b7602d9703e6da68,Imprecise Answers In Distributed Environments: Estimation Of Information Loss For Multi-Ontology Based Query Processing,53f494a7dabfaee0d9c7466a,Vipul Kashyap,Short Paper,None,S25945,INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS,2000,Journal,9,RV62707,Troy Hall,RV78990,Nicholas Morales,H35379,kxctaudtsy,Editor,AR42490,world wide web,R51226,Rejected,"The World Wide Web is fast becoming a ubiquitous computing environment. Prevalent keyword-based search techniques are scalable, but are incapable of accessing information based on concepts. We investigate the use of concepts from multiple, real-world pre-existing, domain ontologies to describe the underlying data content and support information access at a higher level of abstraction. It is not practical to have a single domain ontology to describe the vast amounts of data on the Web. In fact, we expect multiple ontologies to be used as different world views and present an approach to browse ontologies as a paradigm for information access. A critical challenge in this approach is the vocabulary heterogeneity problem. Queries are rewritten using interontology relationships to obtain translations across ontologies. However, some translations may not be semantics preserving, leading to uncertainty or loss in the information retrieved. We present a novel approach for estimating loss of information based on the navigation of ontological terms. We define measures for loss of information based on intensional information as well as on well established metrics like precision and recall based on extensional information. These measures are used to select results having the desired quality of information.",No
53e9b388b7602d9703e6da68,Imprecise Answers In Distributed Environments: Estimation Of Information Loss For Multi-Ontology Based Query Processing,53f47eeedabfae9126cc53b4,Arantza Illarramendi,Short Paper,None,S25945,INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS,2000,Journal,9,RV62707,Troy Hall,RV78990,Nicholas Morales,H35379,kxctaudtsy,Editor,AR42490,world wide web,R51226,Rejected,"The World Wide Web is fast becoming a ubiquitous computing environment. Prevalent keyword-based search techniques are scalable, but are incapable of accessing information based on concepts. We investigate the use of concepts from multiple, real-world pre-existing, domain ontologies to describe the underlying data content and support information access at a higher level of abstraction. It is not practical to have a single domain ontology to describe the vast amounts of data on the Web. In fact, we expect multiple ontologies to be used as different world views and present an approach to browse ontologies as a paradigm for information access. A critical challenge in this approach is the vocabulary heterogeneity problem. Queries are rewritten using interontology relationships to obtain translations across ontologies. However, some translations may not be semantics preserving, leading to uncertainty or loss in the information retrieved. We present a novel approach for estimating loss of information based on the navigation of ontological terms. We define measures for loss of information based on intensional information as well as on well established metrics like precision and recall based on extensional information. These measures are used to select results having the desired quality of information.",No
53e9b388b7602d9703e6da68,Imprecise Answers In Distributed Environments: Estimation Of Information Loss For Multi-Ontology Based Query Processing,53f44cb0dabfaedf435e5ce1,Amit P. Sheth,Short Paper,None,S25945,INTERNATIONAL JOURNAL OF COOPERATIVE INFORMATION SYSTEMS,2000,Journal,9,RV62707,Troy Hall,RV78990,Nicholas Morales,H35379,kxctaudtsy,Editor,AR42490,world wide web,R51226,Rejected,"The World Wide Web is fast becoming a ubiquitous computing environment. Prevalent keyword-based search techniques are scalable, but are incapable of accessing information based on concepts. We investigate the use of concepts from multiple, real-world pre-existing, domain ontologies to describe the underlying data content and support information access at a higher level of abstraction. It is not practical to have a single domain ontology to describe the vast amounts of data on the Web. In fact, we expect multiple ontologies to be used as different world views and present an approach to browse ontologies as a paradigm for information access. A critical challenge in this approach is the vocabulary heterogeneity problem. Queries are rewritten using interontology relationships to obtain translations across ontologies. However, some translations may not be semantics preserving, leading to uncertainty or loss in the information retrieved. We present a novel approach for estimating loss of information based on the navigation of ontological terms. We define measures for loss of information based on intensional information as well as on well established metrics like precision and recall based on extensional information. These measures are used to select results having the desired quality of information.",No
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f4319ddabfaee2a1cb284b,P. Gryboś,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f4586ddabfaec09f20de19,W. Dąbrowski,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f43a6adabfaee43ec56174,P. Hottowy,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f42bf5dabfaedce54af58f,R. Szczygieł,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f438c7dabfaec09f193afa,K. Świentek,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dca8,Multichannel mixed-mode IC for digital readout of silicon strip detectors,53f42d4edabfaee02ac5fe26,P. Wiącek,Short Paper,None,S25186,Microelectronics Reliability,2002,Journal,42,RV60521,Michael Ramsey,RV76423,Kelly Gonzales,H35098,jnqztonnyv,Editor,AR47298,standard deviation,R55533,Accepted,"The design of a mixed mode 64-channel ASIC for digital readout of silicon strip detectors used in X-ray imaging applications is presented. The design is implemented in a 0.8 μm CMOS n-well epi-type substrate technology. The single channel consists of a charge sensitive preamplifier, a shaping amplifier, a discriminator and a pseudo-random 20-bit counter. In addition to the 64-channel core which is responsible for signal processing and data storage, the IC comprises additional blocks like an internal calibration circuit, internal DACs and a command decoder which governs communication of the IC with the external world. Those blocks have been designed keeping in mind testability features and using the chips for building multi-chip modules. The equivalent input noise charge measured at room temperature for a detector capacitance of 2.5 pF and signal integration time of 0.7 μs is 167 e−rms. The average power dissipation is 2.5 mW/channel. The channel-to-channel offset spread referred to the input is only 28 e−rms, while the standard deviation of gain spread is 0.5% of the mean value. The chip occupies area of 2.8×6.5 mm2.",Microelectronics Reliability 42
53e9b38eb7602d9703e6dcc9,Brief Fundamental limitations due to jω-axis zeros in SISO systems,5632061045cedb3399f97fbd,Graham C. Goodwin,Short Paper,Expert Group,S25757,Automatica (Journal of IFAC),1999,Conference,35,RV64766,Nicholas Frank,RV73840,Morgan Salinas,H33209,tfxwaemgsw,Chair,AR43474,axis zero,R52205,Rejected,"This paper shows that fundamental limitations arise from the presence of stable zeros on or near the j ω -axis. The stability of the closed-loop system is used in conjunction with the Laplace transform to show that time domain integral constraints must be satisfied by the system response. The presence of stable zeros near the j ω  axis together with upper limits on the permissible overshoot of the output signal, place an effective lower bound on the settling time of the closed loop system. The constraints are illustrated by an application to the control of a single stand reversing mill.",No
53e9b38eb7602d9703e6dcc9,Brief Fundamental limitations due to jω-axis zeros in SISO systems,53f3aa90dabfae4b34af4654,A. R. Woodyatt,Short Paper,Expert Group,S25757,Automatica (Journal of IFAC),1999,Conference,35,RV64766,Nicholas Frank,RV73840,Morgan Salinas,H33209,tfxwaemgsw,Chair,AR43474,axis zero,R52205,Rejected,"This paper shows that fundamental limitations arise from the presence of stable zeros on or near the j ω -axis. The stability of the closed-loop system is used in conjunction with the Laplace transform to show that time domain integral constraints must be satisfied by the system response. The presence of stable zeros near the j ω  axis together with upper limits on the permissible overshoot of the output signal, place an effective lower bound on the settling time of the closed loop system. The constraints are illustrated by an application to the control of a single stand reversing mill.",No
53e9b38eb7602d9703e6dcc9,Brief Fundamental limitations due to jω-axis zeros in SISO systems,53f448ccdabfaedd74df8538,Richard H. Middleton,Short Paper,Expert Group,S25757,Automatica (Journal of IFAC),1999,Conference,35,RV64766,Nicholas Frank,RV73840,Morgan Salinas,H33209,tfxwaemgsw,Chair,AR43474,axis zero,R52205,Rejected,"This paper shows that fundamental limitations arise from the presence of stable zeros on or near the j ω -axis. The stability of the closed-loop system is used in conjunction with the Laplace transform to show that time domain integral constraints must be satisfied by the system response. The presence of stable zeros near the j ω  axis together with upper limits on the permissible overshoot of the output signal, place an effective lower bound on the settling time of the closed loop system. The constraints are illustrated by an application to the control of a single stand reversing mill.",No
53e9b38eb7602d9703e6dcc9,Brief Fundamental limitations due to jω-axis zeros in SISO systems,54346414dabfaebba585a5a0,J. Shim,Short Paper,Expert Group,S25757,Automatica (Journal of IFAC),1999,Conference,35,RV64766,Nicholas Frank,RV73840,Morgan Salinas,H33209,tfxwaemgsw,Chair,AR43474,axis zero,R52205,Rejected,"This paper shows that fundamental limitations arise from the presence of stable zeros on or near the j ω -axis. The stability of the closed-loop system is used in conjunction with the Laplace transform to show that time domain integral constraints must be satisfied by the system response. The presence of stable zeros near the j ω  axis together with upper limits on the permissible overshoot of the output signal, place an effective lower bound on the settling time of the closed loop system. The constraints are illustrated by an application to the control of a single stand reversing mill.",No
53e9b38eb7602d9703e6dceb,Numerical Techniques For Solving Systems Of Second Order Boundary Value Problems,5432d58edabfaeb542166c53,Muhammad Aslam Noor,Full Paper,None,S26764,INTERNATIONAL JOURNAL OF COMPUTER MATHEMATICS,2001,Journal,77,RV67436,Dr. Jerry Martinez,RV79873,Edward Powell,H30586,qmbtwapfwo,Editor,AR48320,variational inequalities,R56977,Accepted,"In this paper, we compare some collocation, finite difference and spline methods for solving a system of second order boundary value problems associated with obstacle problems. An example is given to illustrate the efficiency of these results.",INTERNATIONAL JOURNAL OF COMPUTER MATHEMATICS 77
53e9b38eb7602d9703e6dceb,Numerical Techniques For Solving Systems Of Second Order Boundary Value Problems,53f46684dabfaedd74e6a46d,Eisa Al-Said,Full Paper,None,S26764,INTERNATIONAL JOURNAL OF COMPUTER MATHEMATICS,2001,Journal,77,RV67436,Dr. Jerry Martinez,RV79873,Edward Powell,H30586,qmbtwapfwo,Editor,AR48320,variational inequalities,R56977,Accepted,"In this paper, we compare some collocation, finite difference and spline methods for solving a system of second order boundary value problems associated with obstacle problems. An example is given to illustrate the efficiency of these results.",INTERNATIONAL JOURNAL OF COMPUTER MATHEMATICS 77
53e9b388b7602d9703e6db40,Conditions To Diffuse Green Management Into Smes And The Role Of Knowledge Support: Agent-Based Modeling,53f32af8dabfae9a8448d0d3,Keiko Zaima,Full Paper,None,S28631,JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS,2013,Journal,17,RV61939,Wayne Reed DVM,RV77086,Justin White,H38180,eyqmtuijrh,Editor,AR41148,agent-based modeling,R59100,Rejected,"Spreading green management among Small and Medium-sized Enterprises (SMEs) is an important environmental policy issue. The objective herein is to design an agent-based model to analyze the conditions that need to be met for the diffusion of green management into SMEs. Our model belongs to the agent-based models related to the diffusion of innovation. One remarkable feature of the basic model is that it includes two related markets for a product and its component, in which there are transactions between consumers and Large Enterprises (LEs) and between LEs and SMEs. Although the basic model is not concerned with a specific product, settings of the model are based on the existing empirical researches on SMEs in machine & metal and plastic processing industries. Simulation results of the basic model reveal effective strategies of enterprises. LEs which raise eco-levels faster than others can acquire profits. SMEs which lower prices faster than others can also improve financial resource. The sensitivity analysis shows that the cost condition is an important factor for the diffusion of green management into SMEs. We suggest that knowledge support to SMEs can take a role in order to reduce environmental cost, based on the results of previous empirical research. An applied model concerning knowledge support is also presented. Simulation results of the applied model reveal that information provision from the external organizations to SMEs could be effective for the diffusion of green management.",No
53e9b388b7602d9703e6db49,Dynamic mean-variance problem with constrained risk control for the insurers,5609532245cedb3396e98203,Lihua Bai,Poster,Regular Conference,S24991,Math. Meth. of OR,2008,Conference,68,RV62073,Andrew Carter,RV70778,Thomas Hernandez,H39871,avduyfrcrm,Chair,AR42948,mean-variance · efficient frontier · efficient strategy · hamilton-jacobi- bellman equation · riccati equation · viscosity solution · lagrange multiplier,R55121,Accepted,"In this paper, we study optimal reinsurance/new business and investment (no-shorting) strategy for the mean-variance problem
 in two risk models: a classical risk model and a diffusion model. The problem is firstly reduced to a stochastic linear-quadratic
 (LQ) control problem with constraints. Then, the efficient frontiers and efficient strategies are derived explicitly by a
 verification theorem with the viscosity solutions of Hamilton–Jacobi–Bellman (HJB) equations, which is different from that
 given in Zhou et al. (SIAM J Control Optim 35:243–253, 1997). Furthermore, by comparisons, we find that they are identical
 under the two risk models.",Math. Meth. of OR 68
53e9b388b7602d9703e6db49,Dynamic mean-variance problem with constrained risk control for the insurers,53f32231dabfae9a8445c8fe,Huayue Zhang,Poster,Regular Conference,S24991,Math. Meth. of OR,2008,Conference,68,RV62073,Andrew Carter,RV70778,Thomas Hernandez,H39871,avduyfrcrm,Chair,AR42948,mean-variance · efficient frontier · efficient strategy · hamilton-jacobi- bellman equation · riccati equation · viscosity solution · lagrange multiplier,R55121,Accepted,"In this paper, we study optimal reinsurance/new business and investment (no-shorting) strategy for the mean-variance problem
 in two risk models: a classical risk model and a diffusion model. The problem is firstly reduced to a stochastic linear-quadratic
 (LQ) control problem with constraints. Then, the efficient frontiers and efficient strategies are derived explicitly by a
 verification theorem with the viscosity solutions of Hamilton–Jacobi–Bellman (HJB) equations, which is different from that
 given in Zhou et al. (SIAM J Control Optim 35:243–253, 1997). Furthermore, by comparisons, we find that they are identical
 under the two risk models.",Math. Meth. of OR 68
